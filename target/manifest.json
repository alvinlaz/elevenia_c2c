{"macros": {"macro.dbt.archive_update": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "archive_update", "package_name": "dbt", "unique_id": "macro.dbt.archive_update", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__no_svv_table_info_warning": {"original_file_path": "macros\\catalog\\redshift_catalog.sql", "tags": [], "name": "redshift__no_svv_table_info_warning", "package_name": "dbt", "unique_id": "macro.dbt.redshift__no_svv_table_info_warning", "path": "macros\\catalog\\redshift_catalog.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro redshift__get_base_catalog() -%}\n  {%- call statement('base_catalog', fetch_result=True) -%}\n    with late_binding as (\n      select\n        table_schema,\n        table_name,\n        'LATE BINDING VIEW'::varchar as table_type,\n        null::text as table_comment,\n\n        column_name,\n        column_index,\n        column_type,\n        null::text as column_comment\n      from pg_get_late_binding_view_cols()\n        cols(table_schema name, table_name name, column_name name,\n             column_type varchar,\n             column_index int)\n        order by \"column_index\"\n    ),\n\n    tables as (\n\n      select\n          table_schema,\n          table_name,\n          table_type\n\n      from information_schema.tables\n\n    ),\n\n    table_owners as (\n\n        select\n            schemaname as table_schema,\n            tablename as table_name,\n            tableowner as table_owner\n\n        from pg_tables\n\n        union all\n\n        select\n            schemaname as table_schema,\n            viewname as table_name,\n            viewowner as table_owner\n\n        from pg_views\n\n    ),\n\n    columns as (\n\n        select\n            table_schema,\n            table_name,\n            null::varchar as table_comment,\n\n            column_name,\n            ordinal_position as column_index,\n            data_type as column_type,\n            null::varchar as column_comment\n\n\n        from information_schema.columns\n\n    ),\n\n    unioned as (\n\n        select *\n        from tables\n        join columns using (table_schema, table_name)\n\n        union all\n\n        select *\n        from late_binding\n\n    )\n\n    select *,\n        table_schema || '.' || table_name as table_id\n\n    from unioned\n    join table_owners using (table_schema, table_name)\n\n    where table_schema != 'information_schema'\n      and table_schema not like 'pg_%'\n\n    order by \"column_index\"\n  {%- endcall -%}\n\n  {{ return(load_result('base_catalog').table) }}\n{%- endmacro %}\n\n{% macro redshift__get_extended_catalog() %}\n  {%- call statement('extended_catalog', fetch_result=True) -%}\n\n    select\n        \"schema\" || '.' || \"table\" as table_id,\n\n        'Encoded'::text as \"stats:encoded:label\",\n        encoded as \"stats:encoded:value\",\n        'Indicates whether any column in the table has compression encoding defined.'::text as \"stats:encoded:description\",\n        true as \"stats:encoded:include\",\n\n        'Dist Style' as \"stats:diststyle:label\",\n        diststyle as \"stats:diststyle:value\",\n        'Distribution style or distribution key column, if key distribution is defined.'::text as \"stats:diststyle:description\",\n        true as \"stats:diststyle:include\",\n\n        'Sort Key 1' as \"stats:sortkey1:label\",\n        -- handle 0xFF byte in response for interleaved sort styles\n        case\n            when sortkey1 like 'INTERLEAVED%' then 'INTERLEAVED'::text\n            else sortkey1\n        end as \"stats:sortkey1:value\",\n        'First column in the sort key.'::text as \"stats:sortkey1:description\",\n        (sortkey1 is not null) as \"stats:sortkey1:include\",\n\n        'Max Varchar' as \"stats:max_varchar:label\",\n        max_varchar as \"stats:max_varchar:value\",\n        'Size of the largest column that uses a VARCHAR data type.'::text as \"stats:max_varchar:description\",\n        true as \"stats:max_varchar:include\",\n\n        -- exclude this, as the data is strangely returned with null-byte characters\n        'Sort Key 1 Encoding' as \"stats:sortkey1_enc:label\",\n        sortkey1_enc as \"stats:sortkey1_enc:value\",\n        'Compression encoding of the first column in the sort key.' as \"stats:sortkey1_enc:description\",\n        false as \"stats:sortkey1_enc:include\",\n\n        '# Sort Keys' as \"stats:sortkey_num:label\",\n        sortkey_num as \"stats:sortkey_num:value\",\n        'Number of columns defined as sort keys.' as \"stats:sortkey_num:description\",\n        (sortkey_num > 0) as \"stats:sortkey_num:include\",\n\n        'Approximate Size' as \"stats:size:label\",\n        size / 1000000.0 as \"stats:size:value\",\n        'Approximate size of the table, calculated from a count of 1MB blocks'::text as \"stats:size:description\",\n        true as \"stats:size:include\",\n\n        'Disk Utilization' as \"stats:pct_used:label\",\n        pct_used / 100.0 as \"stats:pct_used:value\",\n        'Percent of available space that is used by the table.'::text as \"stats:pct_used:description\",\n        true as \"stats:pct_used:include\",\n\n        'Unsorted %' as \"stats:unsorted:label\",\n        unsorted / 100.0 as \"stats:unsorted:value\",\n        'Percent of unsorted rows in the table.'::text as \"stats:unsorted:description\",\n        (unsorted is not null) as \"stats:unsorted:include\",\n\n        'Stats Off' as \"stats:stats_off:label\",\n        stats_off as \"stats:stats_off:value\",\n        'Number that indicates how stale the table statistics are; 0 is current, 100 is out of date.'::text as \"stats:stats_off:description\",\n        true as \"stats:stats_off:include\",\n\n        'Approximate Row Count' as \"stats:rows:label\",\n        tbl_rows as \"stats:rows:value\",\n        'Approximate number of rows in the table. This value includes rows marked for deletion, but not yet vacuumed.'::text as \"stats:rows:description\",\n        true as \"stats:rows:include\",\n\n        'Sort Key Skew' as \"stats:skew_sortkey1:label\",\n        skew_sortkey1 as \"stats:skew_sortkey1:value\",\n        'Ratio of the size of the largest non-sort key column to the size of the first column of the sort key.'::text as \"stats:skew_sortkey1:description\",\n        (skew_sortkey1 is not null) as \"stats:skew_sortkey1:include\",\n\n        'Skew Rows' as \"stats:skew_rows:label\",\n        skew_rows as \"stats:skew_rows:value\",\n        'Ratio of the number of rows in the slice with the most rows to the number of rows in the slice with the fewest rows.'::text as \"stats:skew_rows:description\",\n        (skew_rows is not null) as \"stats:skew_rows:include\"\n\n    from svv_table_info\n\n  {%- endcall -%}\n\n  {{ return(load_result('extended_catalog').table) }}\n\n{% endmacro %}\n\n{% macro redshift__can_select_from(table_name) %}\n\n  {%- call statement('has_table_privilege', fetch_result=True) -%}\n\n    select has_table_privilege(current_user, '{{ table_name }}', 'SELECT') as can_select\n\n  {%- endcall -%}\n\n  {% set can_select = load_result('has_table_privilege').table[0]['can_select'] %}\n  {{ return(can_select) }}\n\n{% endmacro %}\n\n{% macro redshift__no_svv_table_info_warning() %}\n\n    {% set msg %}\n\n    Warning: The database user \"{{ target.user }}\" has insufficient permissions to\n    query the \"svv_table_info\" table. Please grant SELECT permissions on this table\n    to the \"{{ target.user }}\" user to fetch extended table details from Redshift.\n\n    {% endset %}\n\n    {{ log(msg, info=True) }}\n\n{% endmacro %}\n\n\n{% macro redshift__get_catalog() %}\n\n    {#-- Compute a left-outer join in memory. Some Redshift queries are\n      -- leader-only, and cannot be joined to other compute-based queries #}\n\n    {% set catalog = redshift__get_base_catalog() %}\n\n    {% set select_extended =  redshift__can_select_from('svv_table_info') %}\n    {% if select_extended %}\n        {% set extended_catalog = redshift__get_extended_catalog() %}\n        {% set catalog = catalog.join(extended_catalog, 'table_id') %}\n    {% else %}\n        {{ redshift__no_svv_table_info_warning() }}\n    {% endif %}\n\n    {{ return(catalog.exclude(['table_id'])) }}\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_table_default": {"original_file_path": "macros\\materializations\\table\\table.sql", "tags": [], "name": "materialization_table_default", "package_name": "dbt", "unique_id": "macro.dbt.materialization_table_default", "path": "macros\\materializations\\table\\table.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% materialization table, default %}\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_tmp' -%}\n  {%- set backup_identifier = identifier + '__dbt_backup' -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n  {%- set target_relation = api.Relation.create(identifier=identifier,\n                                                schema=schema, type='table') -%}\n  {%- set intermediate_relation = api.Relation.create(identifier=tmp_identifier,\n                                                      schema=schema, type='table') -%}\n\n  /*\n      See ../view/view.sql for more information about this relation.\n  */\n  {%- set backup_relation = api.Relation.create(identifier=backup_identifier,\n                                                schema=schema, type=(old_relation.type or 'table')) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n  {%- set create_as_temporary = (exists_as_table and non_destructive_mode) -%}\n\n\n  -- drop the temp relations if they exists for some reason\n  {{ adapter.drop_relation(intermediate_relation) }}\n  {{ adapter.drop_relation(backup_relation) }}\n\n  -- setup: if the target relation already exists, truncate or drop it (if it's a view)\n  {% if non_destructive_mode -%}\n    {% if exists_as_table -%}\n      {{ adapter.truncate_relation(old_relation) }}\n    {% elif exists_as_view -%}\n      {{ adapter.drop_relation(old_relation) }}\n      {%- set old_relation = none -%}\n    {%- endif %}\n  {%- endif %}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% call statement('main') -%}\n    {%- if non_destructive_mode -%}\n      {%- if old_relation is not none -%}\n        {{ create_table_as(create_as_temporary, intermediate_relation, sql) }}\n\n        {% set dest_columns = adapter.get_columns_in_table(schema, identifier) %}\n        {% set dest_cols_csv = dest_columns | map(attribute='quoted') | join(', ') %}\n\n        insert into {{ target_relation }} ({{ dest_cols_csv }}) (\n          select {{ dest_cols_csv }}\n          from {{ intermediate_relation.include(schema=(not create_as_temporary)) }}\n        );\n      {%- else -%}\n        {{ create_table_as(create_as_temporary, target_relation, sql) }}\n      {%- endif -%}\n    {%- else -%}\n      {{ create_table_as(create_as_temporary, intermediate_relation, sql) }}\n    {%- endif -%}\n  {%- endcall %}\n\n  -- cleanup\n  {% if non_destructive_mode -%}\n    -- noop\n  {%- else -%}\n    {% if old_relation is not none %}\n        {{ adapter.rename_relation(target_relation, backup_relation) }}\n    {% endif %}\n\n    {{ adapter.rename_relation(intermediate_relation, target_relation) }}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  -- finally, drop the existing/backup relation after the commit\n  {{ drop_relation_if_exists(backup_relation) }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__can_select_from": {"original_file_path": "macros\\catalog\\redshift_catalog.sql", "tags": [], "name": "redshift__can_select_from", "package_name": "dbt", "unique_id": "macro.dbt.redshift__can_select_from", "path": "macros\\catalog\\redshift_catalog.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro redshift__get_base_catalog() -%}\n  {%- call statement('base_catalog', fetch_result=True) -%}\n    with late_binding as (\n      select\n        table_schema,\n        table_name,\n        'LATE BINDING VIEW'::varchar as table_type,\n        null::text as table_comment,\n\n        column_name,\n        column_index,\n        column_type,\n        null::text as column_comment\n      from pg_get_late_binding_view_cols()\n        cols(table_schema name, table_name name, column_name name,\n             column_type varchar,\n             column_index int)\n        order by \"column_index\"\n    ),\n\n    tables as (\n\n      select\n          table_schema,\n          table_name,\n          table_type\n\n      from information_schema.tables\n\n    ),\n\n    table_owners as (\n\n        select\n            schemaname as table_schema,\n            tablename as table_name,\n            tableowner as table_owner\n\n        from pg_tables\n\n        union all\n\n        select\n            schemaname as table_schema,\n            viewname as table_name,\n            viewowner as table_owner\n\n        from pg_views\n\n    ),\n\n    columns as (\n\n        select\n            table_schema,\n            table_name,\n            null::varchar as table_comment,\n\n            column_name,\n            ordinal_position as column_index,\n            data_type as column_type,\n            null::varchar as column_comment\n\n\n        from information_schema.columns\n\n    ),\n\n    unioned as (\n\n        select *\n        from tables\n        join columns using (table_schema, table_name)\n\n        union all\n\n        select *\n        from late_binding\n\n    )\n\n    select *,\n        table_schema || '.' || table_name as table_id\n\n    from unioned\n    join table_owners using (table_schema, table_name)\n\n    where table_schema != 'information_schema'\n      and table_schema not like 'pg_%'\n\n    order by \"column_index\"\n  {%- endcall -%}\n\n  {{ return(load_result('base_catalog').table) }}\n{%- endmacro %}\n\n{% macro redshift__get_extended_catalog() %}\n  {%- call statement('extended_catalog', fetch_result=True) -%}\n\n    select\n        \"schema\" || '.' || \"table\" as table_id,\n\n        'Encoded'::text as \"stats:encoded:label\",\n        encoded as \"stats:encoded:value\",\n        'Indicates whether any column in the table has compression encoding defined.'::text as \"stats:encoded:description\",\n        true as \"stats:encoded:include\",\n\n        'Dist Style' as \"stats:diststyle:label\",\n        diststyle as \"stats:diststyle:value\",\n        'Distribution style or distribution key column, if key distribution is defined.'::text as \"stats:diststyle:description\",\n        true as \"stats:diststyle:include\",\n\n        'Sort Key 1' as \"stats:sortkey1:label\",\n        -- handle 0xFF byte in response for interleaved sort styles\n        case\n            when sortkey1 like 'INTERLEAVED%' then 'INTERLEAVED'::text\n            else sortkey1\n        end as \"stats:sortkey1:value\",\n        'First column in the sort key.'::text as \"stats:sortkey1:description\",\n        (sortkey1 is not null) as \"stats:sortkey1:include\",\n\n        'Max Varchar' as \"stats:max_varchar:label\",\n        max_varchar as \"stats:max_varchar:value\",\n        'Size of the largest column that uses a VARCHAR data type.'::text as \"stats:max_varchar:description\",\n        true as \"stats:max_varchar:include\",\n\n        -- exclude this, as the data is strangely returned with null-byte characters\n        'Sort Key 1 Encoding' as \"stats:sortkey1_enc:label\",\n        sortkey1_enc as \"stats:sortkey1_enc:value\",\n        'Compression encoding of the first column in the sort key.' as \"stats:sortkey1_enc:description\",\n        false as \"stats:sortkey1_enc:include\",\n\n        '# Sort Keys' as \"stats:sortkey_num:label\",\n        sortkey_num as \"stats:sortkey_num:value\",\n        'Number of columns defined as sort keys.' as \"stats:sortkey_num:description\",\n        (sortkey_num > 0) as \"stats:sortkey_num:include\",\n\n        'Approximate Size' as \"stats:size:label\",\n        size / 1000000.0 as \"stats:size:value\",\n        'Approximate size of the table, calculated from a count of 1MB blocks'::text as \"stats:size:description\",\n        true as \"stats:size:include\",\n\n        'Disk Utilization' as \"stats:pct_used:label\",\n        pct_used / 100.0 as \"stats:pct_used:value\",\n        'Percent of available space that is used by the table.'::text as \"stats:pct_used:description\",\n        true as \"stats:pct_used:include\",\n\n        'Unsorted %' as \"stats:unsorted:label\",\n        unsorted / 100.0 as \"stats:unsorted:value\",\n        'Percent of unsorted rows in the table.'::text as \"stats:unsorted:description\",\n        (unsorted is not null) as \"stats:unsorted:include\",\n\n        'Stats Off' as \"stats:stats_off:label\",\n        stats_off as \"stats:stats_off:value\",\n        'Number that indicates how stale the table statistics are; 0 is current, 100 is out of date.'::text as \"stats:stats_off:description\",\n        true as \"stats:stats_off:include\",\n\n        'Approximate Row Count' as \"stats:rows:label\",\n        tbl_rows as \"stats:rows:value\",\n        'Approximate number of rows in the table. This value includes rows marked for deletion, but not yet vacuumed.'::text as \"stats:rows:description\",\n        true as \"stats:rows:include\",\n\n        'Sort Key Skew' as \"stats:skew_sortkey1:label\",\n        skew_sortkey1 as \"stats:skew_sortkey1:value\",\n        'Ratio of the size of the largest non-sort key column to the size of the first column of the sort key.'::text as \"stats:skew_sortkey1:description\",\n        (skew_sortkey1 is not null) as \"stats:skew_sortkey1:include\",\n\n        'Skew Rows' as \"stats:skew_rows:label\",\n        skew_rows as \"stats:skew_rows:value\",\n        'Ratio of the number of rows in the slice with the most rows to the number of rows in the slice with the fewest rows.'::text as \"stats:skew_rows:description\",\n        (skew_rows is not null) as \"stats:skew_rows:include\"\n\n    from svv_table_info\n\n  {%- endcall -%}\n\n  {{ return(load_result('extended_catalog').table) }}\n\n{% endmacro %}\n\n{% macro redshift__can_select_from(table_name) %}\n\n  {%- call statement('has_table_privilege', fetch_result=True) -%}\n\n    select has_table_privilege(current_user, '{{ table_name }}', 'SELECT') as can_select\n\n  {%- endcall -%}\n\n  {% set can_select = load_result('has_table_privilege').table[0]['can_select'] %}\n  {{ return(can_select) }}\n\n{% endmacro %}\n\n{% macro redshift__no_svv_table_info_warning() %}\n\n    {% set msg %}\n\n    Warning: The database user \"{{ target.user }}\" has insufficient permissions to\n    query the \"svv_table_info\" table. Please grant SELECT permissions on this table\n    to the \"{{ target.user }}\" user to fetch extended table details from Redshift.\n\n    {% endset %}\n\n    {{ log(msg, info=True) }}\n\n{% endmacro %}\n\n\n{% macro redshift__get_catalog() %}\n\n    {#-- Compute a left-outer join in memory. Some Redshift queries are\n      -- leader-only, and cannot be joined to other compute-based queries #}\n\n    {% set catalog = redshift__get_base_catalog() %}\n\n    {% set select_extended =  redshift__can_select_from('svv_table_info') %}\n    {% if select_extended %}\n        {% set extended_catalog = redshift__get_extended_catalog() %}\n        {% set catalog = catalog.join(extended_catalog, 'table_id') %}\n    {% else %}\n        {{ redshift__no_svv_table_info_warning() }}\n    {% endif %}\n\n    {{ return(catalog.exclude(['table_id'])) }}\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.handle_existing_table": {"original_file_path": "macros\\materializations\\view\\bq_snowflake_view.sql", "tags": [], "name": "handle_existing_table", "package_name": "dbt", "unique_id": "macro.dbt.handle_existing_table", "path": "macros\\materializations\\view\\bq_snowflake_view.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {{ adapter_macro(\"dbt.handle_existing_table\", full_refresh, non_destructive_mode, old_relation) }}\n{% endmacro %}\n\n{% macro default__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- endif -%}\n{% endmacro %}\n\n{% macro bigquery__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if full_refresh and not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- else -%}\n      {{ exceptions.relation_wrong_type(old_relation, 'view') }}\n    {%- endif -%}\n{% endmacro %}\n\n\n{# /*\n       Core materialization implementation. BigQuery and Snowflake are similar\n       because both can use `create or replace view` where the resulting view schema\n       is not necessarily the same as the existing view. On Redshift, this would\n       result in: ERROR:  cannot change number of columns in view\n\n       This implementation is superior to the create_temp, swap_with_existing, drop_old\n       paradigm because transactions don't run DDL queries atomically on Snowflake. By using\n       `create or replace view`, the materialization becomes atomic in nature.\n    */\n#}\n\n{% macro impl_view_materialization(run_outside_transaction_hooks=True) %}\n  {%- set identifier = model['alias'] -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n\n  {%- set old_relation = adapter.get_relation(\n      schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set target_relation = api.Relation.create(\n      identifier=identifier, schema=schema,\n      type='view') -%}\n\n  {%- set should_ignore = non_destructive_mode and exists_as_view %}\n  {%- set has_transactional_hooks = (hooks | selectattr('transaction', 'equalto', True) | list | length) > 0 %}\n\n  {% if run_outside_transaction_hooks %}\n      -- no transactions on BigQuery\n      {{ run_hooks(pre_hooks, inside_transaction=False) }}\n  {% endif %}\n\n  -- `BEGIN` happens here on Snowflake\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- If there's a table with the same name and we weren't told to full refresh,\n  -- that's an error. If we were told to full refresh, drop it. This behavior differs\n  -- for Snowflake and BigQuery, so multiple dispatch is used.\n  {%- if old_relation is not none and old_relation.is_table -%}\n    {{ handle_existing_table(flags.FULL_REFRESH, non_destructive_mode, old_relation) }}\n  {%- endif -%}\n\n  -- build model\n  {% if non_destructive_mode -%}\n    {% call noop_statement('main', status=\"PASS\", res=None) -%}\n      -- Not running : non-destructive mode\n      {{ sql }}\n    {%- endcall %}\n  {%- else -%}\n    {% call statement('main') -%}\n      {{ create_view_as(target_relation, sql) }}\n    {%- endcall %}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  {#\n      -- Don't commit in non-destructive mode _unless_ there are in-transaction hooks\n      -- TODO : Figure out some other way of doing this that isn't as fragile\n  #}\n  {% if has_transactional_hooks or not should_ignore %}\n      {{ adapter.commit() }}\n  {% endif %}\n\n  {% if run_outside_transaction_hooks %}\n      -- No transactions on BigQuery\n      {{ run_hooks(post_hooks, inside_transaction=False) }}\n  {% endif %}\n{% endmacro %}\n\n{% materialization view, adapter='bigquery' -%}\n    {{ impl_view_materialization(run_outside_transaction_hooks=False) }}\n{%- endmaterialization %}\n\n{% materialization view, adapter='snowflake' -%}\n    {{ impl_view_materialization() }}\n{%- endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.statement": {"original_file_path": "macros\\core.sql", "tags": [], "name": "statement", "package_name": "dbt", "unique_id": "macro.dbt.statement", "path": "macros\\core.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro statement(name=None, fetch_result=False, auto_begin=True) -%}\n  {%- if execute: -%}\n    {%- set sql = render(caller()) -%}\n\n    {%- if name == 'main' -%}\n      {{ log('Writing runtime SQL for node \"{}\"'.format(model['unique_id'])) }}\n      {{ write(sql) }}\n    {%- endif -%}\n\n    {%- set status, res = adapter.execute(sql, auto_begin=auto_begin, fetch=fetch_result) -%}\n    {%- if name is not none -%}\n      {{ store_result(name, status=status, agate_table=res) }}\n    {%- endif -%}\n\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro noop_statement(name=None, status=None, res=None) -%}\n  {%- set sql = render(caller()) -%}\n\n  {%- if name == 'main' -%}\n    {{ log('Writing runtime SQL for node \"{}\"'.format(model['unique_id'])) }}\n    {{ write(sql) }}\n  {%- endif -%}\n\n  {%- if name is not none -%}\n    {{ store_result(name, status=status, agate_table=res) }}\n  {%- endif -%}\n\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__create_csv_table": {"original_file_path": "macros\\materializations\\seed\\seed.sql", "tags": [], "name": "default__create_csv_table", "package_name": "dbt", "unique_id": "macro.dbt.default__create_csv_table", "path": "macros\\materializations\\seed\\seed.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro create_csv_table(model) -%}\n  {{ adapter_macro('create_csv_table', model) }}\n{%- endmacro %}\n\n{% macro reset_csv_table(model, full_refresh, old_relation) -%}\n  {{ adapter_macro('reset_csv_table', model, full_refresh, old_relation) }}\n{%- endmacro %}\n\n{% macro load_csv_rows(model) -%}\n  {{ adapter_macro('load_csv_rows', model) }}\n{%- endmacro %}\n\n{% macro default__create_csv_table(model) %}\n  {%- set agate_table = model['agate_table'] -%}\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n\n  {% set sql %}\n    create table {{ this.render(False) }} (\n        {%- for col_name in agate_table.column_names -%}\n            {%- set inferred_type = adapter.convert_type(agate_table, loop.index0) -%}\n            {%- set type = column_override.get(col_name, inferred_type) -%}\n            {{ col_name | string }} {{ type }} {%- if not loop.last -%}, {%- endif -%}\n        {%- endfor -%}\n    )\n  {% endset %}\n\n  {% call statement('_') -%}\n    {{ sql }}\n  {%- endcall %}\n\n  {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__reset_csv_table(model, full_refresh, old_relation) %}\n    {% set sql = \"\" %}\n    {% if full_refresh %}\n        {{ adapter.drop_relation(old_relation) }}\n        {% set sql = create_csv_table(model) %}\n    {% else %}\n        {{ adapter.truncate_relation(old_relation) }}\n        {% set sql = \"truncate table \" ~ old_relation %}\n    {% endif %}\n\n    {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__load_csv_rows(model) %}\n    {% set agate_table = model['agate_table'] %}\n    {% set cols_sql = \", \".join(agate_table.column_names) %}\n    {% set bindings = [] %}\n\n    {% set statements = [] %}\n\n    {% for chunk in agate_table.rows | batch(10000) %}\n        {% set bindings = [] %}\n\n        {% for row in chunk %}\n            {% set _ = bindings.extend(row) %}\n        {% endfor %}\n\n        {% set sql %}\n            insert into {{ this.render(False) }} ({{ cols_sql }}) values\n            {% for row in chunk -%}\n                ({%- for column in agate_table.column_names -%}\n                    %s\n                    {%- if not loop.last%},{%- endif %}\n                {%- endfor -%})\n                {%- if not loop.last%},{%- endif %}\n            {%- endfor %}\n        {% endset %}\n\n        {% set _ = adapter.add_query(sql, bindings=bindings, abridge_sql_log=True) %}\n\n        {% if loop.index0 == 0 %}\n            {% set _ = statements.append(sql) %}\n        {% endif %}\n    {% endfor %}\n\n    {# Return SQL so we can render it out into the compiled files #}\n    {{ return(statements[0]) }}\n{% endmacro %}\n\n\n{% materialization seed, default %}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set csv_table = model[\"agate_table\"] -%}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% set create_table_sql = \"\" %}\n  {% if exists_as_view %}\n    {{ exceptions.raise_compiler_error(\"Cannot seed to '{}', it is a view\".format(old_relation)) }}\n  {% elif exists_as_table %}\n    {% set create_table_sql = reset_csv_table(model, full_refresh_mode, old_relation) %}\n  {% else %}\n    {% set create_table_sql = create_csv_table(model) %}\n  {% endif %}\n\n  {% set status = 'CREATE' if full_refresh_mode else 'INSERT' %}\n  {% set num_rows = (csv_table.rows | length) %}\n  {% set sql = load_csv_rows(model) %}\n\n  {% call noop_statement('main', status ~ ' ' ~ num_rows) %}\n    {{ create_table_sql }};\n    -- dbt seed --\n    {{ sql }}\n  {% endcall %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__get_catalog": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "default__get_catalog", "package_name": "dbt", "unique_id": "macro.dbt.default__get_catalog", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__create_temporary_table": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "default__create_temporary_table", "package_name": "dbt", "unique_id": "macro.dbt.default__create_temporary_table", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.run_hooks": {"original_file_path": "macros\\materializations\\helpers.sql", "tags": [], "name": "run_hooks", "package_name": "dbt", "unique_id": "macro.dbt.run_hooks", "path": "macros\\materializations\\helpers.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro run_hooks(hooks, inside_transaction=True) %}\n  {% for hook in hooks | selectattr('transaction', 'equalto', inside_transaction)  %}\n    {% if not inside_transaction and loop.first %}\n      {% call statement(auto_begin=inside_transaction) %}\n        commit;\n      {% endcall %}\n    {% endif %}\n    {% call statement(auto_begin=inside_transaction) %}\n      {{ hook.get('sql') }}\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n\n{% macro column_list(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro column_list_for_create_table(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {{ col.data_type }} {%- if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro make_hook_config(sql, inside_transaction) %}\n    {{ tojson({\"sql\": sql, \"transaction\": inside_transaction}) }}\n{% endmacro %}\n\n\n{% macro before_begin(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n\n{% macro in_transaction(sql) %}\n    {{ make_hook_config(sql, inside_transaction=True) }}\n{% endmacro %}\n\n\n{% macro after_commit(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n{% macro drop_relation_if_exists(relation) %}\n  {% if relation is not none %}\n    {{ adapter.drop_relation(relation) }}\n  {% endif %}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.snowflake__create_table_as": {"original_file_path": "macros\\adapters\\snowflake.sql", "tags": [], "name": "snowflake__create_table_as", "package_name": "dbt", "unique_id": "macro.dbt.snowflake__create_table_as", "path": "macros\\adapters\\snowflake.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro snowflake__create_table_as(temporary, relation, sql) -%}\n  {% if temporary %}\n    use schema {{ adapter.quote_as_configured(schema, 'schema') }};\n  {% endif %}\n\n  {{ default__create_table_as(temporary, relation, sql) }}\n{% endmacro %}\n\n{% macro snowflake__create_view_as(relation, sql) -%}\n  create or replace view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__load_csv_rows": {"original_file_path": "macros\\materializations\\seed\\seed.sql", "tags": [], "name": "default__load_csv_rows", "package_name": "dbt", "unique_id": "macro.dbt.default__load_csv_rows", "path": "macros\\materializations\\seed\\seed.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro create_csv_table(model) -%}\n  {{ adapter_macro('create_csv_table', model) }}\n{%- endmacro %}\n\n{% macro reset_csv_table(model, full_refresh, old_relation) -%}\n  {{ adapter_macro('reset_csv_table', model, full_refresh, old_relation) }}\n{%- endmacro %}\n\n{% macro load_csv_rows(model) -%}\n  {{ adapter_macro('load_csv_rows', model) }}\n{%- endmacro %}\n\n{% macro default__create_csv_table(model) %}\n  {%- set agate_table = model['agate_table'] -%}\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n\n  {% set sql %}\n    create table {{ this.render(False) }} (\n        {%- for col_name in agate_table.column_names -%}\n            {%- set inferred_type = adapter.convert_type(agate_table, loop.index0) -%}\n            {%- set type = column_override.get(col_name, inferred_type) -%}\n            {{ col_name | string }} {{ type }} {%- if not loop.last -%}, {%- endif -%}\n        {%- endfor -%}\n    )\n  {% endset %}\n\n  {% call statement('_') -%}\n    {{ sql }}\n  {%- endcall %}\n\n  {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__reset_csv_table(model, full_refresh, old_relation) %}\n    {% set sql = \"\" %}\n    {% if full_refresh %}\n        {{ adapter.drop_relation(old_relation) }}\n        {% set sql = create_csv_table(model) %}\n    {% else %}\n        {{ adapter.truncate_relation(old_relation) }}\n        {% set sql = \"truncate table \" ~ old_relation %}\n    {% endif %}\n\n    {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__load_csv_rows(model) %}\n    {% set agate_table = model['agate_table'] %}\n    {% set cols_sql = \", \".join(agate_table.column_names) %}\n    {% set bindings = [] %}\n\n    {% set statements = [] %}\n\n    {% for chunk in agate_table.rows | batch(10000) %}\n        {% set bindings = [] %}\n\n        {% for row in chunk %}\n            {% set _ = bindings.extend(row) %}\n        {% endfor %}\n\n        {% set sql %}\n            insert into {{ this.render(False) }} ({{ cols_sql }}) values\n            {% for row in chunk -%}\n                ({%- for column in agate_table.column_names -%}\n                    %s\n                    {%- if not loop.last%},{%- endif %}\n                {%- endfor -%})\n                {%- if not loop.last%},{%- endif %}\n            {%- endfor %}\n        {% endset %}\n\n        {% set _ = adapter.add_query(sql, bindings=bindings, abridge_sql_log=True) %}\n\n        {% if loop.index0 == 0 %}\n            {% set _ = statements.append(sql) %}\n        {% endif %}\n    {% endfor %}\n\n    {# Return SQL so we can render it out into the compiled files #}\n    {{ return(statements[0]) }}\n{% endmacro %}\n\n\n{% materialization seed, default %}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set csv_table = model[\"agate_table\"] -%}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% set create_table_sql = \"\" %}\n  {% if exists_as_view %}\n    {{ exceptions.raise_compiler_error(\"Cannot seed to '{}', it is a view\".format(old_relation)) }}\n  {% elif exists_as_table %}\n    {% set create_table_sql = reset_csv_table(model, full_refresh_mode, old_relation) %}\n  {% else %}\n    {% set create_table_sql = create_csv_table(model) %}\n  {% endif %}\n\n  {% set status = 'CREATE' if full_refresh_mode else 'INSERT' %}\n  {% set num_rows = (csv_table.rows | length) %}\n  {% set sql = load_csv_rows(model) %}\n\n  {% call noop_statement('main', status ~ ' ' ~ num_rows) %}\n    {{ create_table_sql }};\n    -- dbt seed --\n    {{ sql }}\n  {% endcall %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.postgres__get_relations": {"original_file_path": "macros\\relations\\postgres_relations.sql", "tags": [], "name": "postgres__get_relations", "package_name": "dbt", "unique_id": "macro.dbt.postgres__get_relations", "path": "macros\\relations\\postgres_relations.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro postgres__get_relations () -%}\n  {%- call statement('relations', fetch_result=True) -%}\n    -- {#\n    -- in pg_depend, objid is the dependent, refobjid is the referenced object\n    -- \"a pg_depend entry indicates that the referenced object cannot be dropped without also dropping the dependent object.\"\n    -- #}\n    with relation as (\n        select\n            pg_rewrite.ev_class as class,\n            pg_rewrite.oid as id\n        from pg_rewrite\n    ),\n    class as (\n        select\n            oid as id,\n            relname as name,\n            relnamespace as schema,\n            relkind as kind\n        from pg_class\n    ),\n    dependency as (\n        select\n            pg_depend.objid as id,\n            pg_depend.refobjid as ref\n        from pg_depend\n    ),\n    schema as (\n        select\n            pg_namespace.oid as id,\n            pg_namespace.nspname as name\n        from pg_namespace\n        where nspname != 'information_schema' and nspname not like 'pg_%'\n    ),\n    relationships as (\n        select\n            referenced_class.name as referenced_name,\n            referenced_class.schema as referenced_schema_id,\n            dependent_class.name as dependent_name,\n            dependent_class.schema as dependent_schema_id,\n            referenced_class.kind as kind\n        from relation\n        join class as referenced_class on relation.class=referenced_class.id\n        join dependency on relation.id=dependency.id\n        join class as dependent_class on dependency.ref=dependent_class.id\n        where\n            referenced_class.kind in ('r', 'v') and\n            (referenced_class.name != dependent_class.name or\n             referenced_class.schema != dependent_class.schema)\n    )\n\n    select\n        referenced_schema.name as referenced_schema,\n        relationships.referenced_name as referenced_name,\n        dependent_schema.name as dependent_schema,\n        relationships.dependent_name as dependent_name\n    from relationships\n    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id\n    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id\n    group by referenced_schema, referenced_name, dependent_schema, dependent_name\n    order by referenced_schema, referenced_name, dependent_schema, dependent_name;\n  {%- endcall -%}\n\n  {{ return(load_result('relations').table) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__get_relations": {"original_file_path": "macros\\relations\\redshift_relations.sql", "tags": [], "name": "redshift__get_relations", "package_name": "dbt", "unique_id": "macro.dbt.redshift__get_relations", "path": "macros\\relations\\redshift_relations.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro redshift__get_relations () -%}\n  {{ return(dbt.postgres__get_relations()) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.snowflake__get_catalog": {"original_file_path": "macros\\catalog\\snowflake_catalog.sql", "tags": [], "name": "snowflake__get_catalog", "package_name": "dbt", "unique_id": "macro.dbt.snowflake__get_catalog", "path": "macros\\catalog\\snowflake_catalog.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro snowflake__get_catalog() -%}\n\n    {%- call statement('catalog', fetch_result=True) -%}\n\n    with tables as (\n\n        select\n            table_schema as \"table_schema\",\n            table_name as \"table_name\",\n            table_type as \"table_type\",\n\n            -- note: this is the _role_ that owns the table\n            table_owner as \"table_owner\",\n\n            'Clustering Key' as \"stats:clustering_key:label\",\n            clustering_key as \"stats:clustering_key:value\",\n            'The key used to cluster this table' as \"stats:clustering_key:description\",\n            (clustering_key is not null) as \"stats:clustering_key:include\",\n\n            'Row Count' as \"stats:row_count:label\",\n            row_count as \"stats:row_count:value\",\n            'An approximate count of rows in this table' as \"stats:row_count:description\",\n            (row_count is not null) as \"stats:row_count:include\",\n\n            'Approximate Size' as \"stats:bytes:label\",\n            bytes as \"stats:bytes:value\",\n            'Approximate size of the table as reported by Snowflake' as \"stats:bytes:description\",\n            (bytes is not null) as \"stats:bytes:include\"\n\n        from information_schema.tables\n\n    ),\n\n    columns as (\n\n        select\n\n            table_schema as \"table_schema\",\n            table_name as \"table_name\",\n            null as \"table_comment\",\n\n            column_name as \"column_name\",\n            ordinal_position as \"column_index\",\n            data_type as \"column_type\",\n            null as \"column_comment\"\n\n        from information_schema.columns\n\n    )\n\n    select *\n    from tables\n    join columns using (\"table_schema\", \"table_name\")\n    where \"table_schema\" != 'INFORMATION_SCHEMA'\n    order by \"column_index\"\n\n  {%- endcall -%}\n\n  {{ return(load_result('catalog').table) }}\n\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.convert_datetime": {"original_file_path": "macros\\etc\\datetime.sql", "tags": [], "name": "convert_datetime", "package_name": "dbt", "unique_id": "macro.dbt.convert_datetime", "path": "macros\\etc\\datetime.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro convert_datetime(date_str, date_fmt) %}\n\n  {% set error_msg -%}\n      The provided partition date '{{ date_str }}' does not match the expected format '{{ date_fmt }}'\n  {%- endset %}\n\n  {% set res = try_or_compiler_error(error_msg, modules.datetime.datetime.strptime, date_str.strip(), date_fmt) %}\n  {{ return(res) }}\n\n{% endmacro %}\n\n{% macro dates_in_range(start_date_str, end_date_str=none, in_fmt=\"%Y%m%d\", out_fmt=\"%Y%m%d\") %}\n    {% set end_date_str = start_date_str if end_date_str is none else end_date_str %}\n\n    {% set start_date = convert_datetime(start_date_str, in_fmt) %}\n    {% set end_date = convert_datetime(end_date_str, in_fmt) %}\n\n    {% set day_count = (end_date - start_date).days %}\n    {% if day_count < 0 %}\n        {% set msg -%}\n            Partiton start date is after the end date ({{ start_date }}, {{ end_date }})\n        {%- endset %}\n\n        {{ exceptions.raise_compiler_error(msg, model) }}\n    {% endif %}\n\n    {% set date_list = [] %}\n    {% for i in range(0, day_count + 1) %}\n        {% set the_date = (modules.datetime.timedelta(days=i) + start_date) %}\n        {% if not out_fmt %}\n            {% set _ = date_list.append(the_date) %}\n        {% else %}\n            {% set _ = date_list.append(the_date.strftime(out_fmt)) %}\n        {% endif %}\n    {% endfor %}\n\n    {{ return(date_list) }}\n{% endmacro %}\n\n{% macro partition_range(raw_partition_date, date_fmt='%Y%m%d') %}\n    {% set partition_range = (raw_partition_date | string).split(\",\") %}\n\n    {% if (partition_range | length) == 1 %}\n      {% set start_date = partition_range[0] %}\n      {% set end_date = none %}\n    {% elif (partition_range | length) == 2 %}\n      {% set start_date = partition_range[0] %}\n      {% set end_date = partition_range[1] %}\n    {% else %}\n      {{ dbt.exceptions.raise_compiler_error(\"Invalid partition time. Expected format: {Start Date}[,{End Date}]. Got: \" ~ raw_partition_date) }}\n    {% endif %}\n\n    {{ return(dates_in_range(start_date, end_date, in_fmt=date_fmt)) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_view_bigquery": {"original_file_path": "macros\\materializations\\view\\bq_snowflake_view.sql", "tags": [], "name": "materialization_view_bigquery", "package_name": "dbt", "unique_id": "macro.dbt.materialization_view_bigquery", "path": "macros\\materializations\\view\\bq_snowflake_view.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {{ adapter_macro(\"dbt.handle_existing_table\", full_refresh, non_destructive_mode, old_relation) }}\n{% endmacro %}\n\n{% macro default__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- endif -%}\n{% endmacro %}\n\n{% macro bigquery__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if full_refresh and not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- else -%}\n      {{ exceptions.relation_wrong_type(old_relation, 'view') }}\n    {%- endif -%}\n{% endmacro %}\n\n\n{# /*\n       Core materialization implementation. BigQuery and Snowflake are similar\n       because both can use `create or replace view` where the resulting view schema\n       is not necessarily the same as the existing view. On Redshift, this would\n       result in: ERROR:  cannot change number of columns in view\n\n       This implementation is superior to the create_temp, swap_with_existing, drop_old\n       paradigm because transactions don't run DDL queries atomically on Snowflake. By using\n       `create or replace view`, the materialization becomes atomic in nature.\n    */\n#}\n\n{% macro impl_view_materialization(run_outside_transaction_hooks=True) %}\n  {%- set identifier = model['alias'] -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n\n  {%- set old_relation = adapter.get_relation(\n      schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set target_relation = api.Relation.create(\n      identifier=identifier, schema=schema,\n      type='view') -%}\n\n  {%- set should_ignore = non_destructive_mode and exists_as_view %}\n  {%- set has_transactional_hooks = (hooks | selectattr('transaction', 'equalto', True) | list | length) > 0 %}\n\n  {% if run_outside_transaction_hooks %}\n      -- no transactions on BigQuery\n      {{ run_hooks(pre_hooks, inside_transaction=False) }}\n  {% endif %}\n\n  -- `BEGIN` happens here on Snowflake\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- If there's a table with the same name and we weren't told to full refresh,\n  -- that's an error. If we were told to full refresh, drop it. This behavior differs\n  -- for Snowflake and BigQuery, so multiple dispatch is used.\n  {%- if old_relation is not none and old_relation.is_table -%}\n    {{ handle_existing_table(flags.FULL_REFRESH, non_destructive_mode, old_relation) }}\n  {%- endif -%}\n\n  -- build model\n  {% if non_destructive_mode -%}\n    {% call noop_statement('main', status=\"PASS\", res=None) -%}\n      -- Not running : non-destructive mode\n      {{ sql }}\n    {%- endcall %}\n  {%- else -%}\n    {% call statement('main') -%}\n      {{ create_view_as(target_relation, sql) }}\n    {%- endcall %}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  {#\n      -- Don't commit in non-destructive mode _unless_ there are in-transaction hooks\n      -- TODO : Figure out some other way of doing this that isn't as fragile\n  #}\n  {% if has_transactional_hooks or not should_ignore %}\n      {{ adapter.commit() }}\n  {% endif %}\n\n  {% if run_outside_transaction_hooks %}\n      -- No transactions on BigQuery\n      {{ run_hooks(post_hooks, inside_transaction=False) }}\n  {% endif %}\n{% endmacro %}\n\n{% materialization view, adapter='bigquery' -%}\n    {{ impl_view_materialization(run_outside_transaction_hooks=False) }}\n{%- endmaterialization %}\n\n{% materialization view, adapter='snowflake' -%}\n    {{ impl_view_materialization() }}\n{%- endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__get_merge_sql": {"original_file_path": "macros\\materializations\\common\\merge.sql", "tags": [], "name": "default__get_merge_sql", "package_name": "dbt", "unique_id": "macro.dbt.default__get_merge_sql", "path": "macros\\materializations\\common\\merge.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro get_merge_sql(target, source, unique_key, dest_columns) -%}\n  {{ adapter_macro('get_merge_sql', target, source, unique_key, dest_columns) }}\n{%- endmacro %}\n\n{% macro default__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {%- set dest_cols_csv = dest_columns | map(attribute=\"name\") | join(', ') -%}\n\n    merge into {{ target }} as DBT_INTERNAL_DEST\n    using {{ source }} as DBT_INTERNAL_SOURCE\n\n    {% if unique_key %}\n        on DBT_INTERNAL_SOURCE.{{ unique_key }} = DBT_INTERNAL_DEST.{{ unique_key }}\n    {% else %}\n        on FALSE\n    {% endif %}\n\n    {% if unique_key %}\n    when matched then update set\n        {% for column in dest_columns -%}\n            {{ column.name }} = DBT_INTERNAL_SOURCE.{{ column.name }}\n            {%- if not loop.last %}, {%- endif %}\n        {%- endfor %}\n    {% endif %}\n\n    when not matched then insert\n        ({{ dest_cols_csv }})\n    values\n        ({{ dest_cols_csv }})\n\n{% endmacro %}\n\n{% macro redshift__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {{ exceptions.raise_compiler_error('get_merge_sql is not implemented for Redshift') }}\n{% endmacro %}\n\n{% macro postgres__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {{ exceptions.raise_compiler_error('get_merge_sql is not implemented for Postgres') }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.get_merge_sql": {"original_file_path": "macros\\materializations\\common\\merge.sql", "tags": [], "name": "get_merge_sql", "package_name": "dbt", "unique_id": "macro.dbt.get_merge_sql", "path": "macros\\materializations\\common\\merge.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro get_merge_sql(target, source, unique_key, dest_columns) -%}\n  {{ adapter_macro('get_merge_sql', target, source, unique_key, dest_columns) }}\n{%- endmacro %}\n\n{% macro default__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {%- set dest_cols_csv = dest_columns | map(attribute=\"name\") | join(', ') -%}\n\n    merge into {{ target }} as DBT_INTERNAL_DEST\n    using {{ source }} as DBT_INTERNAL_SOURCE\n\n    {% if unique_key %}\n        on DBT_INTERNAL_SOURCE.{{ unique_key }} = DBT_INTERNAL_DEST.{{ unique_key }}\n    {% else %}\n        on FALSE\n    {% endif %}\n\n    {% if unique_key %}\n    when matched then update set\n        {% for column in dest_columns -%}\n            {{ column.name }} = DBT_INTERNAL_SOURCE.{{ column.name }}\n            {%- if not loop.last %}, {%- endif %}\n        {%- endfor %}\n    {% endif %}\n\n    when not matched then insert\n        ({{ dest_cols_csv }})\n    values\n        ({{ dest_cols_csv }})\n\n{% endmacro %}\n\n{% macro redshift__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {{ exceptions.raise_compiler_error('get_merge_sql is not implemented for Redshift') }}\n{% endmacro %}\n\n{% macro postgres__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {{ exceptions.raise_compiler_error('get_merge_sql is not implemented for Postgres') }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_seed_default": {"original_file_path": "macros\\materializations\\seed\\seed.sql", "tags": [], "name": "materialization_seed_default", "package_name": "dbt", "unique_id": "macro.dbt.materialization_seed_default", "path": "macros\\materializations\\seed\\seed.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro create_csv_table(model) -%}\n  {{ adapter_macro('create_csv_table', model) }}\n{%- endmacro %}\n\n{% macro reset_csv_table(model, full_refresh, old_relation) -%}\n  {{ adapter_macro('reset_csv_table', model, full_refresh, old_relation) }}\n{%- endmacro %}\n\n{% macro load_csv_rows(model) -%}\n  {{ adapter_macro('load_csv_rows', model) }}\n{%- endmacro %}\n\n{% macro default__create_csv_table(model) %}\n  {%- set agate_table = model['agate_table'] -%}\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n\n  {% set sql %}\n    create table {{ this.render(False) }} (\n        {%- for col_name in agate_table.column_names -%}\n            {%- set inferred_type = adapter.convert_type(agate_table, loop.index0) -%}\n            {%- set type = column_override.get(col_name, inferred_type) -%}\n            {{ col_name | string }} {{ type }} {%- if not loop.last -%}, {%- endif -%}\n        {%- endfor -%}\n    )\n  {% endset %}\n\n  {% call statement('_') -%}\n    {{ sql }}\n  {%- endcall %}\n\n  {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__reset_csv_table(model, full_refresh, old_relation) %}\n    {% set sql = \"\" %}\n    {% if full_refresh %}\n        {{ adapter.drop_relation(old_relation) }}\n        {% set sql = create_csv_table(model) %}\n    {% else %}\n        {{ adapter.truncate_relation(old_relation) }}\n        {% set sql = \"truncate table \" ~ old_relation %}\n    {% endif %}\n\n    {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__load_csv_rows(model) %}\n    {% set agate_table = model['agate_table'] %}\n    {% set cols_sql = \", \".join(agate_table.column_names) %}\n    {% set bindings = [] %}\n\n    {% set statements = [] %}\n\n    {% for chunk in agate_table.rows | batch(10000) %}\n        {% set bindings = [] %}\n\n        {% for row in chunk %}\n            {% set _ = bindings.extend(row) %}\n        {% endfor %}\n\n        {% set sql %}\n            insert into {{ this.render(False) }} ({{ cols_sql }}) values\n            {% for row in chunk -%}\n                ({%- for column in agate_table.column_names -%}\n                    %s\n                    {%- if not loop.last%},{%- endif %}\n                {%- endfor -%})\n                {%- if not loop.last%},{%- endif %}\n            {%- endfor %}\n        {% endset %}\n\n        {% set _ = adapter.add_query(sql, bindings=bindings, abridge_sql_log=True) %}\n\n        {% if loop.index0 == 0 %}\n            {% set _ = statements.append(sql) %}\n        {% endif %}\n    {% endfor %}\n\n    {# Return SQL so we can render it out into the compiled files #}\n    {{ return(statements[0]) }}\n{% endmacro %}\n\n\n{% materialization seed, default %}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set csv_table = model[\"agate_table\"] -%}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% set create_table_sql = \"\" %}\n  {% if exists_as_view %}\n    {{ exceptions.raise_compiler_error(\"Cannot seed to '{}', it is a view\".format(old_relation)) }}\n  {% elif exists_as_table %}\n    {% set create_table_sql = reset_csv_table(model, full_refresh_mode, old_relation) %}\n  {% else %}\n    {% set create_table_sql = create_csv_table(model) %}\n  {% endif %}\n\n  {% set status = 'CREATE' if full_refresh_mode else 'INSERT' %}\n  {% set num_rows = (csv_table.rows | length) %}\n  {% set sql = load_csv_rows(model) %}\n\n  {% call noop_statement('main', status ~ ' ' ~ num_rows) %}\n    {{ create_table_sql }};\n    -- dbt seed --\n    {{ sql }}\n  {% endcall %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.test_unique": {"original_file_path": "macros\\schema_tests\\unique.sql", "tags": [], "name": "test_unique", "package_name": "dbt", "unique_id": "macro.dbt.test_unique", "path": "macros\\schema_tests\\unique.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro test_unique(model) %}\n\n{% set column_name = kwargs.get('column_name', kwargs.get('arg')) %}\n\nselect count(*)\nfrom (\n\n    select\n        {{ column_name }}\n\n    from {{ model }}\n    where {{ column_name }} is not null\n    group by {{ column_name }}\n    having count(*) > 1\n\n) validation_errors\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.dates_in_range": {"original_file_path": "macros\\etc\\datetime.sql", "tags": [], "name": "dates_in_range", "package_name": "dbt", "unique_id": "macro.dbt.dates_in_range", "path": "macros\\etc\\datetime.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro convert_datetime(date_str, date_fmt) %}\n\n  {% set error_msg -%}\n      The provided partition date '{{ date_str }}' does not match the expected format '{{ date_fmt }}'\n  {%- endset %}\n\n  {% set res = try_or_compiler_error(error_msg, modules.datetime.datetime.strptime, date_str.strip(), date_fmt) %}\n  {{ return(res) }}\n\n{% endmacro %}\n\n{% macro dates_in_range(start_date_str, end_date_str=none, in_fmt=\"%Y%m%d\", out_fmt=\"%Y%m%d\") %}\n    {% set end_date_str = start_date_str if end_date_str is none else end_date_str %}\n\n    {% set start_date = convert_datetime(start_date_str, in_fmt) %}\n    {% set end_date = convert_datetime(end_date_str, in_fmt) %}\n\n    {% set day_count = (end_date - start_date).days %}\n    {% if day_count < 0 %}\n        {% set msg -%}\n            Partiton start date is after the end date ({{ start_date }}, {{ end_date }})\n        {%- endset %}\n\n        {{ exceptions.raise_compiler_error(msg, model) }}\n    {% endif %}\n\n    {% set date_list = [] %}\n    {% for i in range(0, day_count + 1) %}\n        {% set the_date = (modules.datetime.timedelta(days=i) + start_date) %}\n        {% if not out_fmt %}\n            {% set _ = date_list.append(the_date) %}\n        {% else %}\n            {% set _ = date_list.append(the_date.strftime(out_fmt)) %}\n        {% endif %}\n    {% endfor %}\n\n    {{ return(date_list) }}\n{% endmacro %}\n\n{% macro partition_range(raw_partition_date, date_fmt='%Y%m%d') %}\n    {% set partition_range = (raw_partition_date | string).split(\",\") %}\n\n    {% if (partition_range | length) == 1 %}\n      {% set start_date = partition_range[0] %}\n      {% set end_date = none %}\n    {% elif (partition_range | length) == 2 %}\n      {% set start_date = partition_range[0] %}\n      {% set end_date = partition_range[1] %}\n    {% else %}\n      {{ dbt.exceptions.raise_compiler_error(\"Invalid partition time. Expected format: {Start Date}[,{End Date}]. Got: \" ~ raw_partition_date) }}\n    {% endif %}\n\n    {{ return(dates_in_range(start_date, end_date, in_fmt=date_fmt)) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__create_temporary_table": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "bigquery__create_temporary_table", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__create_temporary_table", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__get_catalog": {"original_file_path": "macros\\catalog\\redshift_catalog.sql", "tags": [], "name": "redshift__get_catalog", "package_name": "dbt", "unique_id": "macro.dbt.redshift__get_catalog", "path": "macros\\catalog\\redshift_catalog.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro redshift__get_base_catalog() -%}\n  {%- call statement('base_catalog', fetch_result=True) -%}\n    with late_binding as (\n      select\n        table_schema,\n        table_name,\n        'LATE BINDING VIEW'::varchar as table_type,\n        null::text as table_comment,\n\n        column_name,\n        column_index,\n        column_type,\n        null::text as column_comment\n      from pg_get_late_binding_view_cols()\n        cols(table_schema name, table_name name, column_name name,\n             column_type varchar,\n             column_index int)\n        order by \"column_index\"\n    ),\n\n    tables as (\n\n      select\n          table_schema,\n          table_name,\n          table_type\n\n      from information_schema.tables\n\n    ),\n\n    table_owners as (\n\n        select\n            schemaname as table_schema,\n            tablename as table_name,\n            tableowner as table_owner\n\n        from pg_tables\n\n        union all\n\n        select\n            schemaname as table_schema,\n            viewname as table_name,\n            viewowner as table_owner\n\n        from pg_views\n\n    ),\n\n    columns as (\n\n        select\n            table_schema,\n            table_name,\n            null::varchar as table_comment,\n\n            column_name,\n            ordinal_position as column_index,\n            data_type as column_type,\n            null::varchar as column_comment\n\n\n        from information_schema.columns\n\n    ),\n\n    unioned as (\n\n        select *\n        from tables\n        join columns using (table_schema, table_name)\n\n        union all\n\n        select *\n        from late_binding\n\n    )\n\n    select *,\n        table_schema || '.' || table_name as table_id\n\n    from unioned\n    join table_owners using (table_schema, table_name)\n\n    where table_schema != 'information_schema'\n      and table_schema not like 'pg_%'\n\n    order by \"column_index\"\n  {%- endcall -%}\n\n  {{ return(load_result('base_catalog').table) }}\n{%- endmacro %}\n\n{% macro redshift__get_extended_catalog() %}\n  {%- call statement('extended_catalog', fetch_result=True) -%}\n\n    select\n        \"schema\" || '.' || \"table\" as table_id,\n\n        'Encoded'::text as \"stats:encoded:label\",\n        encoded as \"stats:encoded:value\",\n        'Indicates whether any column in the table has compression encoding defined.'::text as \"stats:encoded:description\",\n        true as \"stats:encoded:include\",\n\n        'Dist Style' as \"stats:diststyle:label\",\n        diststyle as \"stats:diststyle:value\",\n        'Distribution style or distribution key column, if key distribution is defined.'::text as \"stats:diststyle:description\",\n        true as \"stats:diststyle:include\",\n\n        'Sort Key 1' as \"stats:sortkey1:label\",\n        -- handle 0xFF byte in response for interleaved sort styles\n        case\n            when sortkey1 like 'INTERLEAVED%' then 'INTERLEAVED'::text\n            else sortkey1\n        end as \"stats:sortkey1:value\",\n        'First column in the sort key.'::text as \"stats:sortkey1:description\",\n        (sortkey1 is not null) as \"stats:sortkey1:include\",\n\n        'Max Varchar' as \"stats:max_varchar:label\",\n        max_varchar as \"stats:max_varchar:value\",\n        'Size of the largest column that uses a VARCHAR data type.'::text as \"stats:max_varchar:description\",\n        true as \"stats:max_varchar:include\",\n\n        -- exclude this, as the data is strangely returned with null-byte characters\n        'Sort Key 1 Encoding' as \"stats:sortkey1_enc:label\",\n        sortkey1_enc as \"stats:sortkey1_enc:value\",\n        'Compression encoding of the first column in the sort key.' as \"stats:sortkey1_enc:description\",\n        false as \"stats:sortkey1_enc:include\",\n\n        '# Sort Keys' as \"stats:sortkey_num:label\",\n        sortkey_num as \"stats:sortkey_num:value\",\n        'Number of columns defined as sort keys.' as \"stats:sortkey_num:description\",\n        (sortkey_num > 0) as \"stats:sortkey_num:include\",\n\n        'Approximate Size' as \"stats:size:label\",\n        size / 1000000.0 as \"stats:size:value\",\n        'Approximate size of the table, calculated from a count of 1MB blocks'::text as \"stats:size:description\",\n        true as \"stats:size:include\",\n\n        'Disk Utilization' as \"stats:pct_used:label\",\n        pct_used / 100.0 as \"stats:pct_used:value\",\n        'Percent of available space that is used by the table.'::text as \"stats:pct_used:description\",\n        true as \"stats:pct_used:include\",\n\n        'Unsorted %' as \"stats:unsorted:label\",\n        unsorted / 100.0 as \"stats:unsorted:value\",\n        'Percent of unsorted rows in the table.'::text as \"stats:unsorted:description\",\n        (unsorted is not null) as \"stats:unsorted:include\",\n\n        'Stats Off' as \"stats:stats_off:label\",\n        stats_off as \"stats:stats_off:value\",\n        'Number that indicates how stale the table statistics are; 0 is current, 100 is out of date.'::text as \"stats:stats_off:description\",\n        true as \"stats:stats_off:include\",\n\n        'Approximate Row Count' as \"stats:rows:label\",\n        tbl_rows as \"stats:rows:value\",\n        'Approximate number of rows in the table. This value includes rows marked for deletion, but not yet vacuumed.'::text as \"stats:rows:description\",\n        true as \"stats:rows:include\",\n\n        'Sort Key Skew' as \"stats:skew_sortkey1:label\",\n        skew_sortkey1 as \"stats:skew_sortkey1:value\",\n        'Ratio of the size of the largest non-sort key column to the size of the first column of the sort key.'::text as \"stats:skew_sortkey1:description\",\n        (skew_sortkey1 is not null) as \"stats:skew_sortkey1:include\",\n\n        'Skew Rows' as \"stats:skew_rows:label\",\n        skew_rows as \"stats:skew_rows:value\",\n        'Ratio of the number of rows in the slice with the most rows to the number of rows in the slice with the fewest rows.'::text as \"stats:skew_rows:description\",\n        (skew_rows is not null) as \"stats:skew_rows:include\"\n\n    from svv_table_info\n\n  {%- endcall -%}\n\n  {{ return(load_result('extended_catalog').table) }}\n\n{% endmacro %}\n\n{% macro redshift__can_select_from(table_name) %}\n\n  {%- call statement('has_table_privilege', fetch_result=True) -%}\n\n    select has_table_privilege(current_user, '{{ table_name }}', 'SELECT') as can_select\n\n  {%- endcall -%}\n\n  {% set can_select = load_result('has_table_privilege').table[0]['can_select'] %}\n  {{ return(can_select) }}\n\n{% endmacro %}\n\n{% macro redshift__no_svv_table_info_warning() %}\n\n    {% set msg %}\n\n    Warning: The database user \"{{ target.user }}\" has insufficient permissions to\n    query the \"svv_table_info\" table. Please grant SELECT permissions on this table\n    to the \"{{ target.user }}\" user to fetch extended table details from Redshift.\n\n    {% endset %}\n\n    {{ log(msg, info=True) }}\n\n{% endmacro %}\n\n\n{% macro redshift__get_catalog() %}\n\n    {#-- Compute a left-outer join in memory. Some Redshift queries are\n      -- leader-only, and cannot be joined to other compute-based queries #}\n\n    {% set catalog = redshift__get_base_catalog() %}\n\n    {% set select_extended =  redshift__can_select_from('svv_table_info') %}\n    {% if select_extended %}\n        {% set extended_catalog = redshift__get_extended_catalog() %}\n        {% set catalog = catalog.join(extended_catalog, 'table_id') %}\n    {% else %}\n        {{ redshift__no_svv_table_info_warning() }}\n    {% endif %}\n\n    {{ return(catalog.exclude(['table_id'])) }}\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.generate_schema_name_for_env": {"original_file_path": "macros\\etc\\get_custom_schema.sql", "tags": [], "name": "generate_schema_name_for_env", "package_name": "dbt", "unique_id": "macro.dbt.generate_schema_name_for_env", "path": "macros\\etc\\get_custom_schema.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Renders a schema name given a custom schema name. If the custom\n    schema name is none, then the resulting schema is just the \"schema\"\n    value in the specified target. If a schema override is specified, then\n    the resulting schema is the default schema concatenated with the \n    custom schema.\n\n    This macro can be overriden in projects to define different semantics\n    for rendering a schema name.\n\n    Arguments:\n    custom_schema_name: The custom schema name specified for a model, or none\n\n#}\n{% macro generate_schema_name(custom_schema_name=none) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- else -%}\n\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n\n\n{#\n    Renders a schema name given a custom schema name. In production, this macro\n    will render out the overriden schema name for a model. Otherwise, the default\n    schema specified in the active target is used.\n\n    Arguments:\n    custom_schema_name: The custom schema name specified for a model, or none\n\n#}\n{% macro generate_schema_name_for_env(custom_schema_name=none) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if target.name == 'prod' and custom_schema_name is not none -%}\n\n        {{ custom_schema_name | trim }}\n\n    {%- else -%}\n\n        {{ default_schema }}\n\n    {%- endif -%}\n\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "operation.dbt.get_relations_data": {"original_file_path": "macros\\operations\\relations\\get_relations.sql", "tags": [], "name": "get_relations_data", "package_name": "dbt", "unique_id": "operation.dbt.get_relations_data", "path": "macros\\operations\\relations\\get_relations.sql", "depends_on": {"macros": []}, "resource_type": "operation", "raw_sql": "{% operation get_relations_data %}\n    {% set relations = dbt.get_relations() %}\n    {{ return(relations) }}\n{% endoperation %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.test_not_null": {"original_file_path": "macros\\schema_tests\\not_null.sql", "tags": [], "name": "test_not_null", "package_name": "dbt", "unique_id": "macro.dbt.test_not_null", "path": "macros\\schema_tests\\not_null.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro test_not_null(model) %}\n\n{% set column_name = kwargs.get('column_name', kwargs.get('arg')) %}\n\nselect count(*)\nfrom {{ model }}\nwhere {{ column_name }} is null\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.partition_by": {"original_file_path": "macros\\adapters\\bigquery.sql", "tags": [], "name": "partition_by", "package_name": "dbt", "unique_id": "macro.dbt.partition_by", "path": "macros\\adapters\\bigquery.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro partition_by(raw_partition_by) %}\n  {%- if raw_partition_by is none -%}\n    {{ return('') }}\n  {% endif %}\n\n  {% set partition_by_clause %}\n    partition by {{ raw_partition_by }}\n  {%- endset -%}\n\n  {{ return(partition_by_clause) }}\n{%- endmacro -%}\n\n\n{% macro cluster_by(raw_cluster_by) %}\n  {%- if raw_cluster_by is not none -%}\n  cluster by \n  {% if raw_cluster_by is string -%}\n    {% set raw_cluster_by = [raw_cluster_by] %}\n  {%- endif -%}\n  {%- for cluster in raw_cluster_by -%}\n    {{ cluster }}\n    {%- if not loop.last -%},{%- endif -%}\n  {%- endfor -%}\n\n  {% endif %}\n\n{%- endmacro -%}\n\n\n{% macro bigquery__create_table_as(temporary, relation, sql) -%}\n  {%- set raw_partition_by = config.get('partition_by', none) -%}\n  {%- set raw_cluster_by = config.get('cluster_by', none) -%}\n\n  create or replace table {{ relation }}\n  {{ partition_by(raw_partition_by) }}\n  {{ cluster_by(raw_cluster_by) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro bigquery__create_view_as(relation, sql) -%}\n  create or replace view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.partition_range": {"original_file_path": "macros\\etc\\datetime.sql", "tags": [], "name": "partition_range", "package_name": "dbt", "unique_id": "macro.dbt.partition_range", "path": "macros\\etc\\datetime.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro convert_datetime(date_str, date_fmt) %}\n\n  {% set error_msg -%}\n      The provided partition date '{{ date_str }}' does not match the expected format '{{ date_fmt }}'\n  {%- endset %}\n\n  {% set res = try_or_compiler_error(error_msg, modules.datetime.datetime.strptime, date_str.strip(), date_fmt) %}\n  {{ return(res) }}\n\n{% endmacro %}\n\n{% macro dates_in_range(start_date_str, end_date_str=none, in_fmt=\"%Y%m%d\", out_fmt=\"%Y%m%d\") %}\n    {% set end_date_str = start_date_str if end_date_str is none else end_date_str %}\n\n    {% set start_date = convert_datetime(start_date_str, in_fmt) %}\n    {% set end_date = convert_datetime(end_date_str, in_fmt) %}\n\n    {% set day_count = (end_date - start_date).days %}\n    {% if day_count < 0 %}\n        {% set msg -%}\n            Partiton start date is after the end date ({{ start_date }}, {{ end_date }})\n        {%- endset %}\n\n        {{ exceptions.raise_compiler_error(msg, model) }}\n    {% endif %}\n\n    {% set date_list = [] %}\n    {% for i in range(0, day_count + 1) %}\n        {% set the_date = (modules.datetime.timedelta(days=i) + start_date) %}\n        {% if not out_fmt %}\n            {% set _ = date_list.append(the_date) %}\n        {% else %}\n            {% set _ = date_list.append(the_date.strftime(out_fmt)) %}\n        {% endif %}\n    {% endfor %}\n\n    {{ return(date_list) }}\n{% endmacro %}\n\n{% macro partition_range(raw_partition_date, date_fmt='%Y%m%d') %}\n    {% set partition_range = (raw_partition_date | string).split(\",\") %}\n\n    {% if (partition_range | length) == 1 %}\n      {% set start_date = partition_range[0] %}\n      {% set end_date = none %}\n    {% elif (partition_range | length) == 2 %}\n      {% set start_date = partition_range[0] %}\n      {% set end_date = partition_range[1] %}\n    {% else %}\n      {{ dbt.exceptions.raise_compiler_error(\"Invalid partition time. Expected format: {Start Date}[,{End Date}]. Got: \" ~ raw_partition_date) }}\n    {% endif %}\n\n    {{ return(dates_in_range(start_date, end_date, in_fmt=date_fmt)) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.archive_select": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "archive_select", "package_name": "dbt", "unique_id": "macro.dbt.archive_select", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__archive_scd_hash": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "bigquery__archive_scd_hash", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__archive_scd_hash", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_view_default": {"original_file_path": "macros\\materializations\\view\\view.sql", "tags": [], "name": "materialization_view_default", "package_name": "dbt", "unique_id": "macro.dbt.materialization_view_default", "path": "macros\\materializations\\view\\view.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{%- materialization view, default -%}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_tmp' -%}\n  {%- set backup_identifier = identifier + '__dbt_backup' -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n  {%- set target_relation = api.Relation.create(identifier=identifier, schema=schema,\n                                                type='view') -%}\n  {%- set intermediate_relation = api.Relation.create(identifier=tmp_identifier,\n                                                      schema=schema, type='view') -%}\n\n  /*\n     This relation (probably) doesn't exist yet. If it does exist, it's a leftover from\n     a previous run, and we're going to try to drop it immediately. At the end of this\n     materialization, we're going to rename the \"old_relation\" to this identifier,\n     and then we're going to drop it. In order to make sure we run the correct one of:\n       - drop view ...\n       - drop table ...\n\n     We need to set the type of this relation to be the type of the old_relation, if it exists,\n     or else \"view\" as a sane default if it does not. Note that if the old_relation does not\n     exist, then there is nothing to move out of the way and subsequentally drop. In that case,\n     this relation will be effectively unused.\n  */\n  {%- set backup_relation = api.Relation.create(identifier=backup_identifier,\n                                                schema=schema, type=(old_relation.type or 'view')) -%}\n\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set has_transactional_hooks = (hooks | selectattr('transaction', 'equalto', True) | list | length) > 0 %}\n  {%- set should_ignore = non_destructive_mode and exists_as_view %}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- drop the temp relations if they exists for some reason\n  {{ adapter.drop_relation(intermediate_relation) }}\n  {{ adapter.drop_relation(backup_relation) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% if should_ignore -%}\n    {#\n      -- Materializations need to a statement with name='main'.\n      -- We could issue a no-op query here (like `select 1`), but that's wasteful. Instead:\n      --   1) write the sql contents out to the compiled dirs\n      --   2) return a status and result to the caller\n    #}\n    {% call noop_statement('main', status=\"PASS\", res=None) -%}\n      -- Not running : non-destructive mode\n      {{ sql }}\n    {%- endcall %}\n  {%- else -%}\n    {% call statement('main') -%}\n      {{ create_view_as(intermediate_relation, sql) }}\n    {%- endcall %}\n  {%- endif %}\n\n  -- cleanup\n  {% if not should_ignore -%}\n    -- move the existing view out of the way\n    {% if old_relation is not none %}\n      {{ adapter.rename_relation(target_relation, backup_relation) }}\n    {% endif %}\n    {{ adapter.rename_relation(intermediate_relation, target_relation) }}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  {#\n      -- Don't commit in non-destructive mode _unless_ there are in-transaction hooks\n      -- TODO : Figure out some other way of doing this that isn't as fragile\n  #}\n  {% if has_transactional_hooks or not should_ignore %}\n      {{ adapter.commit() }}\n  {% endif %}\n\n  {% if not should_ignore %}\n    {{ drop_relation_if_exists(backup_relation) }}\n  {% endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n\n{%- endmaterialization -%}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.generate_schema_name": {"original_file_path": "macros\\etc\\get_custom_schema.sql", "tags": [], "name": "generate_schema_name", "package_name": "dbt", "unique_id": "macro.dbt.generate_schema_name", "path": "macros\\etc\\get_custom_schema.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Renders a schema name given a custom schema name. If the custom\n    schema name is none, then the resulting schema is just the \"schema\"\n    value in the specified target. If a schema override is specified, then\n    the resulting schema is the default schema concatenated with the \n    custom schema.\n\n    This macro can be overriden in projects to define different semantics\n    for rendering a schema name.\n\n    Arguments:\n    custom_schema_name: The custom schema name specified for a model, or none\n\n#}\n{% macro generate_schema_name(custom_schema_name=none) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none -%}\n\n        {{ default_schema }}\n\n    {%- else -%}\n\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n\n    {%- endif -%}\n\n{%- endmacro %}\n\n\n{#\n    Renders a schema name given a custom schema name. In production, this macro\n    will render out the overriden schema name for a model. Otherwise, the default\n    schema specified in the active target is used.\n\n    Arguments:\n    custom_schema_name: The custom schema name specified for a model, or none\n\n#}\n{% macro generate_schema_name_for_env(custom_schema_name=none) -%}\n\n    {%- set default_schema = target.schema -%}\n    {%- if target.name == 'prod' and custom_schema_name is not none -%}\n\n        {{ custom_schema_name | trim }}\n\n    {%- else -%}\n\n        {{ default_schema }}\n\n    {%- endif -%}\n\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.postgres__get_merge_sql": {"original_file_path": "macros\\materializations\\common\\merge.sql", "tags": [], "name": "postgres__get_merge_sql", "package_name": "dbt", "unique_id": "macro.dbt.postgres__get_merge_sql", "path": "macros\\materializations\\common\\merge.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro get_merge_sql(target, source, unique_key, dest_columns) -%}\n  {{ adapter_macro('get_merge_sql', target, source, unique_key, dest_columns) }}\n{%- endmacro %}\n\n{% macro default__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {%- set dest_cols_csv = dest_columns | map(attribute=\"name\") | join(', ') -%}\n\n    merge into {{ target }} as DBT_INTERNAL_DEST\n    using {{ source }} as DBT_INTERNAL_SOURCE\n\n    {% if unique_key %}\n        on DBT_INTERNAL_SOURCE.{{ unique_key }} = DBT_INTERNAL_DEST.{{ unique_key }}\n    {% else %}\n        on FALSE\n    {% endif %}\n\n    {% if unique_key %}\n    when matched then update set\n        {% for column in dest_columns -%}\n            {{ column.name }} = DBT_INTERNAL_SOURCE.{{ column.name }}\n            {%- if not loop.last %}, {%- endif %}\n        {%- endfor %}\n    {% endif %}\n\n    when not matched then insert\n        ({{ dest_cols_csv }})\n    values\n        ({{ dest_cols_csv }})\n\n{% endmacro %}\n\n{% macro redshift__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {{ exceptions.raise_compiler_error('get_merge_sql is not implemented for Redshift') }}\n{% endmacro %}\n\n{% macro postgres__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {{ exceptions.raise_compiler_error('get_merge_sql is not implemented for Postgres') }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_table_snowflake": {"original_file_path": "macros\\materializations\\table\\snowflake_table.sql", "tags": [], "name": "materialization_table_snowflake", "package_name": "dbt", "unique_id": "macro.dbt.materialization_table_snowflake", "path": "macros\\materializations\\table\\snowflake_table.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% materialization table, adapter='snowflake' %}\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_tmp' -%}\n  {%- set backup_identifier = identifier + '__dbt_backup' -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n  {%- set target_relation = api.Relation.create(identifier=identifier,\n                                                schema=schema, type='table') -%}\n  {%- set intermediate_relation = api.Relation.create(identifier=tmp_identifier,\n                                                      schema=schema, type='table') -%}\n\n  /*\n      See ../view/view.sql for more information about this relation.\n  */\n  {%- set backup_relation = api.Relation.create(identifier=backup_identifier,\n                                                schema=schema, type=(old_relation.type or 'table')) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n  {%- set create_as_temporary = (exists_as_table and non_destructive_mode) -%}\n\n\n  -- drop the temp relations if they exists for some reason\n  {{ adapter.drop_relation(intermediate_relation) }}\n  {{ adapter.drop_relation(backup_relation) }}\n\n  -- setup: if the target relation already exists, truncate or drop it (if it's a view)\n  {% if non_destructive_mode -%}\n    {% if exists_as_table -%}\n      {{ adapter.truncate_relation(old_relation) }}\n    {% elif exists_as_view -%}\n      {{ adapter.drop_relation(old_relation) }}\n      {%- set old_relation = none -%}\n    {%- endif %}\n  {%- endif %}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% call statement('main') -%}\n    {%- if non_destructive_mode -%}\n      {%- if old_relation is not none -%}\n        {{ create_table_as(create_as_temporary, intermediate_relation, sql) }}\n\n        {% set dest_columns = adapter.get_columns_in_table(schema, identifier) %}\n        {% set dest_cols_csv = dest_columns | map(attribute='quoted') | join(', ') %}\n\n        insert into {{ target_relation }} ({{ dest_cols_csv }}) (\n          select {{ dest_cols_csv }}\n          from {{ intermediate_relation.include(schema=(not create_as_temporary)) }}\n        );\n      {%- else -%}\n        {{ create_table_as(create_as_temporary, target_relation, sql) }}\n      {%- endif -%}\n    {%- else -%}\n      {{ create_table_as(create_as_temporary, intermediate_relation, sql) }}\n    {%- endif -%}\n  {%- endcall %}\n\n  -- cleanup\n  {% if non_destructive_mode -%}\n    -- noop\n  {%- else -%}\n    {% if old_relation is not none %}\n      {% if old_relation.type == 'view' %}\n        {#-- This is the primary difference between Snowflake and Redshift. Renaming this view\n          -- would cause an error if the view has become invalid due to upstream schema changes #}\n        {{ log(\"Dropping relation \" ~ old_relation ~ \" because it is a view and this model is a table.\") }}\n        {{ drop_relation_if_exists(old_relation) }}\n      {% else %}\n        {{ adapter.rename_relation(target_relation, backup_relation) }}\n      {% endif %}\n    {% endif %}\n\n    {{ adapter.rename_relation(intermediate_relation, target_relation) }}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  -- finally, drop the existing/backup relation after the commit\n  {{ drop_relation_if_exists(backup_relation) }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.make_date_partitioned_table": {"original_file_path": "macros\\materializations\\table\\bigquery_table.sql", "tags": [], "name": "make_date_partitioned_table", "package_name": "dbt", "unique_id": "macro.dbt.make_date_partitioned_table", "path": "macros\\materializations\\table\\bigquery_table.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro make_date_partitioned_table(model, relation, dates, should_create, verbose=False) %}\n\n  {% if should_create %}\n      {{ adapter.make_date_partitioned_table(relation.schema, relation.identifier) }}\n  {% endif %}\n\n  {% for date in dates %}\n    {% set date = (date | string) %}\n    {% if verbose %}\n        {% set table_start_time = modules.datetime.datetime.now().strftime(\"%H:%M:%S\") %}\n        {{ log(table_start_time ~ ' | -> Running for day ' ~ date, info=True) }}\n    {% endif %}\n\n    {% set fixed_sql = model['injected_sql'] | replace('[DBT__PARTITION_DATE]', date) %}\n    {% set _ = adapter.execute_model(model, 'table', fixed_sql, decorator=date) %}\n  {% endfor %}\n\n  {% set num_days = dates | length %}\n  {% if num_days == 1 %}\n      {% set result_str = 'CREATED 1 PARTITION' %}\n  {% else %}\n      {% set result_str = 'CREATED ' ~ num_days ~ ' PARTITIONS' %}\n  {% endif %}\n\n  {{ store_result('main', status=result_str) }}\n\n{% endmacro %}\n\n{% materialization table, adapter='bigquery' -%}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n  {%- set exists_not_as_table = (old_relation is not none and not old_relation.is_table) -%}\n  {%- set target_relation = api.Relation.create(schema=schema, identifier=identifier, type='table') -%}\n  {%- set verbose = config.get('verbose', False) -%}\n\n  {# partitions: iterate over each partition, running a separate query in a for-loop #}\n  {%- set partitions = config.get('partitions') -%}\n\n  {% if partitions %}\n      {% if partitions is number or partitions is string %}\n        {% set partitions = [(partitions | string)] %}\n      {% endif %}\n\n      {% if partitions is not iterable %}\n        {{ exceptions.raise_compiler_error(\"Provided `partitions` configuration is not a list. Got: \" ~ partitions, model) }}\n      {% endif %}\n  {% endif %}\n\n  {{ run_hooks(pre_hooks) }}\n\n  {#\n      Since dbt uses WRITE_TRUNCATE mode for tables, we only need to drop this thing\n      if it is not a table. If it _is_ already a table, then we can overwrite it without downtime\n  #}\n  {%- if exists_not_as_table -%}\n      {{ adapter.drop_relation(old_relation) }}\n  {%- endif -%}\n\n  -- build model\n  {% if partitions %}\n    {# Create the dp-table if 1. it does not exist or 2. it existed, but we just dropped it #}\n    {%- set should_create = (old_relation is none or exists_not_as_table) -%}\n    {{ make_date_partitioned_table(model, target_relation, partitions, should_create, verbose) }}\n  {% else %}\n    {% call statement('main') -%}\n      {{ create_table_as(False, target_relation, sql) }}\n    {% endcall -%}\n  {% endif %}\n\n  {{ run_hooks(post_hooks) }}\n\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.create_schema": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "create_schema", "package_name": "dbt", "unique_id": "macro.dbt.create_schema", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.get_catalog": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "get_catalog", "package_name": "dbt", "unique_id": "macro.dbt.get_catalog", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.archive_scd_hash": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "archive_scd_hash", "package_name": "dbt", "unique_id": "macro.dbt.archive_scd_hash", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.before_begin": {"original_file_path": "macros\\materializations\\helpers.sql", "tags": [], "name": "before_begin", "package_name": "dbt", "unique_id": "macro.dbt.before_begin", "path": "macros\\materializations\\helpers.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro run_hooks(hooks, inside_transaction=True) %}\n  {% for hook in hooks | selectattr('transaction', 'equalto', inside_transaction)  %}\n    {% if not inside_transaction and loop.first %}\n      {% call statement(auto_begin=inside_transaction) %}\n        commit;\n      {% endcall %}\n    {% endif %}\n    {% call statement(auto_begin=inside_transaction) %}\n      {{ hook.get('sql') }}\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n\n{% macro column_list(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro column_list_for_create_table(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {{ col.data_type }} {%- if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro make_hook_config(sql, inside_transaction) %}\n    {{ tojson({\"sql\": sql, \"transaction\": inside_transaction}) }}\n{% endmacro %}\n\n\n{% macro before_begin(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n\n{% macro in_transaction(sql) %}\n    {{ make_hook_config(sql, inside_transaction=True) }}\n{% endmacro %}\n\n\n{% macro after_commit(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n{% macro drop_relation_if_exists(relation) %}\n  {% if relation is not none %}\n    {{ adapter.drop_relation(relation) }}\n  {% endif %}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.dist": {"original_file_path": "macros\\adapters\\redshift.sql", "tags": [], "name": "dist", "package_name": "dbt", "unique_id": "macro.dbt.dist", "path": "macros\\adapters\\redshift.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro dist(dist) %}\n  {%- if dist is not none -%}\n      {%- set dist = dist.strip().lower() -%}\n\n      {%- if dist in ['all', 'even'] -%}\n        diststyle {{ dist }}\n      {%- else -%}\n        diststyle key distkey ({{ dist }})\n      {%- endif -%}\n\n  {%- endif -%}\n{%- endmacro -%}\n\n\n{% macro sort(sort_type, sort) %}\n  {%- if sort is not none %}\n      {{ sort_type | default('compound', boolean=true) }} sortkey(\n      {%- if sort is string -%}\n        {%- set sort = [sort] -%}\n      {%- endif -%}\n      {%- for item in sort -%}\n        {{ item }}\n        {%- if not loop.last -%},{%- endif -%}\n      {%- endfor -%}\n      )\n  {%- endif %}\n{%- endmacro -%}\n\n\n{% macro redshift__create_table_as(temporary, relation, sql) -%}\n\n  {%- set _dist = config.get('dist') -%}\n  {%- set _sort_type = config.get(\n          'sort_type',\n          validator=validation.any['compound', 'interleaved']) -%}\n  {%- set _sort = config.get(\n          'sort',\n          validator=validation.any[list, basestring]) -%}\n\n  create {% if temporary -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n    {{ dist(_dist) }}\n    {{ sort(_sort_type, _sort) }}\n  as (\n    {{ sql }}\n  );\n{%- endmacro %}\n\n\n{% macro redshift__create_view_as(relation, sql) -%}\n\n  {% set bind_qualifier = '' if config.get('bind', default=True) else 'with no schema binding' %}\n\n  create view {{ relation }} as (\n    {{ sql }}\n  ) {{ bind_qualifier }};\n{% endmacro %}\n\n\n{% macro redshift__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  )\n  {{ dist('dbt_updated_at') }}\n  {{ sort('compound', ['scd_id']) }};\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.get_relations": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "get_relations", "package_name": "dbt", "unique_id": "macro.dbt.get_relations", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.snowflake__create_view_as": {"original_file_path": "macros\\adapters\\snowflake.sql", "tags": [], "name": "snowflake__create_view_as", "package_name": "dbt", "unique_id": "macro.dbt.snowflake__create_view_as", "path": "macros\\adapters\\snowflake.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro snowflake__create_table_as(temporary, relation, sql) -%}\n  {% if temporary %}\n    use schema {{ adapter.quote_as_configured(schema, 'schema') }};\n  {% endif %}\n\n  {{ default__create_table_as(temporary, relation, sql) }}\n{% endmacro %}\n\n{% macro snowflake__create_view_as(relation, sql) -%}\n  create or replace view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.create_columns": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "create_columns", "package_name": "dbt", "unique_id": "macro.dbt.create_columns", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.sort": {"original_file_path": "macros\\adapters\\redshift.sql", "tags": [], "name": "sort", "package_name": "dbt", "unique_id": "macro.dbt.sort", "path": "macros\\adapters\\redshift.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro dist(dist) %}\n  {%- if dist is not none -%}\n      {%- set dist = dist.strip().lower() -%}\n\n      {%- if dist in ['all', 'even'] -%}\n        diststyle {{ dist }}\n      {%- else -%}\n        diststyle key distkey ({{ dist }})\n      {%- endif -%}\n\n  {%- endif -%}\n{%- endmacro -%}\n\n\n{% macro sort(sort_type, sort) %}\n  {%- if sort is not none %}\n      {{ sort_type | default('compound', boolean=true) }} sortkey(\n      {%- if sort is string -%}\n        {%- set sort = [sort] -%}\n      {%- endif -%}\n      {%- for item in sort -%}\n        {{ item }}\n        {%- if not loop.last -%},{%- endif -%}\n      {%- endfor -%}\n      )\n  {%- endif %}\n{%- endmacro -%}\n\n\n{% macro redshift__create_table_as(temporary, relation, sql) -%}\n\n  {%- set _dist = config.get('dist') -%}\n  {%- set _sort_type = config.get(\n          'sort_type',\n          validator=validation.any['compound', 'interleaved']) -%}\n  {%- set _sort = config.get(\n          'sort',\n          validator=validation.any[list, basestring]) -%}\n\n  create {% if temporary -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n    {{ dist(_dist) }}\n    {{ sort(_sort_type, _sort) }}\n  as (\n    {{ sql }}\n  );\n{%- endmacro %}\n\n\n{% macro redshift__create_view_as(relation, sql) -%}\n\n  {% set bind_qualifier = '' if config.get('bind', default=True) else 'with no schema binding' %}\n\n  create view {{ relation }} as (\n    {{ sql }}\n  ) {{ bind_qualifier }};\n{% endmacro %}\n\n\n{% macro redshift__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  )\n  {{ dist('dbt_updated_at') }}\n  {{ sort('compound', ['scd_id']) }};\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.in_transaction": {"original_file_path": "macros\\materializations\\helpers.sql", "tags": [], "name": "in_transaction", "package_name": "dbt", "unique_id": "macro.dbt.in_transaction", "path": "macros\\materializations\\helpers.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro run_hooks(hooks, inside_transaction=True) %}\n  {% for hook in hooks | selectattr('transaction', 'equalto', inside_transaction)  %}\n    {% if not inside_transaction and loop.first %}\n      {% call statement(auto_begin=inside_transaction) %}\n        commit;\n      {% endcall %}\n    {% endif %}\n    {% call statement(auto_begin=inside_transaction) %}\n      {{ hook.get('sql') }}\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n\n{% macro column_list(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro column_list_for_create_table(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {{ col.data_type }} {%- if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro make_hook_config(sql, inside_transaction) %}\n    {{ tojson({\"sql\": sql, \"transaction\": inside_transaction}) }}\n{% endmacro %}\n\n\n{% macro before_begin(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n\n{% macro in_transaction(sql) %}\n    {{ make_hook_config(sql, inside_transaction=True) }}\n{% endmacro %}\n\n\n{% macro after_commit(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n{% macro drop_relation_if_exists(relation) %}\n  {% if relation is not none %}\n    {{ adapter.drop_relation(relation) }}\n  {% endif %}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.cluster_by": {"original_file_path": "macros\\adapters\\bigquery.sql", "tags": [], "name": "cluster_by", "package_name": "dbt", "unique_id": "macro.dbt.cluster_by", "path": "macros\\adapters\\bigquery.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro partition_by(raw_partition_by) %}\n  {%- if raw_partition_by is none -%}\n    {{ return('') }}\n  {% endif %}\n\n  {% set partition_by_clause %}\n    partition by {{ raw_partition_by }}\n  {%- endset -%}\n\n  {{ return(partition_by_clause) }}\n{%- endmacro -%}\n\n\n{% macro cluster_by(raw_cluster_by) %}\n  {%- if raw_cluster_by is not none -%}\n  cluster by \n  {% if raw_cluster_by is string -%}\n    {% set raw_cluster_by = [raw_cluster_by] %}\n  {%- endif -%}\n  {%- for cluster in raw_cluster_by -%}\n    {{ cluster }}\n    {%- if not loop.last -%},{%- endif -%}\n  {%- endfor -%}\n\n  {% endif %}\n\n{%- endmacro -%}\n\n\n{% macro bigquery__create_table_as(temporary, relation, sql) -%}\n  {%- set raw_partition_by = config.get('partition_by', none) -%}\n  {%- set raw_cluster_by = config.get('cluster_by', none) -%}\n\n  create or replace table {{ relation }}\n  {{ partition_by(raw_partition_by) }}\n  {{ cluster_by(raw_cluster_by) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro bigquery__create_view_as(relation, sql) -%}\n  create or replace view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__get_merge_sql": {"original_file_path": "macros\\materializations\\common\\merge.sql", "tags": [], "name": "redshift__get_merge_sql", "package_name": "dbt", "unique_id": "macro.dbt.redshift__get_merge_sql", "path": "macros\\materializations\\common\\merge.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro get_merge_sql(target, source, unique_key, dest_columns) -%}\n  {{ adapter_macro('get_merge_sql', target, source, unique_key, dest_columns) }}\n{%- endmacro %}\n\n{% macro default__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {%- set dest_cols_csv = dest_columns | map(attribute=\"name\") | join(', ') -%}\n\n    merge into {{ target }} as DBT_INTERNAL_DEST\n    using {{ source }} as DBT_INTERNAL_SOURCE\n\n    {% if unique_key %}\n        on DBT_INTERNAL_SOURCE.{{ unique_key }} = DBT_INTERNAL_DEST.{{ unique_key }}\n    {% else %}\n        on FALSE\n    {% endif %}\n\n    {% if unique_key %}\n    when matched then update set\n        {% for column in dest_columns -%}\n            {{ column.name }} = DBT_INTERNAL_SOURCE.{{ column.name }}\n            {%- if not loop.last %}, {%- endif %}\n        {%- endfor %}\n    {% endif %}\n\n    when not matched then insert\n        ({{ dest_cols_csv }})\n    values\n        ({{ dest_cols_csv }})\n\n{% endmacro %}\n\n{% macro redshift__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {{ exceptions.raise_compiler_error('get_merge_sql is not implemented for Redshift') }}\n{% endmacro %}\n\n{% macro postgres__get_merge_sql(target, source, unique_key, dest_columns) -%}\n    {{ exceptions.raise_compiler_error('get_merge_sql is not implemented for Postgres') }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_incremental_default": {"original_file_path": "macros\\materializations\\incremental\\incremental.sql", "tags": [], "name": "materialization_incremental_default", "package_name": "dbt", "unique_id": "macro.dbt.materialization_incremental_default", "path": "macros\\materializations\\incremental\\incremental.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro dbt__incremental_delete(target_relation, tmp_relation) -%}\n\n  {%- set unique_key = config.require('unique_key') -%}\n\n  delete\n  from {{ target_relation }}\n  where ({{ unique_key }}) in (\n    select ({{ unique_key }})\n    from {{ tmp_relation.include(schema=False) }}\n  );\n\n{%- endmacro %}\n\n{% materialization incremental, default -%}\n  {%- set sql_where = config.require('sql_where') -%}\n  {%- set unique_key = config.get('unique_key') -%}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_incremental_tmp' -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n  {%- set target_relation = api.Relation.create(identifier=identifier, schema=schema, type='table') -%}\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier,\n                                                 schema=schema, type='table') -%}\n\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_not_as_table = (old_relation is not none and not old_relation.is_table) -%}\n\n  {%- set should_truncate = (non_destructive_mode and full_refresh_mode and exists_as_table) -%}\n  {%- set should_drop = (not should_truncate and (full_refresh_mode or exists_not_as_table)) -%}\n  {%- set force_create = (flags.FULL_REFRESH and not flags.NON_DESTRUCTIVE) -%}\n\n  -- setup\n  {% if old_relation is none -%}\n    -- noop\n  {%- elif should_truncate -%}\n    {{ adapter.truncate_relation(old_relation) }}\n  {%- elif should_drop -%}\n    {{ adapter.drop_relation(old_relation) }}\n    {%- set old_relation = none -%}\n  {%- endif %}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% if force_create or old_relation is none -%}\n    {%- call statement('main') -%}\n      {{ create_table_as(False, target_relation, sql) }}\n    {%- endcall -%}\n  {%- else -%}\n     {%- call statement() -%}\n\n       {% set tmp_table_sql -%}\n         {# We are using a subselect instead of a CTE here to allow PostgreSQL to use indexes. -#}\n         select * from (\n           {{ sql }}\n         ) as dbt_incr_sbq\n         where ({{ sql_where }})\n           or ({{ sql_where }}) is null\n       {%- endset %}\n\n       {{ dbt.create_table_as(True, tmp_relation, tmp_table_sql) }}\n\n     {%- endcall -%}\n\n     {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                           to_schema=schema,\n                                           to_table=identifier) }}\n\n     {%- call statement('main') -%}\n       {% set dest_columns = adapter.get_columns_in_table(schema, identifier) %}\n       {% set dest_cols_csv = dest_columns | map(attribute='quoted') | join(', ') %}\n\n       {% if unique_key is not none -%}\n\n         {{ dbt__incremental_delete(target_relation, tmp_relation) }}\n\n       {%- endif %}\n\n       insert into {{ target_relation }} ({{ dest_cols_csv }})\n       (\n         select {{ dest_cols_csv }}\n         from {{ tmp_relation.include(schema=False) }}\n       );\n     {% endcall %}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n\n{%- endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_table_bigquery": {"original_file_path": "macros\\materializations\\table\\bigquery_table.sql", "tags": [], "name": "materialization_table_bigquery", "package_name": "dbt", "unique_id": "macro.dbt.materialization_table_bigquery", "path": "macros\\materializations\\table\\bigquery_table.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro make_date_partitioned_table(model, relation, dates, should_create, verbose=False) %}\n\n  {% if should_create %}\n      {{ adapter.make_date_partitioned_table(relation.schema, relation.identifier) }}\n  {% endif %}\n\n  {% for date in dates %}\n    {% set date = (date | string) %}\n    {% if verbose %}\n        {% set table_start_time = modules.datetime.datetime.now().strftime(\"%H:%M:%S\") %}\n        {{ log(table_start_time ~ ' | -> Running for day ' ~ date, info=True) }}\n    {% endif %}\n\n    {% set fixed_sql = model['injected_sql'] | replace('[DBT__PARTITION_DATE]', date) %}\n    {% set _ = adapter.execute_model(model, 'table', fixed_sql, decorator=date) %}\n  {% endfor %}\n\n  {% set num_days = dates | length %}\n  {% if num_days == 1 %}\n      {% set result_str = 'CREATED 1 PARTITION' %}\n  {% else %}\n      {% set result_str = 'CREATED ' ~ num_days ~ ' PARTITIONS' %}\n  {% endif %}\n\n  {{ store_result('main', status=result_str) }}\n\n{% endmacro %}\n\n{% materialization table, adapter='bigquery' -%}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n  {%- set exists_not_as_table = (old_relation is not none and not old_relation.is_table) -%}\n  {%- set target_relation = api.Relation.create(schema=schema, identifier=identifier, type='table') -%}\n  {%- set verbose = config.get('verbose', False) -%}\n\n  {# partitions: iterate over each partition, running a separate query in a for-loop #}\n  {%- set partitions = config.get('partitions') -%}\n\n  {% if partitions %}\n      {% if partitions is number or partitions is string %}\n        {% set partitions = [(partitions | string)] %}\n      {% endif %}\n\n      {% if partitions is not iterable %}\n        {{ exceptions.raise_compiler_error(\"Provided `partitions` configuration is not a list. Got: \" ~ partitions, model) }}\n      {% endif %}\n  {% endif %}\n\n  {{ run_hooks(pre_hooks) }}\n\n  {#\n      Since dbt uses WRITE_TRUNCATE mode for tables, we only need to drop this thing\n      if it is not a table. If it _is_ already a table, then we can overwrite it without downtime\n  #}\n  {%- if exists_not_as_table -%}\n      {{ adapter.drop_relation(old_relation) }}\n  {%- endif -%}\n\n  -- build model\n  {% if partitions %}\n    {# Create the dp-table if 1. it does not exist or 2. it existed, but we just dropped it #}\n    {%- set should_create = (old_relation is none or exists_not_as_table) -%}\n    {{ make_date_partitioned_table(model, target_relation, partitions, should_create, verbose) }}\n  {% else %}\n    {% call statement('main') -%}\n      {{ create_table_as(False, target_relation, sql) }}\n    {% endcall -%}\n  {% endif %}\n\n  {{ run_hooks(post_hooks) }}\n\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "operation.dbt.get_catalog_data": {"original_file_path": "macros\\operations\\catalog\\get_catalog.sql", "tags": [], "name": "get_catalog_data", "package_name": "dbt", "unique_id": "operation.dbt.get_catalog_data", "path": "macros\\operations\\catalog\\get_catalog.sql", "depends_on": {"macros": []}, "resource_type": "operation", "raw_sql": "{% operation get_catalog_data %}\n    {% set catalog = dbt.get_catalog() %}\n    {{ return(catalog) }}\n{% endoperation %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__create_archive_table": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "default__create_archive_table", "package_name": "dbt", "unique_id": "macro.dbt.default__create_archive_table", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.create_view_as": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "create_view_as", "package_name": "dbt", "unique_id": "macro.dbt.create_view_as", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_incremental_bigquery": {"original_file_path": "macros\\materializations\\incremental\\bigquery_incremental.sql", "tags": [], "name": "materialization_incremental_bigquery", "package_name": "dbt", "unique_id": "macro.dbt.materialization_incremental_bigquery", "path": "macros\\materializations\\incremental\\bigquery_incremental.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% materialization incremental, adapter='bigquery' -%}\n\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set sql_where = config.get('sql_where') -%}\n\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {% if non_destructive_mode %}\n    {{ exceptions.raise_compiler_error(\"--non-destructive mode is not supported on BigQuery\") }}\n  {% endif %}\n\n  {%- set identifier = model['alias'] -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n\n  {%- set target_relation = api.Relation.create(identifier=identifier, schema=schema, type='table') -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_not_as_table = (old_relation is not none and not old_relation.is_table) -%}\n\n  {%- set should_drop = (full_refresh_mode or exists_not_as_table) -%}\n  {%- set force_create = (full_refresh_mode) -%}\n\n  -- setup\n  {% if old_relation is none -%}\n    -- noop\n  {%- elif should_drop -%}\n    {{ adapter.drop_relation(old_relation) }}\n    {%- set old_relation = none -%}\n  {%- endif %}\n\n  {% set source_sql -%}\n     {#-- wrap sql in parens to make it a subquery --#}\n     (\n        select * from (\n            {{ sql }}\n        )\n        where ({{ sql_where }}) or ({{ sql_where }}) is null\n    )\n  {%- endset -%}\n\n\n  {{ run_hooks(pre_hooks) }}\n\n  -- build model\n  {% if force_create or old_relation is none -%}\n    {%- call statement('main') -%}\n      {{ create_table_as(False, target_relation, sql) }}\n    {%- endcall -%}\n  {%- else -%}\n     {% set dest_columns = adapter.get_columns_in_table(schema, identifier) %}\n     {%- call statement('main') -%}\n       {{ get_merge_sql(target_relation, source_sql, unique_key, dest_columns) }}\n     {% endcall %}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks) }}\n\n{%- endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.reset_csv_table": {"original_file_path": "macros\\materializations\\seed\\seed.sql", "tags": [], "name": "reset_csv_table", "package_name": "dbt", "unique_id": "macro.dbt.reset_csv_table", "path": "macros\\materializations\\seed\\seed.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro create_csv_table(model) -%}\n  {{ adapter_macro('create_csv_table', model) }}\n{%- endmacro %}\n\n{% macro reset_csv_table(model, full_refresh, old_relation) -%}\n  {{ adapter_macro('reset_csv_table', model, full_refresh, old_relation) }}\n{%- endmacro %}\n\n{% macro load_csv_rows(model) -%}\n  {{ adapter_macro('load_csv_rows', model) }}\n{%- endmacro %}\n\n{% macro default__create_csv_table(model) %}\n  {%- set agate_table = model['agate_table'] -%}\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n\n  {% set sql %}\n    create table {{ this.render(False) }} (\n        {%- for col_name in agate_table.column_names -%}\n            {%- set inferred_type = adapter.convert_type(agate_table, loop.index0) -%}\n            {%- set type = column_override.get(col_name, inferred_type) -%}\n            {{ col_name | string }} {{ type }} {%- if not loop.last -%}, {%- endif -%}\n        {%- endfor -%}\n    )\n  {% endset %}\n\n  {% call statement('_') -%}\n    {{ sql }}\n  {%- endcall %}\n\n  {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__reset_csv_table(model, full_refresh, old_relation) %}\n    {% set sql = \"\" %}\n    {% if full_refresh %}\n        {{ adapter.drop_relation(old_relation) }}\n        {% set sql = create_csv_table(model) %}\n    {% else %}\n        {{ adapter.truncate_relation(old_relation) }}\n        {% set sql = \"truncate table \" ~ old_relation %}\n    {% endif %}\n\n    {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__load_csv_rows(model) %}\n    {% set agate_table = model['agate_table'] %}\n    {% set cols_sql = \", \".join(agate_table.column_names) %}\n    {% set bindings = [] %}\n\n    {% set statements = [] %}\n\n    {% for chunk in agate_table.rows | batch(10000) %}\n        {% set bindings = [] %}\n\n        {% for row in chunk %}\n            {% set _ = bindings.extend(row) %}\n        {% endfor %}\n\n        {% set sql %}\n            insert into {{ this.render(False) }} ({{ cols_sql }}) values\n            {% for row in chunk -%}\n                ({%- for column in agate_table.column_names -%}\n                    %s\n                    {%- if not loop.last%},{%- endif %}\n                {%- endfor -%})\n                {%- if not loop.last%},{%- endif %}\n            {%- endfor %}\n        {% endset %}\n\n        {% set _ = adapter.add_query(sql, bindings=bindings, abridge_sql_log=True) %}\n\n        {% if loop.index0 == 0 %}\n            {% set _ = statements.append(sql) %}\n        {% endif %}\n    {% endfor %}\n\n    {# Return SQL so we can render it out into the compiled files #}\n    {{ return(statements[0]) }}\n{% endmacro %}\n\n\n{% materialization seed, default %}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set csv_table = model[\"agate_table\"] -%}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% set create_table_sql = \"\" %}\n  {% if exists_as_view %}\n    {{ exceptions.raise_compiler_error(\"Cannot seed to '{}', it is a view\".format(old_relation)) }}\n  {% elif exists_as_table %}\n    {% set create_table_sql = reset_csv_table(model, full_refresh_mode, old_relation) %}\n  {% else %}\n    {% set create_table_sql = create_csv_table(model) %}\n  {% endif %}\n\n  {% set status = 'CREATE' if full_refresh_mode else 'INSERT' %}\n  {% set num_rows = (csv_table.rows | length) %}\n  {% set sql = load_csv_rows(model) %}\n\n  {% call noop_statement('main', status ~ ' ' ~ num_rows) %}\n    {{ create_table_sql }};\n    -- dbt seed --\n    {{ sql }}\n  {% endcall %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__get_relations": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "default__get_relations", "package_name": "dbt", "unique_id": "macro.dbt.default__get_relations", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__create_view_as": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "default__create_view_as", "package_name": "dbt", "unique_id": "macro.dbt.default__create_view_as", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.date_sharded_table": {"original_file_path": "macros\\etc\\bigquery.sql", "tags": [], "name": "date_sharded_table", "package_name": "dbt", "unique_id": "macro.dbt.date_sharded_table", "path": "macros\\etc\\bigquery.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro date_sharded_table(base_name) %}\n    {{ return(base_name ~ \"[DBT__PARTITION_DATE]\") }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.drop_relation_if_exists": {"original_file_path": "macros\\materializations\\helpers.sql", "tags": [], "name": "drop_relation_if_exists", "package_name": "dbt", "unique_id": "macro.dbt.drop_relation_if_exists", "path": "macros\\materializations\\helpers.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro run_hooks(hooks, inside_transaction=True) %}\n  {% for hook in hooks | selectattr('transaction', 'equalto', inside_transaction)  %}\n    {% if not inside_transaction and loop.first %}\n      {% call statement(auto_begin=inside_transaction) %}\n        commit;\n      {% endcall %}\n    {% endif %}\n    {% call statement(auto_begin=inside_transaction) %}\n      {{ hook.get('sql') }}\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n\n{% macro column_list(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro column_list_for_create_table(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {{ col.data_type }} {%- if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro make_hook_config(sql, inside_transaction) %}\n    {{ tojson({\"sql\": sql, \"transaction\": inside_transaction}) }}\n{% endmacro %}\n\n\n{% macro before_begin(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n\n{% macro in_transaction(sql) %}\n    {{ make_hook_config(sql, inside_transaction=True) }}\n{% endmacro %}\n\n\n{% macro after_commit(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n{% macro drop_relation_if_exists(relation) %}\n  {% if relation is not none %}\n    {{ adapter.drop_relation(relation) }}\n  {% endif %}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__create_columns": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "bigquery__create_columns", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__create_columns", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__create_view_as": {"original_file_path": "macros\\adapters\\redshift.sql", "tags": [], "name": "redshift__create_view_as", "package_name": "dbt", "unique_id": "macro.dbt.redshift__create_view_as", "path": "macros\\adapters\\redshift.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro dist(dist) %}\n  {%- if dist is not none -%}\n      {%- set dist = dist.strip().lower() -%}\n\n      {%- if dist in ['all', 'even'] -%}\n        diststyle {{ dist }}\n      {%- else -%}\n        diststyle key distkey ({{ dist }})\n      {%- endif -%}\n\n  {%- endif -%}\n{%- endmacro -%}\n\n\n{% macro sort(sort_type, sort) %}\n  {%- if sort is not none %}\n      {{ sort_type | default('compound', boolean=true) }} sortkey(\n      {%- if sort is string -%}\n        {%- set sort = [sort] -%}\n      {%- endif -%}\n      {%- for item in sort -%}\n        {{ item }}\n        {%- if not loop.last -%},{%- endif -%}\n      {%- endfor -%}\n      )\n  {%- endif %}\n{%- endmacro -%}\n\n\n{% macro redshift__create_table_as(temporary, relation, sql) -%}\n\n  {%- set _dist = config.get('dist') -%}\n  {%- set _sort_type = config.get(\n          'sort_type',\n          validator=validation.any['compound', 'interleaved']) -%}\n  {%- set _sort = config.get(\n          'sort',\n          validator=validation.any[list, basestring]) -%}\n\n  create {% if temporary -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n    {{ dist(_dist) }}\n    {{ sort(_sort_type, _sort) }}\n  as (\n    {{ sql }}\n  );\n{%- endmacro %}\n\n\n{% macro redshift__create_view_as(relation, sql) -%}\n\n  {% set bind_qualifier = '' if config.get('bind', default=True) else 'with no schema binding' %}\n\n  create view {{ relation }} as (\n    {{ sql }}\n  ) {{ bind_qualifier }};\n{% endmacro %}\n\n\n{% macro redshift__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  )\n  {{ dist('dbt_updated_at') }}\n  {{ sort('compound', ['scd_id']) }};\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__create_view_as": {"original_file_path": "macros\\adapters\\bigquery.sql", "tags": [], "name": "bigquery__create_view_as", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__create_view_as", "path": "macros\\adapters\\bigquery.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro partition_by(raw_partition_by) %}\n  {%- if raw_partition_by is none -%}\n    {{ return('') }}\n  {% endif %}\n\n  {% set partition_by_clause %}\n    partition by {{ raw_partition_by }}\n  {%- endset -%}\n\n  {{ return(partition_by_clause) }}\n{%- endmacro -%}\n\n\n{% macro cluster_by(raw_cluster_by) %}\n  {%- if raw_cluster_by is not none -%}\n  cluster by \n  {% if raw_cluster_by is string -%}\n    {% set raw_cluster_by = [raw_cluster_by] %}\n  {%- endif -%}\n  {%- for cluster in raw_cluster_by -%}\n    {{ cluster }}\n    {%- if not loop.last -%},{%- endif -%}\n  {%- endfor -%}\n\n  {% endif %}\n\n{%- endmacro -%}\n\n\n{% macro bigquery__create_table_as(temporary, relation, sql) -%}\n  {%- set raw_partition_by = config.get('partition_by', none) -%}\n  {%- set raw_cluster_by = config.get('cluster_by', none) -%}\n\n  create or replace table {{ relation }}\n  {{ partition_by(raw_partition_by) }}\n  {{ cluster_by(raw_cluster_by) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro bigquery__create_view_as(relation, sql) -%}\n  create or replace view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__archive_update": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "default__archive_update", "package_name": "dbt", "unique_id": "macro.dbt.default__archive_update", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__create_table_as": {"original_file_path": "macros\\adapters\\redshift.sql", "tags": [], "name": "redshift__create_table_as", "package_name": "dbt", "unique_id": "macro.dbt.redshift__create_table_as", "path": "macros\\adapters\\redshift.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro dist(dist) %}\n  {%- if dist is not none -%}\n      {%- set dist = dist.strip().lower() -%}\n\n      {%- if dist in ['all', 'even'] -%}\n        diststyle {{ dist }}\n      {%- else -%}\n        diststyle key distkey ({{ dist }})\n      {%- endif -%}\n\n  {%- endif -%}\n{%- endmacro -%}\n\n\n{% macro sort(sort_type, sort) %}\n  {%- if sort is not none %}\n      {{ sort_type | default('compound', boolean=true) }} sortkey(\n      {%- if sort is string -%}\n        {%- set sort = [sort] -%}\n      {%- endif -%}\n      {%- for item in sort -%}\n        {{ item }}\n        {%- if not loop.last -%},{%- endif -%}\n      {%- endfor -%}\n      )\n  {%- endif %}\n{%- endmacro -%}\n\n\n{% macro redshift__create_table_as(temporary, relation, sql) -%}\n\n  {%- set _dist = config.get('dist') -%}\n  {%- set _sort_type = config.get(\n          'sort_type',\n          validator=validation.any['compound', 'interleaved']) -%}\n  {%- set _sort = config.get(\n          'sort',\n          validator=validation.any[list, basestring]) -%}\n\n  create {% if temporary -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n    {{ dist(_dist) }}\n    {{ sort(_sort_type, _sort) }}\n  as (\n    {{ sql }}\n  );\n{%- endmacro %}\n\n\n{% macro redshift__create_view_as(relation, sql) -%}\n\n  {% set bind_qualifier = '' if config.get('bind', default=True) else 'with no schema binding' %}\n\n  create view {{ relation }} as (\n    {{ sql }}\n  ) {{ bind_qualifier }};\n{% endmacro %}\n\n\n{% macro redshift__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  )\n  {{ dist('dbt_updated_at') }}\n  {{ sort('compound', ['scd_id']) }};\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.test_accepted_values": {"original_file_path": "macros\\schema_tests\\accepted_values.sql", "tags": [], "name": "test_accepted_values", "package_name": "dbt", "unique_id": "macro.dbt.test_accepted_values", "path": "macros\\schema_tests\\accepted_values.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro test_accepted_values(model, values) %}\n\n{% set column_name = kwargs.get('column_name', kwargs.get('field')) %}\n\nwith all_values as (\n\n    select distinct\n        {{ column_name }} as value_field\n\n    from {{ model }}\n\n),\n\nvalidation_errors as (\n\n    select\n        value_field\n\n    from all_values\n    where value_field not in (\n        {% for value in values -%}\n\n            '{{ value }}' {% if not loop.last -%} , {%- endif %}\n\n        {%- endfor %}\n    )\n)\n\nselect count(*)\nfrom validation_errors\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__handle_existing_table": {"original_file_path": "macros\\materializations\\view\\bq_snowflake_view.sql", "tags": [], "name": "bigquery__handle_existing_table", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__handle_existing_table", "path": "macros\\materializations\\view\\bq_snowflake_view.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {{ adapter_macro(\"dbt.handle_existing_table\", full_refresh, non_destructive_mode, old_relation) }}\n{% endmacro %}\n\n{% macro default__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- endif -%}\n{% endmacro %}\n\n{% macro bigquery__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if full_refresh and not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- else -%}\n      {{ exceptions.relation_wrong_type(old_relation, 'view') }}\n    {%- endif -%}\n{% endmacro %}\n\n\n{# /*\n       Core materialization implementation. BigQuery and Snowflake are similar\n       because both can use `create or replace view` where the resulting view schema\n       is not necessarily the same as the existing view. On Redshift, this would\n       result in: ERROR:  cannot change number of columns in view\n\n       This implementation is superior to the create_temp, swap_with_existing, drop_old\n       paradigm because transactions don't run DDL queries atomically on Snowflake. By using\n       `create or replace view`, the materialization becomes atomic in nature.\n    */\n#}\n\n{% macro impl_view_materialization(run_outside_transaction_hooks=True) %}\n  {%- set identifier = model['alias'] -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n\n  {%- set old_relation = adapter.get_relation(\n      schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set target_relation = api.Relation.create(\n      identifier=identifier, schema=schema,\n      type='view') -%}\n\n  {%- set should_ignore = non_destructive_mode and exists_as_view %}\n  {%- set has_transactional_hooks = (hooks | selectattr('transaction', 'equalto', True) | list | length) > 0 %}\n\n  {% if run_outside_transaction_hooks %}\n      -- no transactions on BigQuery\n      {{ run_hooks(pre_hooks, inside_transaction=False) }}\n  {% endif %}\n\n  -- `BEGIN` happens here on Snowflake\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- If there's a table with the same name and we weren't told to full refresh,\n  -- that's an error. If we were told to full refresh, drop it. This behavior differs\n  -- for Snowflake and BigQuery, so multiple dispatch is used.\n  {%- if old_relation is not none and old_relation.is_table -%}\n    {{ handle_existing_table(flags.FULL_REFRESH, non_destructive_mode, old_relation) }}\n  {%- endif -%}\n\n  -- build model\n  {% if non_destructive_mode -%}\n    {% call noop_statement('main', status=\"PASS\", res=None) -%}\n      -- Not running : non-destructive mode\n      {{ sql }}\n    {%- endcall %}\n  {%- else -%}\n    {% call statement('main') -%}\n      {{ create_view_as(target_relation, sql) }}\n    {%- endcall %}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  {#\n      -- Don't commit in non-destructive mode _unless_ there are in-transaction hooks\n      -- TODO : Figure out some other way of doing this that isn't as fragile\n  #}\n  {% if has_transactional_hooks or not should_ignore %}\n      {{ adapter.commit() }}\n  {% endif %}\n\n  {% if run_outside_transaction_hooks %}\n      -- No transactions on BigQuery\n      {{ run_hooks(post_hooks, inside_transaction=False) }}\n  {% endif %}\n{% endmacro %}\n\n{% materialization view, adapter='bigquery' -%}\n    {{ impl_view_materialization(run_outside_transaction_hooks=False) }}\n{%- endmaterialization %}\n\n{% materialization view, adapter='snowflake' -%}\n    {{ impl_view_materialization() }}\n{%- endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.column_list": {"original_file_path": "macros\\materializations\\helpers.sql", "tags": [], "name": "column_list", "package_name": "dbt", "unique_id": "macro.dbt.column_list", "path": "macros\\materializations\\helpers.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro run_hooks(hooks, inside_transaction=True) %}\n  {% for hook in hooks | selectattr('transaction', 'equalto', inside_transaction)  %}\n    {% if not inside_transaction and loop.first %}\n      {% call statement(auto_begin=inside_transaction) %}\n        commit;\n      {% endcall %}\n    {% endif %}\n    {% call statement(auto_begin=inside_transaction) %}\n      {{ hook.get('sql') }}\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n\n{% macro column_list(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro column_list_for_create_table(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {{ col.data_type }} {%- if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro make_hook_config(sql, inside_transaction) %}\n    {{ tojson({\"sql\": sql, \"transaction\": inside_transaction}) }}\n{% endmacro %}\n\n\n{% macro before_begin(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n\n{% macro in_transaction(sql) %}\n    {{ make_hook_config(sql, inside_transaction=True) }}\n{% endmacro %}\n\n\n{% macro after_commit(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n{% macro drop_relation_if_exists(relation) %}\n  {% if relation is not none %}\n    {{ adapter.drop_relation(relation) }}\n  {% endif %}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__reset_csv_table": {"original_file_path": "macros\\materializations\\seed\\seed.sql", "tags": [], "name": "default__reset_csv_table", "package_name": "dbt", "unique_id": "macro.dbt.default__reset_csv_table", "path": "macros\\materializations\\seed\\seed.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro create_csv_table(model) -%}\n  {{ adapter_macro('create_csv_table', model) }}\n{%- endmacro %}\n\n{% macro reset_csv_table(model, full_refresh, old_relation) -%}\n  {{ adapter_macro('reset_csv_table', model, full_refresh, old_relation) }}\n{%- endmacro %}\n\n{% macro load_csv_rows(model) -%}\n  {{ adapter_macro('load_csv_rows', model) }}\n{%- endmacro %}\n\n{% macro default__create_csv_table(model) %}\n  {%- set agate_table = model['agate_table'] -%}\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n\n  {% set sql %}\n    create table {{ this.render(False) }} (\n        {%- for col_name in agate_table.column_names -%}\n            {%- set inferred_type = adapter.convert_type(agate_table, loop.index0) -%}\n            {%- set type = column_override.get(col_name, inferred_type) -%}\n            {{ col_name | string }} {{ type }} {%- if not loop.last -%}, {%- endif -%}\n        {%- endfor -%}\n    )\n  {% endset %}\n\n  {% call statement('_') -%}\n    {{ sql }}\n  {%- endcall %}\n\n  {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__reset_csv_table(model, full_refresh, old_relation) %}\n    {% set sql = \"\" %}\n    {% if full_refresh %}\n        {{ adapter.drop_relation(old_relation) }}\n        {% set sql = create_csv_table(model) %}\n    {% else %}\n        {{ adapter.truncate_relation(old_relation) }}\n        {% set sql = \"truncate table \" ~ old_relation %}\n    {% endif %}\n\n    {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__load_csv_rows(model) %}\n    {% set agate_table = model['agate_table'] %}\n    {% set cols_sql = \", \".join(agate_table.column_names) %}\n    {% set bindings = [] %}\n\n    {% set statements = [] %}\n\n    {% for chunk in agate_table.rows | batch(10000) %}\n        {% set bindings = [] %}\n\n        {% for row in chunk %}\n            {% set _ = bindings.extend(row) %}\n        {% endfor %}\n\n        {% set sql %}\n            insert into {{ this.render(False) }} ({{ cols_sql }}) values\n            {% for row in chunk -%}\n                ({%- for column in agate_table.column_names -%}\n                    %s\n                    {%- if not loop.last%},{%- endif %}\n                {%- endfor -%})\n                {%- if not loop.last%},{%- endif %}\n            {%- endfor %}\n        {% endset %}\n\n        {% set _ = adapter.add_query(sql, bindings=bindings, abridge_sql_log=True) %}\n\n        {% if loop.index0 == 0 %}\n            {% set _ = statements.append(sql) %}\n        {% endif %}\n    {% endfor %}\n\n    {# Return SQL so we can render it out into the compiled files #}\n    {{ return(statements[0]) }}\n{% endmacro %}\n\n\n{% materialization seed, default %}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set csv_table = model[\"agate_table\"] -%}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% set create_table_sql = \"\" %}\n  {% if exists_as_view %}\n    {{ exceptions.raise_compiler_error(\"Cannot seed to '{}', it is a view\".format(old_relation)) }}\n  {% elif exists_as_table %}\n    {% set create_table_sql = reset_csv_table(model, full_refresh_mode, old_relation) %}\n  {% else %}\n    {% set create_table_sql = create_csv_table(model) %}\n  {% endif %}\n\n  {% set status = 'CREATE' if full_refresh_mode else 'INSERT' %}\n  {% set num_rows = (csv_table.rows | length) %}\n  {% set sql = load_csv_rows(model) %}\n\n  {% call noop_statement('main', status ~ ' ' ~ num_rows) %}\n    {{ create_table_sql }};\n    -- dbt seed --\n    {{ sql }}\n  {% endcall %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__archive_scd_hash": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "default__archive_scd_hash", "package_name": "dbt", "unique_id": "macro.dbt.default__archive_scd_hash", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.postgres__get_catalog": {"original_file_path": "macros\\catalog\\postgres_catalog.sql", "tags": [], "name": "postgres__get_catalog", "package_name": "dbt", "unique_id": "macro.dbt.postgres__get_catalog", "path": "macros\\catalog\\postgres_catalog.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro postgres__get_catalog() -%}\n\n  {%- call statement('catalog', fetch_result=True) -%}\n\n    with table_owners as (\n\n        select\n            schemaname as table_schema,\n            tablename as table_name,\n            tableowner as table_owner\n\n        from pg_tables\n\n        union all\n\n        select\n            schemaname as table_schema,\n            viewname as table_name,\n            viewowner as table_owner\n\n        from pg_views\n\n    ),\n\n    tables as (\n\n        select\n\n            table_schema,\n            table_name,\n            table_type\n\n        from information_schema.tables\n\n    ),\n\n    columns as (\n\n        select\n            table_schema,\n            table_name,\n            null as table_comment,\n            column_name,\n            ordinal_position as column_index,\n            data_type as column_type,\n            null as column_comment\n\n        from information_schema.columns\n\n    )\n\n    select *\n    from tables\n    join columns using (table_schema, table_name)\n    join table_owners using (table_schema, table_name)\n\n    where table_schema != 'information_schema'\n      and table_schema not like 'pg_%'\n\n    order by column_index\n\n  {%- endcall -%}\n\n  {{ return(load_result('catalog').table) }}\n\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__create_archive_table": {"original_file_path": "macros\\adapters\\redshift.sql", "tags": [], "name": "redshift__create_archive_table", "package_name": "dbt", "unique_id": "macro.dbt.redshift__create_archive_table", "path": "macros\\adapters\\redshift.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro dist(dist) %}\n  {%- if dist is not none -%}\n      {%- set dist = dist.strip().lower() -%}\n\n      {%- if dist in ['all', 'even'] -%}\n        diststyle {{ dist }}\n      {%- else -%}\n        diststyle key distkey ({{ dist }})\n      {%- endif -%}\n\n  {%- endif -%}\n{%- endmacro -%}\n\n\n{% macro sort(sort_type, sort) %}\n  {%- if sort is not none %}\n      {{ sort_type | default('compound', boolean=true) }} sortkey(\n      {%- if sort is string -%}\n        {%- set sort = [sort] -%}\n      {%- endif -%}\n      {%- for item in sort -%}\n        {{ item }}\n        {%- if not loop.last -%},{%- endif -%}\n      {%- endfor -%}\n      )\n  {%- endif %}\n{%- endmacro -%}\n\n\n{% macro redshift__create_table_as(temporary, relation, sql) -%}\n\n  {%- set _dist = config.get('dist') -%}\n  {%- set _sort_type = config.get(\n          'sort_type',\n          validator=validation.any['compound', 'interleaved']) -%}\n  {%- set _sort = config.get(\n          'sort',\n          validator=validation.any[list, basestring]) -%}\n\n  create {% if temporary -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n    {{ dist(_dist) }}\n    {{ sort(_sort_type, _sort) }}\n  as (\n    {{ sql }}\n  );\n{%- endmacro %}\n\n\n{% macro redshift__create_view_as(relation, sql) -%}\n\n  {% set bind_qualifier = '' if config.get('bind', default=True) else 'with no schema binding' %}\n\n  create view {{ relation }} as (\n    {{ sql }}\n  ) {{ bind_qualifier }};\n{% endmacro %}\n\n\n{% macro redshift__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  )\n  {{ dist('dbt_updated_at') }}\n  {{ sort('compound', ['scd_id']) }};\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.create_archive_table": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "create_archive_table", "package_name": "dbt", "unique_id": "macro.dbt.create_archive_table", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.test_relationships": {"original_file_path": "macros\\schema_tests\\relationships.sql", "tags": [], "name": "test_relationships", "package_name": "dbt", "unique_id": "macro.dbt.test_relationships", "path": "macros\\schema_tests\\relationships.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro test_relationships(model, to, field) %}\n\n{% set column_name = kwargs.get('column_name', kwargs.get('from')) %}\n\n\nselect count(*)\nfrom (\n    select {{ column_name }} as id from {{ model }}\n) as child\nleft join (\n    select {{ field }} as id from {{ to }}\n) as parent on parent.id = child.id\nwhere child.id is not null\n  and parent.id is null\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__create_columns": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "default__create_columns", "package_name": "dbt", "unique_id": "macro.dbt.default__create_columns", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.after_commit": {"original_file_path": "macros\\materializations\\helpers.sql", "tags": [], "name": "after_commit", "package_name": "dbt", "unique_id": "macro.dbt.after_commit", "path": "macros\\materializations\\helpers.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro run_hooks(hooks, inside_transaction=True) %}\n  {% for hook in hooks | selectattr('transaction', 'equalto', inside_transaction)  %}\n    {% if not inside_transaction and loop.first %}\n      {% call statement(auto_begin=inside_transaction) %}\n        commit;\n      {% endcall %}\n    {% endif %}\n    {% call statement(auto_begin=inside_transaction) %}\n      {{ hook.get('sql') }}\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n\n{% macro column_list(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro column_list_for_create_table(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {{ col.data_type }} {%- if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro make_hook_config(sql, inside_transaction) %}\n    {{ tojson({\"sql\": sql, \"transaction\": inside_transaction}) }}\n{% endmacro %}\n\n\n{% macro before_begin(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n\n{% macro in_transaction(sql) %}\n    {{ make_hook_config(sql, inside_transaction=True) }}\n{% endmacro %}\n\n\n{% macro after_commit(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n{% macro drop_relation_if_exists(relation) %}\n  {% if relation is not none %}\n    {{ adapter.drop_relation(relation) }}\n  {% endif %}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__archive_update": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "bigquery__archive_update", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__archive_update", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.dbt__incremental_delete": {"original_file_path": "macros\\materializations\\incremental\\incremental.sql", "tags": [], "name": "dbt__incremental_delete", "package_name": "dbt", "unique_id": "macro.dbt.dbt__incremental_delete", "path": "macros\\materializations\\incremental\\incremental.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro dbt__incremental_delete(target_relation, tmp_relation) -%}\n\n  {%- set unique_key = config.require('unique_key') -%}\n\n  delete\n  from {{ target_relation }}\n  where ({{ unique_key }}) in (\n    select ({{ unique_key }})\n    from {{ tmp_relation.include(schema=False) }}\n  );\n\n{%- endmacro %}\n\n{% materialization incremental, default -%}\n  {%- set sql_where = config.require('sql_where') -%}\n  {%- set unique_key = config.get('unique_key') -%}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_incremental_tmp' -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n  {%- set target_relation = api.Relation.create(identifier=identifier, schema=schema, type='table') -%}\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier,\n                                                 schema=schema, type='table') -%}\n\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_not_as_table = (old_relation is not none and not old_relation.is_table) -%}\n\n  {%- set should_truncate = (non_destructive_mode and full_refresh_mode and exists_as_table) -%}\n  {%- set should_drop = (not should_truncate and (full_refresh_mode or exists_not_as_table)) -%}\n  {%- set force_create = (flags.FULL_REFRESH and not flags.NON_DESTRUCTIVE) -%}\n\n  -- setup\n  {% if old_relation is none -%}\n    -- noop\n  {%- elif should_truncate -%}\n    {{ adapter.truncate_relation(old_relation) }}\n  {%- elif should_drop -%}\n    {{ adapter.drop_relation(old_relation) }}\n    {%- set old_relation = none -%}\n  {%- endif %}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% if force_create or old_relation is none -%}\n    {%- call statement('main') -%}\n      {{ create_table_as(False, target_relation, sql) }}\n    {%- endcall -%}\n  {%- else -%}\n     {%- call statement() -%}\n\n       {% set tmp_table_sql -%}\n         {# We are using a subselect instead of a CTE here to allow PostgreSQL to use indexes. -#}\n         select * from (\n           {{ sql }}\n         ) as dbt_incr_sbq\n         where ({{ sql_where }})\n           or ({{ sql_where }}) is null\n       {%- endset %}\n\n       {{ dbt.create_table_as(True, tmp_relation, tmp_table_sql) }}\n\n     {%- endcall -%}\n\n     {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                           to_schema=schema,\n                                           to_table=identifier) }}\n\n     {%- call statement('main') -%}\n       {% set dest_columns = adapter.get_columns_in_table(schema, identifier) %}\n       {% set dest_cols_csv = dest_columns | map(attribute='quoted') | join(', ') %}\n\n       {% if unique_key is not none -%}\n\n         {{ dbt__incremental_delete(target_relation, tmp_relation) }}\n\n       {%- endif %}\n\n       insert into {{ target_relation }} ({{ dest_cols_csv }})\n       (\n         select {{ dest_cols_csv }}\n         from {{ tmp_relation.include(schema=False) }}\n       );\n     {% endcall %}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n\n{%- endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__create_table_as": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "default__create_table_as", "package_name": "dbt", "unique_id": "macro.dbt.default__create_table_as", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.noop_statement": {"original_file_path": "macros\\core.sql", "tags": [], "name": "noop_statement", "package_name": "dbt", "unique_id": "macro.dbt.noop_statement", "path": "macros\\core.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro statement(name=None, fetch_result=False, auto_begin=True) -%}\n  {%- if execute: -%}\n    {%- set sql = render(caller()) -%}\n\n    {%- if name == 'main' -%}\n      {{ log('Writing runtime SQL for node \"{}\"'.format(model['unique_id'])) }}\n      {{ write(sql) }}\n    {%- endif -%}\n\n    {%- set status, res = adapter.execute(sql, auto_begin=auto_begin, fetch=fetch_result) -%}\n    {%- if name is not none -%}\n      {{ store_result(name, status=status, agate_table=res) }}\n    {%- endif -%}\n\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro noop_statement(name=None, status=None, res=None) -%}\n  {%- set sql = render(caller()) -%}\n\n  {%- if name == 'main' -%}\n    {{ log('Writing runtime SQL for node \"{}\"'.format(model['unique_id'])) }}\n    {{ write(sql) }}\n  {%- endif -%}\n\n  {%- if name is not none -%}\n    {{ store_result(name, status=status, agate_table=res) }}\n  {%- endif -%}\n\n{%- endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.load_csv_rows": {"original_file_path": "macros\\materializations\\seed\\seed.sql", "tags": [], "name": "load_csv_rows", "package_name": "dbt", "unique_id": "macro.dbt.load_csv_rows", "path": "macros\\materializations\\seed\\seed.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro create_csv_table(model) -%}\n  {{ adapter_macro('create_csv_table', model) }}\n{%- endmacro %}\n\n{% macro reset_csv_table(model, full_refresh, old_relation) -%}\n  {{ adapter_macro('reset_csv_table', model, full_refresh, old_relation) }}\n{%- endmacro %}\n\n{% macro load_csv_rows(model) -%}\n  {{ adapter_macro('load_csv_rows', model) }}\n{%- endmacro %}\n\n{% macro default__create_csv_table(model) %}\n  {%- set agate_table = model['agate_table'] -%}\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n\n  {% set sql %}\n    create table {{ this.render(False) }} (\n        {%- for col_name in agate_table.column_names -%}\n            {%- set inferred_type = adapter.convert_type(agate_table, loop.index0) -%}\n            {%- set type = column_override.get(col_name, inferred_type) -%}\n            {{ col_name | string }} {{ type }} {%- if not loop.last -%}, {%- endif -%}\n        {%- endfor -%}\n    )\n  {% endset %}\n\n  {% call statement('_') -%}\n    {{ sql }}\n  {%- endcall %}\n\n  {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__reset_csv_table(model, full_refresh, old_relation) %}\n    {% set sql = \"\" %}\n    {% if full_refresh %}\n        {{ adapter.drop_relation(old_relation) }}\n        {% set sql = create_csv_table(model) %}\n    {% else %}\n        {{ adapter.truncate_relation(old_relation) }}\n        {% set sql = \"truncate table \" ~ old_relation %}\n    {% endif %}\n\n    {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__load_csv_rows(model) %}\n    {% set agate_table = model['agate_table'] %}\n    {% set cols_sql = \", \".join(agate_table.column_names) %}\n    {% set bindings = [] %}\n\n    {% set statements = [] %}\n\n    {% for chunk in agate_table.rows | batch(10000) %}\n        {% set bindings = [] %}\n\n        {% for row in chunk %}\n            {% set _ = bindings.extend(row) %}\n        {% endfor %}\n\n        {% set sql %}\n            insert into {{ this.render(False) }} ({{ cols_sql }}) values\n            {% for row in chunk -%}\n                ({%- for column in agate_table.column_names -%}\n                    %s\n                    {%- if not loop.last%},{%- endif %}\n                {%- endfor -%})\n                {%- if not loop.last%},{%- endif %}\n            {%- endfor %}\n        {% endset %}\n\n        {% set _ = adapter.add_query(sql, bindings=bindings, abridge_sql_log=True) %}\n\n        {% if loop.index0 == 0 %}\n            {% set _ = statements.append(sql) %}\n        {% endif %}\n    {% endfor %}\n\n    {# Return SQL so we can render it out into the compiled files #}\n    {{ return(statements[0]) }}\n{% endmacro %}\n\n\n{% materialization seed, default %}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set csv_table = model[\"agate_table\"] -%}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% set create_table_sql = \"\" %}\n  {% if exists_as_view %}\n    {{ exceptions.raise_compiler_error(\"Cannot seed to '{}', it is a view\".format(old_relation)) }}\n  {% elif exists_as_table %}\n    {% set create_table_sql = reset_csv_table(model, full_refresh_mode, old_relation) %}\n  {% else %}\n    {% set create_table_sql = create_csv_table(model) %}\n  {% endif %}\n\n  {% set status = 'CREATE' if full_refresh_mode else 'INSERT' %}\n  {% set num_rows = (csv_table.rows | length) %}\n  {% set sql = load_csv_rows(model) %}\n\n  {% call noop_statement('main', status ~ ' ' ~ num_rows) %}\n    {{ create_table_sql }};\n    -- dbt seed --\n    {{ sql }}\n  {% endcall %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_archive_default": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "materialization_archive_default", "package_name": "dbt", "unique_id": "macro.dbt.materialization_archive_default", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.column_list_for_create_table": {"original_file_path": "macros\\materializations\\helpers.sql", "tags": [], "name": "column_list_for_create_table", "package_name": "dbt", "unique_id": "macro.dbt.column_list_for_create_table", "path": "macros\\materializations\\helpers.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro run_hooks(hooks, inside_transaction=True) %}\n  {% for hook in hooks | selectattr('transaction', 'equalto', inside_transaction)  %}\n    {% if not inside_transaction and loop.first %}\n      {% call statement(auto_begin=inside_transaction) %}\n        commit;\n      {% endcall %}\n    {% endif %}\n    {% call statement(auto_begin=inside_transaction) %}\n      {{ hook.get('sql') }}\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n\n{% macro column_list(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro column_list_for_create_table(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {{ col.data_type }} {%- if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro make_hook_config(sql, inside_transaction) %}\n    {{ tojson({\"sql\": sql, \"transaction\": inside_transaction}) }}\n{% endmacro %}\n\n\n{% macro before_begin(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n\n{% macro in_transaction(sql) %}\n    {{ make_hook_config(sql, inside_transaction=True) }}\n{% endmacro %}\n\n\n{% macro after_commit(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n{% macro drop_relation_if_exists(relation) %}\n  {% if relation is not none %}\n    {{ adapter.drop_relation(relation) }}\n  {% endif %}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__get_base_catalog": {"original_file_path": "macros\\catalog\\redshift_catalog.sql", "tags": [], "name": "redshift__get_base_catalog", "package_name": "dbt", "unique_id": "macro.dbt.redshift__get_base_catalog", "path": "macros\\catalog\\redshift_catalog.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro redshift__get_base_catalog() -%}\n  {%- call statement('base_catalog', fetch_result=True) -%}\n    with late_binding as (\n      select\n        table_schema,\n        table_name,\n        'LATE BINDING VIEW'::varchar as table_type,\n        null::text as table_comment,\n\n        column_name,\n        column_index,\n        column_type,\n        null::text as column_comment\n      from pg_get_late_binding_view_cols()\n        cols(table_schema name, table_name name, column_name name,\n             column_type varchar,\n             column_index int)\n        order by \"column_index\"\n    ),\n\n    tables as (\n\n      select\n          table_schema,\n          table_name,\n          table_type\n\n      from information_schema.tables\n\n    ),\n\n    table_owners as (\n\n        select\n            schemaname as table_schema,\n            tablename as table_name,\n            tableowner as table_owner\n\n        from pg_tables\n\n        union all\n\n        select\n            schemaname as table_schema,\n            viewname as table_name,\n            viewowner as table_owner\n\n        from pg_views\n\n    ),\n\n    columns as (\n\n        select\n            table_schema,\n            table_name,\n            null::varchar as table_comment,\n\n            column_name,\n            ordinal_position as column_index,\n            data_type as column_type,\n            null::varchar as column_comment\n\n\n        from information_schema.columns\n\n    ),\n\n    unioned as (\n\n        select *\n        from tables\n        join columns using (table_schema, table_name)\n\n        union all\n\n        select *\n        from late_binding\n\n    )\n\n    select *,\n        table_schema || '.' || table_name as table_id\n\n    from unioned\n    join table_owners using (table_schema, table_name)\n\n    where table_schema != 'information_schema'\n      and table_schema not like 'pg_%'\n\n    order by \"column_index\"\n  {%- endcall -%}\n\n  {{ return(load_result('base_catalog').table) }}\n{%- endmacro %}\n\n{% macro redshift__get_extended_catalog() %}\n  {%- call statement('extended_catalog', fetch_result=True) -%}\n\n    select\n        \"schema\" || '.' || \"table\" as table_id,\n\n        'Encoded'::text as \"stats:encoded:label\",\n        encoded as \"stats:encoded:value\",\n        'Indicates whether any column in the table has compression encoding defined.'::text as \"stats:encoded:description\",\n        true as \"stats:encoded:include\",\n\n        'Dist Style' as \"stats:diststyle:label\",\n        diststyle as \"stats:diststyle:value\",\n        'Distribution style or distribution key column, if key distribution is defined.'::text as \"stats:diststyle:description\",\n        true as \"stats:diststyle:include\",\n\n        'Sort Key 1' as \"stats:sortkey1:label\",\n        -- handle 0xFF byte in response for interleaved sort styles\n        case\n            when sortkey1 like 'INTERLEAVED%' then 'INTERLEAVED'::text\n            else sortkey1\n        end as \"stats:sortkey1:value\",\n        'First column in the sort key.'::text as \"stats:sortkey1:description\",\n        (sortkey1 is not null) as \"stats:sortkey1:include\",\n\n        'Max Varchar' as \"stats:max_varchar:label\",\n        max_varchar as \"stats:max_varchar:value\",\n        'Size of the largest column that uses a VARCHAR data type.'::text as \"stats:max_varchar:description\",\n        true as \"stats:max_varchar:include\",\n\n        -- exclude this, as the data is strangely returned with null-byte characters\n        'Sort Key 1 Encoding' as \"stats:sortkey1_enc:label\",\n        sortkey1_enc as \"stats:sortkey1_enc:value\",\n        'Compression encoding of the first column in the sort key.' as \"stats:sortkey1_enc:description\",\n        false as \"stats:sortkey1_enc:include\",\n\n        '# Sort Keys' as \"stats:sortkey_num:label\",\n        sortkey_num as \"stats:sortkey_num:value\",\n        'Number of columns defined as sort keys.' as \"stats:sortkey_num:description\",\n        (sortkey_num > 0) as \"stats:sortkey_num:include\",\n\n        'Approximate Size' as \"stats:size:label\",\n        size / 1000000.0 as \"stats:size:value\",\n        'Approximate size of the table, calculated from a count of 1MB blocks'::text as \"stats:size:description\",\n        true as \"stats:size:include\",\n\n        'Disk Utilization' as \"stats:pct_used:label\",\n        pct_used / 100.0 as \"stats:pct_used:value\",\n        'Percent of available space that is used by the table.'::text as \"stats:pct_used:description\",\n        true as \"stats:pct_used:include\",\n\n        'Unsorted %' as \"stats:unsorted:label\",\n        unsorted / 100.0 as \"stats:unsorted:value\",\n        'Percent of unsorted rows in the table.'::text as \"stats:unsorted:description\",\n        (unsorted is not null) as \"stats:unsorted:include\",\n\n        'Stats Off' as \"stats:stats_off:label\",\n        stats_off as \"stats:stats_off:value\",\n        'Number that indicates how stale the table statistics are; 0 is current, 100 is out of date.'::text as \"stats:stats_off:description\",\n        true as \"stats:stats_off:include\",\n\n        'Approximate Row Count' as \"stats:rows:label\",\n        tbl_rows as \"stats:rows:value\",\n        'Approximate number of rows in the table. This value includes rows marked for deletion, but not yet vacuumed.'::text as \"stats:rows:description\",\n        true as \"stats:rows:include\",\n\n        'Sort Key Skew' as \"stats:skew_sortkey1:label\",\n        skew_sortkey1 as \"stats:skew_sortkey1:value\",\n        'Ratio of the size of the largest non-sort key column to the size of the first column of the sort key.'::text as \"stats:skew_sortkey1:description\",\n        (skew_sortkey1 is not null) as \"stats:skew_sortkey1:include\",\n\n        'Skew Rows' as \"stats:skew_rows:label\",\n        skew_rows as \"stats:skew_rows:value\",\n        'Ratio of the number of rows in the slice with the most rows to the number of rows in the slice with the fewest rows.'::text as \"stats:skew_rows:description\",\n        (skew_rows is not null) as \"stats:skew_rows:include\"\n\n    from svv_table_info\n\n  {%- endcall -%}\n\n  {{ return(load_result('extended_catalog').table) }}\n\n{% endmacro %}\n\n{% macro redshift__can_select_from(table_name) %}\n\n  {%- call statement('has_table_privilege', fetch_result=True) -%}\n\n    select has_table_privilege(current_user, '{{ table_name }}', 'SELECT') as can_select\n\n  {%- endcall -%}\n\n  {% set can_select = load_result('has_table_privilege').table[0]['can_select'] %}\n  {{ return(can_select) }}\n\n{% endmacro %}\n\n{% macro redshift__no_svv_table_info_warning() %}\n\n    {% set msg %}\n\n    Warning: The database user \"{{ target.user }}\" has insufficient permissions to\n    query the \"svv_table_info\" table. Please grant SELECT permissions on this table\n    to the \"{{ target.user }}\" user to fetch extended table details from Redshift.\n\n    {% endset %}\n\n    {{ log(msg, info=True) }}\n\n{% endmacro %}\n\n\n{% macro redshift__get_catalog() %}\n\n    {#-- Compute a left-outer join in memory. Some Redshift queries are\n      -- leader-only, and cannot be joined to other compute-based queries #}\n\n    {% set catalog = redshift__get_base_catalog() %}\n\n    {% set select_extended =  redshift__can_select_from('svv_table_info') %}\n    {% if select_extended %}\n        {% set extended_catalog = redshift__get_extended_catalog() %}\n        {% set catalog = catalog.join(extended_catalog, 'table_id') %}\n    {% else %}\n        {{ redshift__no_svv_table_info_warning() }}\n    {% endif %}\n\n    {{ return(catalog.exclude(['table_id'])) }}\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__load_csv_rows": {"original_file_path": "macros\\materializations\\seed\\bigquery.sql", "tags": [], "name": "bigquery__load_csv_rows", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__load_csv_rows", "path": "macros\\materializations\\seed\\bigquery.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro bigquery__create_csv_table(model) %}\n    -- no-op\n{% endmacro %}\n\n{% macro bigquery__reset_csv_table(model, full_refresh, old_relation) %}\n    {{ adapter.drop_relation(old_relation) }}\n{% endmacro %}\n\n{% macro bigquery__load_csv_rows(model) %}\n\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n  {{ adapter.load_dataframe(model['schema'], model['alias'], model['agate_table'], column_override) }}\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.default__handle_existing_table": {"original_file_path": "macros\\materializations\\view\\bq_snowflake_view.sql", "tags": [], "name": "default__handle_existing_table", "package_name": "dbt", "unique_id": "macro.dbt.default__handle_existing_table", "path": "macros\\materializations\\view\\bq_snowflake_view.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {{ adapter_macro(\"dbt.handle_existing_table\", full_refresh, non_destructive_mode, old_relation) }}\n{% endmacro %}\n\n{% macro default__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- endif -%}\n{% endmacro %}\n\n{% macro bigquery__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if full_refresh and not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- else -%}\n      {{ exceptions.relation_wrong_type(old_relation, 'view') }}\n    {%- endif -%}\n{% endmacro %}\n\n\n{# /*\n       Core materialization implementation. BigQuery and Snowflake are similar\n       because both can use `create or replace view` where the resulting view schema\n       is not necessarily the same as the existing view. On Redshift, this would\n       result in: ERROR:  cannot change number of columns in view\n\n       This implementation is superior to the create_temp, swap_with_existing, drop_old\n       paradigm because transactions don't run DDL queries atomically on Snowflake. By using\n       `create or replace view`, the materialization becomes atomic in nature.\n    */\n#}\n\n{% macro impl_view_materialization(run_outside_transaction_hooks=True) %}\n  {%- set identifier = model['alias'] -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n\n  {%- set old_relation = adapter.get_relation(\n      schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set target_relation = api.Relation.create(\n      identifier=identifier, schema=schema,\n      type='view') -%}\n\n  {%- set should_ignore = non_destructive_mode and exists_as_view %}\n  {%- set has_transactional_hooks = (hooks | selectattr('transaction', 'equalto', True) | list | length) > 0 %}\n\n  {% if run_outside_transaction_hooks %}\n      -- no transactions on BigQuery\n      {{ run_hooks(pre_hooks, inside_transaction=False) }}\n  {% endif %}\n\n  -- `BEGIN` happens here on Snowflake\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- If there's a table with the same name and we weren't told to full refresh,\n  -- that's an error. If we were told to full refresh, drop it. This behavior differs\n  -- for Snowflake and BigQuery, so multiple dispatch is used.\n  {%- if old_relation is not none and old_relation.is_table -%}\n    {{ handle_existing_table(flags.FULL_REFRESH, non_destructive_mode, old_relation) }}\n  {%- endif -%}\n\n  -- build model\n  {% if non_destructive_mode -%}\n    {% call noop_statement('main', status=\"PASS\", res=None) -%}\n      -- Not running : non-destructive mode\n      {{ sql }}\n    {%- endcall %}\n  {%- else -%}\n    {% call statement('main') -%}\n      {{ create_view_as(target_relation, sql) }}\n    {%- endcall %}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  {#\n      -- Don't commit in non-destructive mode _unless_ there are in-transaction hooks\n      -- TODO : Figure out some other way of doing this that isn't as fragile\n  #}\n  {% if has_transactional_hooks or not should_ignore %}\n      {{ adapter.commit() }}\n  {% endif %}\n\n  {% if run_outside_transaction_hooks %}\n      -- No transactions on BigQuery\n      {{ run_hooks(post_hooks, inside_transaction=False) }}\n  {% endif %}\n{% endmacro %}\n\n{% materialization view, adapter='bigquery' -%}\n    {{ impl_view_materialization(run_outside_transaction_hooks=False) }}\n{%- endmaterialization %}\n\n{% materialization view, adapter='snowflake' -%}\n    {{ impl_view_materialization() }}\n{%- endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__create_table_as": {"original_file_path": "macros\\adapters\\bigquery.sql", "tags": [], "name": "bigquery__create_table_as", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__create_table_as", "path": "macros\\adapters\\bigquery.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro partition_by(raw_partition_by) %}\n  {%- if raw_partition_by is none -%}\n    {{ return('') }}\n  {% endif %}\n\n  {% set partition_by_clause %}\n    partition by {{ raw_partition_by }}\n  {%- endset -%}\n\n  {{ return(partition_by_clause) }}\n{%- endmacro -%}\n\n\n{% macro cluster_by(raw_cluster_by) %}\n  {%- if raw_cluster_by is not none -%}\n  cluster by \n  {% if raw_cluster_by is string -%}\n    {% set raw_cluster_by = [raw_cluster_by] %}\n  {%- endif -%}\n  {%- for cluster in raw_cluster_by -%}\n    {{ cluster }}\n    {%- if not loop.last -%},{%- endif -%}\n  {%- endfor -%}\n\n  {% endif %}\n\n{%- endmacro -%}\n\n\n{% macro bigquery__create_table_as(temporary, relation, sql) -%}\n  {%- set raw_partition_by = config.get('partition_by', none) -%}\n  {%- set raw_cluster_by = config.get('cluster_by', none) -%}\n\n  create or replace table {{ relation }}\n  {{ partition_by(raw_partition_by) }}\n  {{ cluster_by(raw_cluster_by) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro bigquery__create_view_as(relation, sql) -%}\n  create or replace view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.adapter_macro": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "adapter_macro", "package_name": "dbt", "unique_id": "macro.dbt.adapter_macro", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.make_hook_config": {"original_file_path": "macros\\materializations\\helpers.sql", "tags": [], "name": "make_hook_config", "package_name": "dbt", "unique_id": "macro.dbt.make_hook_config", "path": "macros\\materializations\\helpers.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro run_hooks(hooks, inside_transaction=True) %}\n  {% for hook in hooks | selectattr('transaction', 'equalto', inside_transaction)  %}\n    {% if not inside_transaction and loop.first %}\n      {% call statement(auto_begin=inside_transaction) %}\n        commit;\n      {% endcall %}\n    {% endif %}\n    {% call statement(auto_begin=inside_transaction) %}\n      {{ hook.get('sql') }}\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n\n{% macro column_list(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro column_list_for_create_table(columns) %}\n  {%- for col in columns %}\n    {{ adapter.quote(col.name) }} {{ col.data_type }} {%- if not loop.last %},{% endif %}\n  {% endfor -%}\n{% endmacro %}\n\n\n{% macro make_hook_config(sql, inside_transaction) %}\n    {{ tojson({\"sql\": sql, \"transaction\": inside_transaction}) }}\n{% endmacro %}\n\n\n{% macro before_begin(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n\n{% macro in_transaction(sql) %}\n    {{ make_hook_config(sql, inside_transaction=True) }}\n{% endmacro %}\n\n\n{% macro after_commit(sql) %}\n    {{ make_hook_config(sql, inside_transaction=False) }}\n{% endmacro %}\n\n{% macro drop_relation_if_exists(relation) %}\n  {% if relation is not none %}\n    {{ adapter.drop_relation(relation) }}\n  {% endif %}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__reset_csv_table": {"original_file_path": "macros\\materializations\\seed\\bigquery.sql", "tags": [], "name": "bigquery__reset_csv_table", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__reset_csv_table", "path": "macros\\materializations\\seed\\bigquery.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro bigquery__create_csv_table(model) %}\n    -- no-op\n{% endmacro %}\n\n{% macro bigquery__reset_csv_table(model, full_refresh, old_relation) %}\n    {{ adapter.drop_relation(old_relation) }}\n{% endmacro %}\n\n{% macro bigquery__load_csv_rows(model) %}\n\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n  {{ adapter.load_dataframe(model['schema'], model['alias'], model['agate_table'], column_override) }}\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.create_temporary_table": {"original_file_path": "macros\\materializations\\archive\\archive.sql", "tags": [], "name": "create_temporary_table", "package_name": "dbt", "unique_id": "macro.dbt.create_temporary_table", "path": "macros\\materializations\\archive\\archive.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{#\n    Create SCD Hash SQL fields cross-db\n#}\n\n{% macro archive_scd_hash() %}\n  {{ adapter_macro('archive_scd_hash') }}\n{% endmacro %}\n\n{% macro default__archive_scd_hash() %}\n    md5(\"dbt_pk\" || '|' || \"dbt_updated_at\")\n{% endmacro %}\n\n{% macro bigquery__archive_scd_hash() %}\n    to_hex(md5(concat(cast(`dbt_pk` as string), '|', cast(`dbt_updated_at` as string))))\n{% endmacro %}\n\n{% macro create_temporary_table(sql, relation) %}\n  {{ return(adapter_macro('create_temporary_table', sql, relation)) }}\n{% endmacro %}\n\n{% macro default__create_temporary_table(sql, relation) %}\n    {% call statement() %}\n        {{ create_table_as(True, relation, sql) }}\n    {% endcall %}\n    {{ return(relation) }}\n{% endmacro %}\n\n{% macro bigquery__create_temporary_table(sql, relation) %}\n    {% set tmp_relation = adapter.create_temporary_table(sql) %}\n    {{ return(tmp_relation) }}\n{% endmacro %}\n\n\n{#\n    Add new columns to the table if applicable\n#}\n{% macro create_columns(relation, columns) %}\n  {{ adapter_macro('create_columns', relation, columns) }}\n{% endmacro %}\n\n{% macro default__create_columns(relation, columns) %}\n  {% for column in columns %}\n    {% call statement() %}\n      alter table {{ relation }} add column \"{{ column.name }}\" {{ column.data_type }};\n    {% endcall %}\n  {% endfor %}\n{% endmacro %}\n\n{% macro bigquery__create_columns(relation, columns) %}\n  {{ adapter.alter_table_add_columns(relation, columns) }}\n{% endmacro %}\n\n{#\n    Run the update part of an archive query. Different databases have\n    tricky differences in their `update` semantics. Table projection is\n    not allowed on Redshift/pg, but is effectively required on bq.\n#}\n\n{% macro archive_update(target_relation, tmp_relation) %}\n    {{ adapter_macro('archive_update', target_relation, tmp_relation) }}\n{% endmacro %}\n\n{% macro default__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }}\n    set {{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = {{ target_relation }}.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n{% macro bigquery__archive_update(target_relation, tmp_relation) %}\n    update {{ target_relation }} as dest\n    set dest.{{ adapter.quote('valid_to') }} = tmp.{{ adapter.quote('valid_to') }}\n    from {{ tmp_relation }} as tmp\n    where tmp.{{ adapter.quote('scd_id') }} = dest.{{ adapter.quote('scd_id') }}\n      and {{ adapter.quote('change_type') }} = 'update';\n{% endmacro %}\n\n\n{#\n    Cross-db compatible archival implementation\n#}\n{% macro archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) %}\n\n    {% set timestamp_column = api.Column.create('_', 'timestamp') %}\n\n    with current_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }} {% if not loop.last %},{% endif %}\n            {% endfor %},\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ updated_at }} as {{ adapter.quote('valid_from') }},\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ source_relation }}\n\n    ),\n\n    archived_data as (\n\n        select\n            {% for col in source_columns %}\n                {{ adapter.quote(col.name) }},\n            {% endfor %}\n            {{ updated_at }} as {{ adapter.quote('dbt_updated_at') }},\n            {{ unique_key }} as {{ adapter.quote('dbt_pk') }},\n            {{ adapter.quote('valid_from') }},\n            {{ adapter.quote('valid_to') }} as {{ adapter.quote('tmp_valid_to') }}\n        from {{ target_relation }}\n\n    ),\n\n    insertions as (\n\n        select\n            current_data.*,\n            {{ timestamp_column.literal('null') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is null or (\n          archived_data.{{ adapter.quote('dbt_pk') }} is not null and\n          current_data.{{ adapter.quote('dbt_updated_at') }} > archived_data.{{ adapter.quote('dbt_updated_at') }} and\n          archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n        )\n    ),\n\n    updates as (\n\n        select\n            archived_data.*,\n            current_data.{{ adapter.quote('dbt_updated_at') }} as {{ adapter.quote('valid_to') }}\n        from current_data\n        left outer join archived_data\n          on archived_data.{{ adapter.quote('dbt_pk') }} = current_data.{{ adapter.quote('dbt_pk') }}\n        where archived_data.{{ adapter.quote('dbt_pk') }} is not null\n          and archived_data.{{ adapter.quote('dbt_updated_at') }} < current_data.{{ adapter.quote('dbt_updated_at') }}\n          and archived_data.{{ adapter.quote('tmp_valid_to') }} is null\n    ),\n\n    merged as (\n\n      select *, 'update' as {{ adapter.quote('change_type') }} from updates\n      union all\n      select *, 'insert' as {{ adapter.quote('change_type') }} from insertions\n\n    )\n\n    select *,\n        {{ archive_scd_hash() }} as {{ adapter.quote('scd_id') }}\n    from merged\n\n{% endmacro %}\n\n{% materialization archive, default %}\n  {%- set config = model['config'] -%}\n\n  {%- set target_schema = config.get('target_schema') -%}\n  {%- set target_table = config.get('target_table') -%}\n\n  {%- set source_schema = config.get('source_schema') -%}\n  {%- set source_table = config.get('source_table') -%}\n\n  {{ create_schema(target_schema) }}\n\n  {%- set source_relation = adapter.get_relation(\n      schema=source_schema,\n      identifier=source_table) -%}\n\n  {%- set target_relation = adapter.get_relation(\n      schema=target_schema,\n      identifier=target_table) -%}\n\n  {%- if source_relation is none -%}\n    {{ exceptions.missing_relation(source_relation) }}\n  {%- endif -%}\n\n  {%- if target_relation is none -%}\n    {%- set target_relation = api.Relation.create(\n        schema=target_schema,\n        identifier=target_table) -%}\n  {%- elif not target_relation.is_table -%}\n    {{ exceptions.relation_wrong_type(target_relation, 'table') }}\n  {%- endif -%}\n\n  {%- set source_columns = adapter.get_columns_in_table(source_schema, source_table) -%}\n  {%- set unique_key = config.get('unique_key') -%}\n  {%- set updated_at = config.get('updated_at') -%}\n  {%- set dest_columns = source_columns + [\n      api.Column.create('valid_from', 'timestamp'),\n      api.Column.create('valid_to', 'timestamp'),\n      api.Column.create('scd_id', 'string'),\n      api.Column.create('dbt_updated_at', 'timestamp'),\n  ] -%}\n\n  {% call statement() %}\n    {{ create_archive_table(target_relation, dest_columns) }}\n  {% endcall %}\n\n  {% set missing_columns = adapter.get_missing_columns(source_relation.schema, source_relation.table,\n                                                       target_relation.schema, target_relation.table) %}\n\n  {{ create_columns(target_relation, missing_columns) }}\n\n\n  {%- set identifier = model['alias'] -%}\n  {%- set tmp_identifier = identifier + '__dbt_archival_tmp' -%}\n\n  {% set tmp_table_sql -%}\n\n      with dbt_archive_sbq as (\n        {{ archive_select(source_relation, target_relation, source_columns, unique_key, updated_at) }}\n      )\n      select * from dbt_archive_sbq\n\n  {%- endset %}\n\n  {%- set tmp_relation = api.Relation.create(identifier=tmp_identifier, type='table') -%}\n  {%- set tmp_relation = create_temporary_table(tmp_table_sql, tmp_relation) -%}\n\n  {{ adapter.expand_target_column_types(temp_table=tmp_identifier,\n                                        to_schema=target_schema,\n                                        to_table=target_table) }}\n\n  {% call statement('_') -%}\n    {{ archive_update(target_relation, tmp_relation) }}\n  {% endcall %}\n\n  {% call statement('main') -%}\n\n    insert into {{ target_relation }} (\n      {{ column_list(dest_columns) }}\n    )\n    select {{ column_list(dest_columns) }} from {{ tmp_relation }}\n    where {{ adapter.quote('change_type') }} = 'insert';\n  {% endcall %}\n\n  {{ adapter.commit() }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.create_table_as": {"original_file_path": "macros\\adapters\\common.sql", "tags": [], "name": "create_table_as", "package_name": "dbt", "unique_id": "macro.dbt.create_table_as", "path": "macros\\adapters\\common.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro adapter_macro(name) -%}\n{% set original_name = name %}\n  {% if '.' in name %}\n    {% set package_name, name = name.split(\".\", 1) %}\n  {% else %}\n    {% set package_name = none %}\n  {% endif %}\n\n  {% if package_name is none %}\n    {% set package_context = context %}\n  {% elif package_name in context %}\n    {% set package_context = context[package_name] %}\n  {% else %}\n    {% set error_msg %}\n        In adapter_macro: could not find package '{{package_name}}', called with '{{original_name}}'\n    {% endset %}\n    {{ exceptions.raise_compiler_error(error_msg | trim) }}\n  {% endif %}\n\n  {%- set separator = '__' -%}\n  {%- set search_name = adapter.type() + separator + name -%}\n  {%- set default_name = 'default' + separator + name -%}\n\n  {%- if package_context.get(search_name) is not none -%}\n    {{ return(package_context[search_name](*varargs, **kwargs)) }}\n  {%- else -%}\n    {{ return(package_context[default_name](*varargs, **kwargs)) }}\n  {%- endif -%}\n{%- endmacro %}\n\n{% macro create_schema(schema_name) %}\n  {{ adapter.create_schema(schema_name) }}\n{% endmacro %}\n\n{% macro create_table_as(temporary, relation, sql) -%}\n  {{ adapter_macro('create_table_as', temporary, relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_table_as(temporary, relation, sql) -%}\n  create {% if temporary: -%}temporary{%- endif %} table\n    {{ relation.include(schema=(not temporary)) }}\n  as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_view_as(relation, sql) -%}\n  {{ adapter_macro('create_view_as', relation, sql) }}\n{%- endmacro %}\n\n{% macro default__create_view_as(relation, sql) -%}\n  create view {{ relation }} as (\n    {{ sql }}\n  );\n{% endmacro %}\n\n\n{% macro create_archive_table(relation, columns) -%}\n  {{ adapter_macro('create_archive_table', relation, columns) }}\n{%- endmacro %}\n\n{% macro default__create_archive_table(relation, columns) -%}\n  create table if not exists {{ relation }} (\n    {{ column_list_for_create_table(columns) }}\n  );\n{% endmacro %}\n\n\n{% macro get_catalog() -%}\n  {{ return(adapter_macro('get_catalog')) }}\n{%- endmacro %}\n\n{% macro default__get_catalog() -%}\n\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_catalog not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}\n\n{% macro get_relations() -%}\n  {{ return(adapter_macro('get_relations')) }}\n{% endmacro %}\n\n\n{% macro default__get_relations() -%}\n  {% set typename = adapter.type() %}\n  {% set msg -%}\n    get_relations not implemented for {{ typename }}\n  {%- endset %}\n\n  {{ exceptions.raise_compiler_error(msg) }}\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.impl_view_materialization": {"original_file_path": "macros\\materializations\\view\\bq_snowflake_view.sql", "tags": [], "name": "impl_view_materialization", "package_name": "dbt", "unique_id": "macro.dbt.impl_view_materialization", "path": "macros\\materializations\\view\\bq_snowflake_view.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {{ adapter_macro(\"dbt.handle_existing_table\", full_refresh, non_destructive_mode, old_relation) }}\n{% endmacro %}\n\n{% macro default__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- endif -%}\n{% endmacro %}\n\n{% macro bigquery__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if full_refresh and not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- else -%}\n      {{ exceptions.relation_wrong_type(old_relation, 'view') }}\n    {%- endif -%}\n{% endmacro %}\n\n\n{# /*\n       Core materialization implementation. BigQuery and Snowflake are similar\n       because both can use `create or replace view` where the resulting view schema\n       is not necessarily the same as the existing view. On Redshift, this would\n       result in: ERROR:  cannot change number of columns in view\n\n       This implementation is superior to the create_temp, swap_with_existing, drop_old\n       paradigm because transactions don't run DDL queries atomically on Snowflake. By using\n       `create or replace view`, the materialization becomes atomic in nature.\n    */\n#}\n\n{% macro impl_view_materialization(run_outside_transaction_hooks=True) %}\n  {%- set identifier = model['alias'] -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n\n  {%- set old_relation = adapter.get_relation(\n      schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set target_relation = api.Relation.create(\n      identifier=identifier, schema=schema,\n      type='view') -%}\n\n  {%- set should_ignore = non_destructive_mode and exists_as_view %}\n  {%- set has_transactional_hooks = (hooks | selectattr('transaction', 'equalto', True) | list | length) > 0 %}\n\n  {% if run_outside_transaction_hooks %}\n      -- no transactions on BigQuery\n      {{ run_hooks(pre_hooks, inside_transaction=False) }}\n  {% endif %}\n\n  -- `BEGIN` happens here on Snowflake\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- If there's a table with the same name and we weren't told to full refresh,\n  -- that's an error. If we were told to full refresh, drop it. This behavior differs\n  -- for Snowflake and BigQuery, so multiple dispatch is used.\n  {%- if old_relation is not none and old_relation.is_table -%}\n    {{ handle_existing_table(flags.FULL_REFRESH, non_destructive_mode, old_relation) }}\n  {%- endif -%}\n\n  -- build model\n  {% if non_destructive_mode -%}\n    {% call noop_statement('main', status=\"PASS\", res=None) -%}\n      -- Not running : non-destructive mode\n      {{ sql }}\n    {%- endcall %}\n  {%- else -%}\n    {% call statement('main') -%}\n      {{ create_view_as(target_relation, sql) }}\n    {%- endcall %}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  {#\n      -- Don't commit in non-destructive mode _unless_ there are in-transaction hooks\n      -- TODO : Figure out some other way of doing this that isn't as fragile\n  #}\n  {% if has_transactional_hooks or not should_ignore %}\n      {{ adapter.commit() }}\n  {% endif %}\n\n  {% if run_outside_transaction_hooks %}\n      -- No transactions on BigQuery\n      {{ run_hooks(post_hooks, inside_transaction=False) }}\n  {% endif %}\n{% endmacro %}\n\n{% materialization view, adapter='bigquery' -%}\n    {{ impl_view_materialization(run_outside_transaction_hooks=False) }}\n{%- endmaterialization %}\n\n{% materialization view, adapter='snowflake' -%}\n    {{ impl_view_materialization() }}\n{%- endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.redshift__get_extended_catalog": {"original_file_path": "macros\\catalog\\redshift_catalog.sql", "tags": [], "name": "redshift__get_extended_catalog", "package_name": "dbt", "unique_id": "macro.dbt.redshift__get_extended_catalog", "path": "macros\\catalog\\redshift_catalog.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro redshift__get_base_catalog() -%}\n  {%- call statement('base_catalog', fetch_result=True) -%}\n    with late_binding as (\n      select\n        table_schema,\n        table_name,\n        'LATE BINDING VIEW'::varchar as table_type,\n        null::text as table_comment,\n\n        column_name,\n        column_index,\n        column_type,\n        null::text as column_comment\n      from pg_get_late_binding_view_cols()\n        cols(table_schema name, table_name name, column_name name,\n             column_type varchar,\n             column_index int)\n        order by \"column_index\"\n    ),\n\n    tables as (\n\n      select\n          table_schema,\n          table_name,\n          table_type\n\n      from information_schema.tables\n\n    ),\n\n    table_owners as (\n\n        select\n            schemaname as table_schema,\n            tablename as table_name,\n            tableowner as table_owner\n\n        from pg_tables\n\n        union all\n\n        select\n            schemaname as table_schema,\n            viewname as table_name,\n            viewowner as table_owner\n\n        from pg_views\n\n    ),\n\n    columns as (\n\n        select\n            table_schema,\n            table_name,\n            null::varchar as table_comment,\n\n            column_name,\n            ordinal_position as column_index,\n            data_type as column_type,\n            null::varchar as column_comment\n\n\n        from information_schema.columns\n\n    ),\n\n    unioned as (\n\n        select *\n        from tables\n        join columns using (table_schema, table_name)\n\n        union all\n\n        select *\n        from late_binding\n\n    )\n\n    select *,\n        table_schema || '.' || table_name as table_id\n\n    from unioned\n    join table_owners using (table_schema, table_name)\n\n    where table_schema != 'information_schema'\n      and table_schema not like 'pg_%'\n\n    order by \"column_index\"\n  {%- endcall -%}\n\n  {{ return(load_result('base_catalog').table) }}\n{%- endmacro %}\n\n{% macro redshift__get_extended_catalog() %}\n  {%- call statement('extended_catalog', fetch_result=True) -%}\n\n    select\n        \"schema\" || '.' || \"table\" as table_id,\n\n        'Encoded'::text as \"stats:encoded:label\",\n        encoded as \"stats:encoded:value\",\n        'Indicates whether any column in the table has compression encoding defined.'::text as \"stats:encoded:description\",\n        true as \"stats:encoded:include\",\n\n        'Dist Style' as \"stats:diststyle:label\",\n        diststyle as \"stats:diststyle:value\",\n        'Distribution style or distribution key column, if key distribution is defined.'::text as \"stats:diststyle:description\",\n        true as \"stats:diststyle:include\",\n\n        'Sort Key 1' as \"stats:sortkey1:label\",\n        -- handle 0xFF byte in response for interleaved sort styles\n        case\n            when sortkey1 like 'INTERLEAVED%' then 'INTERLEAVED'::text\n            else sortkey1\n        end as \"stats:sortkey1:value\",\n        'First column in the sort key.'::text as \"stats:sortkey1:description\",\n        (sortkey1 is not null) as \"stats:sortkey1:include\",\n\n        'Max Varchar' as \"stats:max_varchar:label\",\n        max_varchar as \"stats:max_varchar:value\",\n        'Size of the largest column that uses a VARCHAR data type.'::text as \"stats:max_varchar:description\",\n        true as \"stats:max_varchar:include\",\n\n        -- exclude this, as the data is strangely returned with null-byte characters\n        'Sort Key 1 Encoding' as \"stats:sortkey1_enc:label\",\n        sortkey1_enc as \"stats:sortkey1_enc:value\",\n        'Compression encoding of the first column in the sort key.' as \"stats:sortkey1_enc:description\",\n        false as \"stats:sortkey1_enc:include\",\n\n        '# Sort Keys' as \"stats:sortkey_num:label\",\n        sortkey_num as \"stats:sortkey_num:value\",\n        'Number of columns defined as sort keys.' as \"stats:sortkey_num:description\",\n        (sortkey_num > 0) as \"stats:sortkey_num:include\",\n\n        'Approximate Size' as \"stats:size:label\",\n        size / 1000000.0 as \"stats:size:value\",\n        'Approximate size of the table, calculated from a count of 1MB blocks'::text as \"stats:size:description\",\n        true as \"stats:size:include\",\n\n        'Disk Utilization' as \"stats:pct_used:label\",\n        pct_used / 100.0 as \"stats:pct_used:value\",\n        'Percent of available space that is used by the table.'::text as \"stats:pct_used:description\",\n        true as \"stats:pct_used:include\",\n\n        'Unsorted %' as \"stats:unsorted:label\",\n        unsorted / 100.0 as \"stats:unsorted:value\",\n        'Percent of unsorted rows in the table.'::text as \"stats:unsorted:description\",\n        (unsorted is not null) as \"stats:unsorted:include\",\n\n        'Stats Off' as \"stats:stats_off:label\",\n        stats_off as \"stats:stats_off:value\",\n        'Number that indicates how stale the table statistics are; 0 is current, 100 is out of date.'::text as \"stats:stats_off:description\",\n        true as \"stats:stats_off:include\",\n\n        'Approximate Row Count' as \"stats:rows:label\",\n        tbl_rows as \"stats:rows:value\",\n        'Approximate number of rows in the table. This value includes rows marked for deletion, but not yet vacuumed.'::text as \"stats:rows:description\",\n        true as \"stats:rows:include\",\n\n        'Sort Key Skew' as \"stats:skew_sortkey1:label\",\n        skew_sortkey1 as \"stats:skew_sortkey1:value\",\n        'Ratio of the size of the largest non-sort key column to the size of the first column of the sort key.'::text as \"stats:skew_sortkey1:description\",\n        (skew_sortkey1 is not null) as \"stats:skew_sortkey1:include\",\n\n        'Skew Rows' as \"stats:skew_rows:label\",\n        skew_rows as \"stats:skew_rows:value\",\n        'Ratio of the number of rows in the slice with the most rows to the number of rows in the slice with the fewest rows.'::text as \"stats:skew_rows:description\",\n        (skew_rows is not null) as \"stats:skew_rows:include\"\n\n    from svv_table_info\n\n  {%- endcall -%}\n\n  {{ return(load_result('extended_catalog').table) }}\n\n{% endmacro %}\n\n{% macro redshift__can_select_from(table_name) %}\n\n  {%- call statement('has_table_privilege', fetch_result=True) -%}\n\n    select has_table_privilege(current_user, '{{ table_name }}', 'SELECT') as can_select\n\n  {%- endcall -%}\n\n  {% set can_select = load_result('has_table_privilege').table[0]['can_select'] %}\n  {{ return(can_select) }}\n\n{% endmacro %}\n\n{% macro redshift__no_svv_table_info_warning() %}\n\n    {% set msg %}\n\n    Warning: The database user \"{{ target.user }}\" has insufficient permissions to\n    query the \"svv_table_info\" table. Please grant SELECT permissions on this table\n    to the \"{{ target.user }}\" user to fetch extended table details from Redshift.\n\n    {% endset %}\n\n    {{ log(msg, info=True) }}\n\n{% endmacro %}\n\n\n{% macro redshift__get_catalog() %}\n\n    {#-- Compute a left-outer join in memory. Some Redshift queries are\n      -- leader-only, and cannot be joined to other compute-based queries #}\n\n    {% set catalog = redshift__get_base_catalog() %}\n\n    {% set select_extended =  redshift__can_select_from('svv_table_info') %}\n    {% if select_extended %}\n        {% set extended_catalog = redshift__get_extended_catalog() %}\n        {% set catalog = catalog.join(extended_catalog, 'table_id') %}\n    {% else %}\n        {{ redshift__no_svv_table_info_warning() }}\n    {% endif %}\n\n    {{ return(catalog.exclude(['table_id'])) }}\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.bigquery__create_csv_table": {"original_file_path": "macros\\materializations\\seed\\bigquery.sql", "tags": [], "name": "bigquery__create_csv_table", "package_name": "dbt", "unique_id": "macro.dbt.bigquery__create_csv_table", "path": "macros\\materializations\\seed\\bigquery.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro bigquery__create_csv_table(model) %}\n    -- no-op\n{% endmacro %}\n\n{% macro bigquery__reset_csv_table(model, full_refresh, old_relation) %}\n    {{ adapter.drop_relation(old_relation) }}\n{% endmacro %}\n\n{% macro bigquery__load_csv_rows(model) %}\n\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n  {{ adapter.load_dataframe(model['schema'], model['alias'], model['agate_table'], column_override) }}\n\n{% endmacro %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.create_csv_table": {"original_file_path": "macros\\materializations\\seed\\seed.sql", "tags": [], "name": "create_csv_table", "package_name": "dbt", "unique_id": "macro.dbt.create_csv_table", "path": "macros\\materializations\\seed\\seed.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro create_csv_table(model) -%}\n  {{ adapter_macro('create_csv_table', model) }}\n{%- endmacro %}\n\n{% macro reset_csv_table(model, full_refresh, old_relation) -%}\n  {{ adapter_macro('reset_csv_table', model, full_refresh, old_relation) }}\n{%- endmacro %}\n\n{% macro load_csv_rows(model) -%}\n  {{ adapter_macro('load_csv_rows', model) }}\n{%- endmacro %}\n\n{% macro default__create_csv_table(model) %}\n  {%- set agate_table = model['agate_table'] -%}\n  {%- set column_override = model['config'].get('column_types', {}) -%}\n\n  {% set sql %}\n    create table {{ this.render(False) }} (\n        {%- for col_name in agate_table.column_names -%}\n            {%- set inferred_type = adapter.convert_type(agate_table, loop.index0) -%}\n            {%- set type = column_override.get(col_name, inferred_type) -%}\n            {{ col_name | string }} {{ type }} {%- if not loop.last -%}, {%- endif -%}\n        {%- endfor -%}\n    )\n  {% endset %}\n\n  {% call statement('_') -%}\n    {{ sql }}\n  {%- endcall %}\n\n  {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__reset_csv_table(model, full_refresh, old_relation) %}\n    {% set sql = \"\" %}\n    {% if full_refresh %}\n        {{ adapter.drop_relation(old_relation) }}\n        {% set sql = create_csv_table(model) %}\n    {% else %}\n        {{ adapter.truncate_relation(old_relation) }}\n        {% set sql = \"truncate table \" ~ old_relation %}\n    {% endif %}\n\n    {{ return(sql) }}\n{% endmacro %}\n\n\n{% macro default__load_csv_rows(model) %}\n    {% set agate_table = model['agate_table'] %}\n    {% set cols_sql = \", \".join(agate_table.column_names) %}\n    {% set bindings = [] %}\n\n    {% set statements = [] %}\n\n    {% for chunk in agate_table.rows | batch(10000) %}\n        {% set bindings = [] %}\n\n        {% for row in chunk %}\n            {% set _ = bindings.extend(row) %}\n        {% endfor %}\n\n        {% set sql %}\n            insert into {{ this.render(False) }} ({{ cols_sql }}) values\n            {% for row in chunk -%}\n                ({%- for column in agate_table.column_names -%}\n                    %s\n                    {%- if not loop.last%},{%- endif %}\n                {%- endfor -%})\n                {%- if not loop.last%},{%- endif %}\n            {%- endfor %}\n        {% endset %}\n\n        {% set _ = adapter.add_query(sql, bindings=bindings, abridge_sql_log=True) %}\n\n        {% if loop.index0 == 0 %}\n            {% set _ = statements.append(sql) %}\n        {% endif %}\n    {% endfor %}\n\n    {# Return SQL so we can render it out into the compiled files #}\n    {{ return(statements[0]) }}\n{% endmacro %}\n\n\n{% materialization seed, default %}\n\n  {%- set identifier = model['alias'] -%}\n  {%- set full_refresh_mode = (flags.FULL_REFRESH == True) -%}\n\n  {%- set old_relation = adapter.get_relation(schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_table = (old_relation is not none and old_relation.is_table) -%}\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set csv_table = model[\"agate_table\"] -%}\n\n  {{ run_hooks(pre_hooks, inside_transaction=False) }}\n\n  -- `BEGIN` happens here:\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- build model\n  {% set create_table_sql = \"\" %}\n  {% if exists_as_view %}\n    {{ exceptions.raise_compiler_error(\"Cannot seed to '{}', it is a view\".format(old_relation)) }}\n  {% elif exists_as_table %}\n    {% set create_table_sql = reset_csv_table(model, full_refresh_mode, old_relation) %}\n  {% else %}\n    {% set create_table_sql = create_csv_table(model) %}\n  {% endif %}\n\n  {% set status = 'CREATE' if full_refresh_mode else 'INSERT' %}\n  {% set num_rows = (csv_table.rows | length) %}\n  {% set sql = load_csv_rows(model) %}\n\n  {% call noop_statement('main', status ~ ' ' ~ num_rows) %}\n    {{ create_table_sql }};\n    -- dbt seed --\n    {{ sql }}\n  {% endcall %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  -- `COMMIT` happens here\n  {{ adapter.commit() }}\n\n  {{ run_hooks(post_hooks, inside_transaction=False) }}\n{% endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}, "macro.dbt.materialization_view_snowflake": {"original_file_path": "macros\\materializations\\view\\bq_snowflake_view.sql", "tags": [], "name": "materialization_view_snowflake", "package_name": "dbt", "unique_id": "macro.dbt.materialization_view_snowflake", "path": "macros\\materializations\\view\\bq_snowflake_view.sql", "depends_on": {"macros": []}, "resource_type": "macro", "raw_sql": "{% macro handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {{ adapter_macro(\"dbt.handle_existing_table\", full_refresh, non_destructive_mode, old_relation) }}\n{% endmacro %}\n\n{% macro default__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- endif -%}\n{% endmacro %}\n\n{% macro bigquery__handle_existing_table(full_refresh, non_destructive_mode, old_relation) %}\n    {%- if full_refresh and not non_destructive_mode -%}\n      {{ adapter.drop_relation(old_relation) }}\n    {%- else -%}\n      {{ exceptions.relation_wrong_type(old_relation, 'view') }}\n    {%- endif -%}\n{% endmacro %}\n\n\n{# /*\n       Core materialization implementation. BigQuery and Snowflake are similar\n       because both can use `create or replace view` where the resulting view schema\n       is not necessarily the same as the existing view. On Redshift, this would\n       result in: ERROR:  cannot change number of columns in view\n\n       This implementation is superior to the create_temp, swap_with_existing, drop_old\n       paradigm because transactions don't run DDL queries atomically on Snowflake. By using\n       `create or replace view`, the materialization becomes atomic in nature.\n    */\n#}\n\n{% macro impl_view_materialization(run_outside_transaction_hooks=True) %}\n  {%- set identifier = model['alias'] -%}\n  {%- set non_destructive_mode = (flags.NON_DESTRUCTIVE == True) -%}\n\n  {%- set old_relation = adapter.get_relation(\n      schema=schema, identifier=identifier) -%}\n\n  {%- set exists_as_view = (old_relation is not none and old_relation.is_view) -%}\n\n  {%- set target_relation = api.Relation.create(\n      identifier=identifier, schema=schema,\n      type='view') -%}\n\n  {%- set should_ignore = non_destructive_mode and exists_as_view %}\n  {%- set has_transactional_hooks = (hooks | selectattr('transaction', 'equalto', True) | list | length) > 0 %}\n\n  {% if run_outside_transaction_hooks %}\n      -- no transactions on BigQuery\n      {{ run_hooks(pre_hooks, inside_transaction=False) }}\n  {% endif %}\n\n  -- `BEGIN` happens here on Snowflake\n  {{ run_hooks(pre_hooks, inside_transaction=True) }}\n\n  -- If there's a table with the same name and we weren't told to full refresh,\n  -- that's an error. If we were told to full refresh, drop it. This behavior differs\n  -- for Snowflake and BigQuery, so multiple dispatch is used.\n  {%- if old_relation is not none and old_relation.is_table -%}\n    {{ handle_existing_table(flags.FULL_REFRESH, non_destructive_mode, old_relation) }}\n  {%- endif -%}\n\n  -- build model\n  {% if non_destructive_mode -%}\n    {% call noop_statement('main', status=\"PASS\", res=None) -%}\n      -- Not running : non-destructive mode\n      {{ sql }}\n    {%- endcall %}\n  {%- else -%}\n    {% call statement('main') -%}\n      {{ create_view_as(target_relation, sql) }}\n    {%- endcall %}\n  {%- endif %}\n\n  {{ run_hooks(post_hooks, inside_transaction=True) }}\n\n  {#\n      -- Don't commit in non-destructive mode _unless_ there are in-transaction hooks\n      -- TODO : Figure out some other way of doing this that isn't as fragile\n  #}\n  {% if has_transactional_hooks or not should_ignore %}\n      {{ adapter.commit() }}\n  {% endif %}\n\n  {% if run_outside_transaction_hooks %}\n      -- No transactions on BigQuery\n      {{ run_hooks(post_hooks, inside_transaction=False) }}\n  {% endif %}\n{% endmacro %}\n\n{% materialization view, adapter='bigquery' -%}\n    {{ impl_view_materialization(run_outside_transaction_hooks=False) }}\n{%- endmaterialization %}\n\n{% materialization view, adapter='snowflake' -%}\n    {{ impl_view_materialization() }}\n{%- endmaterialization %}", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}}, "parent_map": {"model.elevenia_c2c.paid_trx_hourly": [], "model.elevenia_c2c.sd_sales_dtls": [], "model.elevenia_c2c.xbi_1087_top_100": [], "model.elevenia_c2c.sessions_paid_test": ["model.elevenia_c2c.paid_trx_hourly"], "model.elevenia_c2c.ord_no_hr_paid": [], "model.elevenia_c2c.sessions_ord_no_hr": ["model.elevenia_c2c.ord_no_hr"], "model.elevenia_c2c.sessions_dlv": ["model.elevenia_c2c.ff_free_delivery_d"], "model.elevenia_c2c.trx_list": ["model.elevenia_c2c.dd_disp_ctgr", "model.elevenia_c2c.dd_member", "model.elevenia_c2c.dd_member", "model.elevenia_c2c.dd_product", "model.elevenia_c2c.dd_tester", "model.elevenia_c2c.sd_sales_dtls"], "model.elevenia_c2c.ord_no_hr": [], "model.elevenia_c2c.dd_disp_ctgr": [], "model.elevenia_c2c.dd_tester": [], "model.elevenia_c2c.sessions_paid": ["model.elevenia_c2c.paid_trx_hourly"], "model.elevenia_c2c.dd_address_v2": ["model.elevenia_c2c.dd_address"], "model.elevenia_c2c.dd_member": [], "model.elevenia_c2c.ord_trx_hourly": [], "model.elevenia_c2c.ff_free_delivery_d": [], "model.elevenia_c2c.sessions": ["model.elevenia_c2c.ord_trx_hourly"], "model.elevenia_c2c.trx_list_meta_sum": ["model.elevenia_c2c.trx_list"], "model.elevenia_c2c.dd_product": [], "model.elevenia_c2c.sessions_ord_test": ["model.elevenia_c2c.ord_trx_hourly"], "model.elevenia_c2c.dd_address": [], "model.elevenia_c2c.trx_list_md_sum": ["model.elevenia_c2c.trx_list"], "model.elevenia_c2c.sd_sales_dtls_mon": ["model.elevenia_c2c.sd_sales_dtls_mon_v"], "model.elevenia_c2c.rangga_test": [], "model.elevenia_c2c.trx_list_pulsa_sum": ["model.elevenia_c2c.trx_list"], "model.elevenia_c2c.sd_sales_dtls_mon_v": [], "model.elevenia_c2c.dd_address_v3": ["model.elevenia_c2c.dd_address"]}, "metadata": {"user_id": null, "send_anonymous_usage_stats": false, "project_id": "a669f3c26f0d6ef0363c52154662eed0"}, "docs": {"dbt.__overview__": {"original_file_path": "docs\\overview.md", "block_contents": "### Welcome!\n\nWelcome to the auto-generated documentation for your dbt project!\n\n### Navigation\n\nYou can use the `Project` and `Database` navigation tabs on the left side of the window to explore the models\nin your project.\n\n#### Project Tab\nThe `Project` tab mirrors the directory structure of your dbt project. In this tab, you can see all of the\nmodels defined in your dbt project, as well as models imported from dbt packages.\n\n#### Database Tab\nThe `Database` tab also exposes your models, but in a format that looks more like a database explorer. This view\nshows relations (tables and views) grouped into database schemas. Note that ephemeral models are _not_ shown\nin this interface, as they do not exist in the database.\n\n### Graph Exploration\nYou can click the blue icon on the bottom-right corner of the page to view the lineage graph of your models.\n\nOn model pages, you'll see the immediate parents and children of the model you're exploring. By clicking the `Expand`\nbutton at the top-right of this lineage pane, you'll be able to see all of the models that are used to build,\nor are built from, the model you're exploring.\n\nOnce expanded, you'll be able to use the `--models` and `--exclude` model selection syntax to filter the\nmodels in the graph. For more information on model selection, check out the [dbt docs](https://docs.getdbt.com/reference#section-specifying-models-to-run).\n\nNote that you can also right-click on models to interactively filter and explore the graph.\n\n---\n\n### More information\n\n- [What is dbt](https://docs.getdbt.com/docs/overview)?\n- Read the [dbt viewpoint](https://docs.getdbt.com/docs/viewpoint)\n- [Installation](https://docs.getdbt.com/docs/installation)\n- Join the [chat](https://slack.getdbt.com/) on Slack for live questions and support.", "name": "__overview__", "file_contents": "\n{% docs __overview__ %}\n\n### Welcome!\n\nWelcome to the auto-generated documentation for your dbt project!\n\n### Navigation\n\nYou can use the `Project` and `Database` navigation tabs on the left side of the window to explore the models\nin your project.\n\n#### Project Tab\nThe `Project` tab mirrors the directory structure of your dbt project. In this tab, you can see all of the\nmodels defined in your dbt project, as well as models imported from dbt packages.\n\n#### Database Tab\nThe `Database` tab also exposes your models, but in a format that looks more like a database explorer. This view\nshows relations (tables and views) grouped into database schemas. Note that ephemeral models are _not_ shown\nin this interface, as they do not exist in the database.\n\n### Graph Exploration\nYou can click the blue icon on the bottom-right corner of the page to view the lineage graph of your models.\n\nOn model pages, you'll see the immediate parents and children of the model you're exploring. By clicking the `Expand`\nbutton at the top-right of this lineage pane, you'll be able to see all of the models that are used to build,\nor are built from, the model you're exploring.\n\nOnce expanded, you'll be able to use the `--models` and `--exclude` model selection syntax to filter the\nmodels in the graph. For more information on model selection, check out the [dbt docs](https://docs.getdbt.com/reference#section-specifying-models-to-run).\n\nNote that you can also right-click on models to interactively filter and explore the graph.\n\n---\n\n### More information\n\n- [What is dbt](https://docs.getdbt.com/docs/overview)?\n- Read the [dbt viewpoint](https://docs.getdbt.com/docs/viewpoint)\n- [Installation](https://docs.getdbt.com/docs/installation)\n- Join the [chat](https://slack.getdbt.com/) on Slack for live questions and support.\n\n{% enddocs %}\n", "package_name": "dbt", "unique_id": "dbt.__overview__", "path": "overview.md", "resource_type": "documentation", "root_path": "c:\\users\\alvinlaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\dbt\\include\\global_project"}}, "disabled": [], "nodes": {"model.elevenia_c2c.paid_trx_hourly": {"original_file_path": "models\\incremental_table\\paid_trx_hourly.sql", "alias": "paid_trx_hourly", "unique_id": "model.elevenia_c2c.paid_trx_hourly", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "paid_trx_hourly"], "tags": [], "name": "paid_trx_hourly", "schema": "dbt_dev", "path": "incremental_table\\paid_trx_hourly.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "dist": "ord_stl_end_dt_hour_of_day", "sort": "ord_stl_end_dt_hour_of_day", "tags": [], "quoting": {}, "column_types": {}, "sql_where": "ord_stl_end_dt_hour_of_day > (select max(ord_stl_end_dt_hour_of_day) from )", "pre-hook": [], "unique_key": "ord_stl_end_dt_hour_of_day", "enabled": true}, "raw_sql": "SELECT DATE(sd_sales_dtls.ord_stl_end_dt ) AS ord_stl_end_dt_date\r\n, DATE_PART(hour, sd_sales_dtls.ord_stl_end_dt )::integer AS ord_stl_end_dt_hour_of_day\r\n, dateadd(hour,7,getdate()) created_dt\r\n, COUNT(DISTINCT case when (DATE(sd_sales_dtls.ord_stl_end_dt )) is not null then concat(sd_sales_dtls.ord_no,sd_sales_dtls.prd_no) end ) paid_item_cnt\r\nFROM dwuser.sd_sales_dtls AS sd_sales_dtls\r\nLEFT JOIN dwuser.dd_tester  AS dd_tester_byr ON sd_sales_dtls.buy_mem_no = dd_tester_byr.mem_no \r\nLEFT JOIN dwuser.dd_tester  AS dd_tester_slr ON sd_sales_dtls.seller_mem_no = dd_tester_slr.mem_no \r\nLEFT JOIN dwuser.dd_product  AS dd_product ON sd_sales_dtls.prd_no = dd_product.prd_no \r\nLEFT JOIN dwuser.dd_prd_typ  AS dd_prd_typ ON dd_product.prd_typ_cd = dd_prd_typ.prd_typ_cd \r\nWHERE ((((sd_sales_dtls.ord_stl_end_dt ) >= ((DATE_TRUNC('day',GETDATE()))) AND (sd_sales_dtls.ord_stl_end_dt ) < ((DATEADD(day,1, DATE_TRUNC('day',GETDATE()) )))))) AND (dd_prd_typ.en_prd_typ_nm <> 'B2B Order' OR dd_prd_typ.en_prd_typ_nm IS NULL) AND (dd_tester_byr.mem_no is null and dd_tester_slr.mem_no is null)\r\nGROUP BY 1,2,3", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.sd_sales_dtls": {"original_file_path": "models\\transactions\\sd_sales_dtls.sql", "alias": "sd_sales_dtls", "unique_id": "model.elevenia_c2c.sd_sales_dtls", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "transactions", "sd_sales_dtls"], "tags": [], "name": "sd_sales_dtls", "schema": "dbt_dev", "path": "transactions\\sd_sales_dtls.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select * from dwuser.sd_sales_dtls where ord_dy = to_char(dateadd(day,-1,getdate()),'YYYYMMDD')", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.xbi_1087_top_100": {"original_file_path": "models\\xbi_1087_top_100.sql", "alias": "xbi_1087_top_100", "unique_id": "model.elevenia_c2c.xbi_1087_top_100", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "xbi_1087_top_100"], "tags": [], "name": "xbi_1087_top_100", "schema": "dbt_dev", "path": "xbi_1087_top_100.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "with week_ref as (\r\nselect t1.basic_week\r\n, t1.week_bgn_dd\r\n, t1.week_end_dd\r\n, t2.basic_dd\r\n, dense_rank() over (order by t1.basic_week desc) week_rn\r\nfrom dwuser.dd_basic_dd t2\r\ninner join dwuser.dd_basic_week t1 on t2.basic_week = t1.basic_week\r\nwhere t2.basic_dd <= to_char(getdate(),'YYYYMMDD')\r\nand left(t1.basic_week,4) between left(to_char(dateadd(year,-1,getdate()),'YYYYMMDD'),4) and left(to_char(getdate(),'YYYYMMDD'),4)\r\n)\r\n,\r\nsrc as (\r\nselect a.*\r\n, b.basic_week\r\n, b.basic_dd basic_dd_full\r\n, b.week_bgn_dd\r\n, b.week_end_dd\r\n, b.week_rn\r\nfrom dwuser.sd_view_prd a --21 columns\r\nleft join week_ref b on a.basic_dd = b.basic_dd\r\n--where ((((to_date(a.basic_dd,'YYYYMMDD') ) >= ((DATEADD(day,-30, DATE_TRUNC('day',GETDATE()) ))) AND (to_date(a.basic_dd,'YYYYMMDD') ) < ((DATEADD(day,30, DATEADD(day,-30, DATE_TRUNC('day',GETDATE()) ) ))))))\r\n--where prd_no in (23217159, 25169895, 25804453, 11339, 24892734)\r\n)\r\n,\r\nsrc2 as (\r\nselect *\r\n, row_number() over(partition by prd_no order by week_rn, basic_dd_full) prd_rn\r\n, count(distinct view_prd_seq) prd_view_cnt\r\n, count(distinct mem_no) prd_viewer_cnt\r\nfrom src\r\nwhere week_rn >= 2\r\ngroup by 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26\r\n)\r\n,\r\nsd_sales_dtls as (\r\nselect distinct prd_no--, ord_dy\r\nfrom dwuser.sd_sales_dtls \r\nwhere (left(ord_dy,4) between left(to_char(dateadd(year,-1,getdate()),'YYYYMMDD'),4) and left(to_char(getdate(),'YYYYMMDD'),4))\r\n--and ((((to_date(ord_dy,'YYYYMMDD') ) >= ((DATEADD(day,-30, DATE_TRUNC('day',GETDATE()) ))) AND (to_date(ord_dy,'YYYYMMDD') ) < ((DATEADD(day,30, DATEADD(day,-30, DATE_TRUNC('day',GETDATE()) ) ))))))\r\n)\r\n,\r\nlast_30_1 as (\r\nselect a.prd_no\r\n, sum(a.prd_view_cnt) prd_view_30\r\n, sum(a.prd_viewer_cnt) prd_viewer_30\r\nfrom src2 a\r\nleft join sd_sales_dtls b on a.prd_no = b.prd_no\r\n--and a.basic_dd_full = b.ord_dy\r\nwhere b.prd_no is null\r\nand ((((to_date(basic_dd_full,'YYYYMMDD') ) >= ((DATEADD(day,-30, DATE_TRUNC('day',GETDATE()) ))) AND (to_date(basic_dd_full,'YYYYMMDD') ) < ((DATEADD(day,30, DATEADD(day,-30, DATE_TRUNC('day',GETDATE()) ) ))))))\r\ngroup by 1\r\n)\r\n,\r\nprd as (\r\nselect a.basic_week\r\n, a.week_bgn_dd\r\n, a.week_end_dd\r\n, a.week_rn\r\n, a.prd_no\r\n, c.prd_nm\r\n, f.meta_ctgr_nm\r\n, d.mem_no sel_mem_no\r\n, d.mem_id\r\n, d.prtbl_tlphn_no\r\n, d.store_nm\r\n, g.am_name\r\n, g.am_backup\r\n, max(e.prd_view_30) prd_view_30\r\n, max(e.prd_viewer_30) prd_viewer_30\r\n, sum(case when prd_rn = 1 then a.sel_prc end) sel_prc\r\n, sum(case when prd_rn = 1 then a.final_dsc_prc end) final_dsc_prc\r\n, sum(prd_view_cnt) prd_view\r\n, sum(prd_viewer_cnt) prd_viewer\r\nfrom src2 a\r\nleft join sd_sales_dtls b on a.prd_no = b.prd_no\r\n--and a.basic_dd_full = b.ord_dy\r\nleft join dwuser.dd_product c on a.prd_no = c.prd_no\r\nleft join dwuser.dd_disp_ctgr f on c.disp_ctgr_no = f.disp_ctgr_no\r\nleft join dwuser.dd_member d on c.seller_no = d.mem_no\r\nleft join dwuser.dd_am_sellers_distribution g on c.seller_no = g.seller_no\r\nleft join dwuser.dd_tester test on c.seller_no = test.mem_no\r\ninner join last_30_1 e on a.prd_no = e.prd_no \r\nwhere b.prd_no is null\r\nand c.sel_stat_cd in ('103')\r\nand c.seller_no <> -1\r\nand d.mem_typ_cd in ('02','03')\r\nand test.mem_no is null\r\ngroup by 1,2,3,4,5,6,7,8,9,10,11,12,13\r\norder by 1 desc\r\n)\r\nselect * from prd where week_rn = 2", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.sessions_paid_test": {"original_file_path": "models\\incremental_table\\sessions_paid_test.sql", "alias": "sessions_paid_test", "unique_id": "model.elevenia_c2c.sessions_paid_test", "resource_type": "model", "columns": {}, "refs": [["paid_trx_hourly"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.paid_trx_hourly"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "sessions_paid_test"], "tags": [], "name": "sessions_paid_test", "schema": "dbt_dev", "path": "incremental_table\\sessions_paid_test.sql", "empty": false, "config": {"materialized": "incremental", "post-hook": [], "vars": {}, "dist": "ord_stl_end_dt_hour_of_day", "sort": "ord_stl_end_dt_hour_of_day", "tags": [], "quoting": {}, "column_types": {}, "sql_where": "ord_stl_end_dt_hour_of_day > (select max(ord_stl_end_dt_hour_of_day) from )", "pre-hook": [], "unique_key": "ord_stl_end_dt_hour_of_day", "enabled": true}, "raw_sql": "select * from {{ ref('paid_trx_hourly') }}\r\n{% if adapter.already_exists(this.schema, this.table) and not flags.FULL_REFRESH %}\r\nwhere ord_stl_end_dt_hour_of_day >= (select max(ord_stl_end_dt_hour_of_day) from {{ this }})\r\n{% endif %}", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.ord_trx_hourly": {"original_file_path": "models\\incremental_table\\ord_trx_hourly.sql", "alias": "ord_trx_hourly", "unique_id": "model.elevenia_c2c.ord_trx_hourly", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "ord_trx_hourly"], "tags": [], "name": "ord_trx_hourly", "schema": "dbt_dev", "path": "incremental_table\\ord_trx_hourly.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "dist": "ord_dt_hour_of_day", "sort": "ord_dt_hour_of_day", "tags": [], "quoting": {}, "column_types": {}, "sql_where": "ord_dt_hour_of_day > (select max(ord_dt_hour_of_day) from )", "pre-hook": [], "unique_key": "ord_dt_hour_of_day", "enabled": true}, "raw_sql": "SELECT DATE(sd_sales_dtls.ord_dt ) AS ord_dt_date\r\n, DATE_PART(hour, sd_sales_dtls.ord_dt )::integer AS ord_dt_hour_of_day\r\n, dateadd(hour,7,getdate()) created_dt\r\n, COUNT(DISTINCT sd_sales_dtls.ord_no||sd_sales_dtls.prd_no) ord_item_cnt\r\nFROM dwuser.sd_sales_dtls AS sd_sales_dtls\r\nLEFT JOIN dwuser.dd_tester  AS dd_tester_byr ON sd_sales_dtls.buy_mem_no = dd_tester_byr.mem_no \r\nLEFT JOIN dwuser.dd_tester  AS dd_tester_slr ON sd_sales_dtls.seller_mem_no = dd_tester_slr.mem_no \r\nLEFT JOIN dwuser.dd_product  AS dd_product ON sd_sales_dtls.prd_no = dd_product.prd_no \r\nLEFT JOIN dwuser.dd_prd_typ  AS dd_prd_typ ON dd_product.prd_typ_cd = dd_prd_typ.prd_typ_cd \r\nWHERE ((((sd_sales_dtls.ord_dt ) >= ((DATE_TRUNC('day',GETDATE()))) AND (sd_sales_dtls.ord_dt ) < ((DATEADD(day,1, DATE_TRUNC('day',GETDATE()) )))))) AND (dd_prd_typ.en_prd_typ_nm <> 'B2B Order' OR dd_prd_typ.en_prd_typ_nm IS NULL) AND (dd_tester_byr.mem_no is null and dd_tester_slr.mem_no is null)\r\nGROUP BY 1,2,3", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.sessions_ord_no_hr": {"original_file_path": "models\\incremental_table\\sessions_ord_no_hr.sql", "alias": "sessions_ord_no_hr", "unique_id": "model.elevenia_c2c.sessions_ord_no_hr", "resource_type": "model", "columns": {}, "refs": [["ord_no_hr"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.ord_no_hr"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "sessions_ord_no_hr"], "tags": [], "name": "sessions_ord_no_hr", "schema": "dbt_dev", "path": "incremental_table\\sessions_ord_no_hr.sql", "empty": false, "config": {"materialized": "incremental", "post-hook": [], "vars": {}, "dist": "even", "sort": "ord_dy_conc_hr", "tags": [], "quoting": {}, "column_types": {}, "sql_where": "ord_dy_conc_hr >= (select max(ord_dy_conc_hr) from dbt_dev.sessions_ord_no_hr)", "pre-hook": [], "unique_key": "ord_dy_conc_hr", "enabled": true}, "raw_sql": "with all_ord_no_hr as (\r\n\r\n    select * from {{ ref('ord_no_hr') }}\r\n\r\n),\r\n\r\n-- Filter out only the events that have arrived since events have last been processed\r\nnew_ord_no_hr as (\r\n\r\n    select *\r\n    from all_ord_no_hr\r\n\r\n    -- This conditional is executed just before the model is\r\n    -- executed and returns either True or False\r\n    -- The enclosed `where` filter will be conditionally applied\r\n    -- only if this model exists in the current schema\r\n    {% if adapter.already_exists(this.schema, this.table) and not flags.FULL_REFRESH %}\r\n        where ord_dy_conc_hr >= (select max(ord_dy_conc_hr) from {{ this }})\r\n        --OR ISNULL(ord_stl_end_dy_conc_hr,'-') >= (select max(ISNULL(ord_stl_end_dy_conc_hr,'-')) from {{ this }})\r\n        --OR ISNULL(pocnfrm_dy_conc_hr,'-') >= (select max(ISNULL(pocnfrm_dy_conc_hr,'-')) from {{ this }})\r\n        --OR en_ord_prd_stat_nm >= (select max(en_ord_prd_stat_nm) from {{ this }})\r\n        --ord_dt_date >= (select max(ord_dt_date) from {{ this }})\r\n        --OR ord_dt_hr >= (select max(ord_dt_hr) from {{ this }})\r\n    {% endif %}\r\n\r\n)\r\nselect * from new_ord_no_hr", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.sessions_dlv": {"original_file_path": "models\\incremental_table\\sessions_dlv.sql", "alias": "sessions_dlv", "unique_id": "model.elevenia_c2c.sessions_dlv", "resource_type": "model", "columns": {}, "refs": [["ff_free_delivery_d"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.ff_free_delivery_d"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "sessions_dlv"], "tags": [], "name": "sessions_dlv", "schema": "dbt_dev", "path": "incremental_table\\sessions_dlv.sql", "empty": false, "config": {"materialized": "incremental", "post-hook": [], "vars": {}, "dist": "basic_dd", "sort": "basic_dd", "tags": [], "quoting": {}, "column_types": {}, "sql_where": "basic_dd >= ( select max(basic_dd) from dbt_dev.sessions_dlv )", "pre-hook": [], "unique_key": "basic_dd", "enabled": true}, "raw_sql": "with all_dlv as (\r\n\r\n    select * from {{ ref('ff_free_delivery_d') }}\r\n\r\n),\r\n\r\n-- Filter out only the events that have arrived since events have last been processed\r\nnew_dlv as (\r\n\r\n   select *\r\n   from all_dlv\r\n\r\n    -- This conditional is executed just before the model is\r\n    -- executed and returns either True or False\r\n    -- The enclosed `where` filter will be conditionally applied\r\n    -- only if this model exists in the current schema\r\n   {% if adapter.already_exists(this.schema, this.table) and not flags.FULL_REFRESH %}\r\n        where basic_dd >= ( select max(basic_dd) from {{ this }} )\r\n   {% endif %}\r\n\r\n)\r\nselect * from new_dlv", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.trx_list": {"original_file_path": "models\\transactions\\trx_list.sql", "alias": "trx_list", "unique_id": "model.elevenia_c2c.trx_list", "resource_type": "model", "columns": {}, "refs": [["dd_member"], ["dd_member"], ["dd_tester"], ["dd_product"], ["dd_disp_ctgr"], ["sd_sales_dtls"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.dd_member", "model.elevenia_c2c.dd_member", "model.elevenia_c2c.dd_tester", "model.elevenia_c2c.dd_product", "model.elevenia_c2c.dd_disp_ctgr", "model.elevenia_c2c.sd_sales_dtls"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "transactions", "trx_list"], "tags": [], "name": "trx_list", "schema": "dbt_dev", "path": "transactions\\trx_list.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "with dd_member as (\r\n  select * from {{ ref('dd_member') }}\r\n),\r\ndd_member_slr as (\r\n  select * from {{ ref('dd_member') }}\r\n),\r\ndd_tester as (\r\n  select * from {{ ref('dd_tester') }}\r\n),\r\ndd_product as (\r\n  select * from {{ ref('dd_product') }}\r\n)\r\n,\r\ndd_disp_ctgr as (\r\n  select * from {{ ref('dd_disp_ctgr') }}\r\n)\r\n,\r\nsd_sales_dtls as (\r\n  select * from {{ ref('sd_sales_dtls') }}\r\n)\r\n,\r\ndaily_paid_gmv_list as (\r\n  select t1.ord_stl_end_dt\r\n  , t1.ord_no\r\n  , t2.prd_no\r\n  , t2.disp_ctgr_no\r\n  , t3.dept_cd\r\n  , t3.meta_ctgr_nm\r\n  , t3.disp_ctgr_big_cls_nm\r\n  , t3.disp_ctgr_mid_cls_nm\r\n  , t3.disp_ctgr_small_cls_nm\r\n  , t2.prd_nm\r\n  , t1.buy_mem_no\r\n  , dd_member_buyer.mem_id byr_id\r\n  , t1.seller_mem_no\r\n  , dd_member_slr.store_nm\r\n  , COALESCE(SUM(((t1.paid_qty * t1.sel_prc) + t1.paid_opt_won_stl + t1.paid_tiket_trans_fee) ), 0) AS paid_gmv\r\n  , COALESCE(SUM((((t1.paid_qty * t1.sel_prc) + t1.paid_opt_won_stl + t1.paid_tiket_trans_fee)\r\n    - (t1.paid_seller_disc_amt_add + t1.paid_seller_disc_amt_dirct + t1.paid_seller_disc_amt_dup)) ), 0) AS paid_gmv_seller\r\n  , COALESCE(SUM((((t1.paid_qty * t1.sel_prc) + t1.paid_opt_won_stl + t1.paid_tiket_trans_fee)\r\n    - (t1.paid_seller_disc_amt_add + t1.paid_seller_disc_amt_dirct + t1.paid_seller_disc_amt_dup)\r\n    + (t1.paid_dlv_cst + t1.paid_11st_disc_amt_dlv_crm + t1.paid_11st_disc_amt_dlv_cs + t1.paid_11st_disc_amt_dlv_md + t1.paid_11st_disc_amt_dlv_mkt + t1.paid_11st_disc_amt_dlv_sm)) ), 0) AS paid_gmv_seller_dlv\r\n  from sd_sales_dtls t1\r\n  left join dd_tester test_buy on t1.buy_mem_no = test_buy.mem_no\r\n  left join dd_tester test_sel on t1.seller_mem_no = test_sel.mem_no\r\n  left join dd_product t2 on t1.prd_no = t2.prd_no\r\n  left join dd_disp_ctgr t3 on t2.disp_ctgr_no = t3.disp_ctgr_no\r\n  left join dd_member dd_member_buyer on t1.buy_mem_no = dd_member_buyer.mem_no\r\n  left join dd_member dd_member_slr on t1.seller_mem_no = dd_member_slr.mem_no\r\n  where (test_buy.mem_no is null and test_sel.mem_no is null)\r\n  and ord_stl_end_dt is not null\r\n  group by 1,2,3,4,5,6,7,8,9,10,11,12,13,14\r\n)\r\nselect * from daily_paid_gmv_list", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.ord_no_hr": {"original_file_path": "models\\incremental_table\\ord_no_hr.sql", "alias": "ord_no_hr", "unique_id": "model.elevenia_c2c.ord_no_hr", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "ord_no_hr"], "tags": [], "name": "ord_no_hr", "schema": "dbt_dev", "path": "incremental_table\\ord_no_hr.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "SELECT to_char(ord_dt,'YYYYMMDD-HH24') ord_dy_conc_hr\r\n--, to_char(ord_stl_end_dt,'YYYYMMDD-HH24') ord_stl_end_dy_conc_hr\r\n--, to_char(pocnfrm_dt,'YYYYMMDD-HH24') pocnfrm_dy_conc_hr\r\n, dateadd(hour,7,getdate()) created_dt\r\n--, t2.en_ord_prd_stat_nm\r\n, max(ord_no) ord_no_max\r\n, max(case when ord_stl_end_dt is not null then ord_no end) ord_no_paid_max\r\nfrom dwuser.sd_sales_dtls t1\r\n--inner join dwuser.dd_ord_prd_stat t2 on t1.ord_prd_stat = t2.ord_prd_stat_cd\r\n--where to_char(ord_dt,'YYYYMMDD') >= to_char(dateadd(day,-3,getdate()),'YYYYMMDD')\r\nwhere ord_dt::DATE >= trunc(getdate()) \r\ngroup by 1\r\norder by 1 desc--,2 desc, 5 asc", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.rangga_test": {"original_file_path": "models\\p2\\rangga_test.sql", "alias": "rangga_test", "unique_id": "model.elevenia_c2c.rangga_test", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "p2", "rangga_test"], "tags": [], "name": "rangga_test", "schema": "dbt_dev", "path": "p2\\rangga_test.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select * from dwuser.dd_prd_stat\r\nUNION ALL\r\nselect '1', '1', NULL, NULL, NULL", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.dd_disp_ctgr": {"original_file_path": "models\\transactions\\dd_disp_ctgr.sql", "alias": "dd_disp_ctgr", "unique_id": "model.elevenia_c2c.dd_disp_ctgr", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "transactions", "dd_disp_ctgr"], "tags": [], "name": "dd_disp_ctgr", "schema": "dbt_dev", "path": "transactions\\dd_disp_ctgr.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select * from dwuser.dd_disp_ctgr", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.dd_tester": {"original_file_path": "models\\transactions\\dd_tester.sql", "alias": "dd_tester", "unique_id": "model.elevenia_c2c.dd_tester", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "transactions", "dd_tester"], "tags": [], "name": "dd_tester", "schema": "dbt_dev", "path": "transactions\\dd_tester.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select * from dwuser.dd_tester", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.dd_address": {"original_file_path": "models\\p1\\dd_address.sql", "alias": "dd_address", "unique_id": "model.elevenia_c2c.dd_address", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "p1", "dd_address"], "tags": [], "name": "dd_address", "schema": "dbt_dev", "path": "p1\\dd_address.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select *, dateadd(hour,7,getdate()) load_dt_v from dwuser.dd_address", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.dd_address_v2": {"original_file_path": "models\\p1\\dd_address_v2.sql", "alias": "dd_address_v2", "unique_id": "model.elevenia_c2c.dd_address_v2", "resource_type": "model", "columns": {}, "refs": [["dd_address"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.dd_address"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "p1", "dd_address_v2"], "tags": [], "name": "dd_address_v2", "schema": "dbt_dev", "path": "p1\\dd_address_v2.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "with dd_address as (\r\nselect * from {{ref('dd_address')}}\r\n)\r\nselect province\r\n, count(distinct city) no_of_city\r\nfrom dd_address\r\ngroup by 1", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.dd_member": {"original_file_path": "models\\transactions\\dd_member.sql", "alias": "dd_member", "unique_id": "model.elevenia_c2c.dd_member", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "transactions", "dd_member"], "tags": [], "name": "dd_member", "schema": "dbt_dev", "path": "transactions\\dd_member.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select * from dwuser.dd_member", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.ord_no_hr_paid": {"original_file_path": "models\\incremental_table\\ord_no_hr_paid.sql", "alias": "ord_no_hr_paid", "unique_id": "model.elevenia_c2c.ord_no_hr_paid", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "ord_no_hr_paid"], "tags": [], "name": "ord_no_hr_paid", "schema": "dbt_dev", "path": "incremental_table\\ord_no_hr_paid.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {"event_type": "to_char(dateadd(hour,7,getdate()),'YYYYMMDDHH24')"}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "SELECT to_char(ord_stl_end_dt,'YYYYMMDD-HH24') ord_stl_end_dy_conc_hr\r\n, dateadd(hour,7,getdate()) created_dt\r\n, max(case when ord_stl_end_dt is not null then ord_no end) ord_no_max\r\nfrom dwuser.sd_sales_dtls t1\r\nwhere to_char(ord_stl_end_dt,'YYYYMMDDHH24') >= '{{ var(\"event_type\") }}'\r\ngroup by 1\r\norder by 1 desc", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.ff_free_delivery_d": {"original_file_path": "models\\materialized_table\\ff_free_delivery_d.sql", "alias": "ff_free_delivery_d", "unique_id": "model.elevenia_c2c.ff_free_delivery_d", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "materialized_table", "ff_free_delivery_d"], "tags": [], "name": "ff_free_delivery_d", "schema": "dbt_dev", "path": "materialized_table\\ff_free_delivery_d.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select trunc(load_dt) load_dt\r\n, basic_dd\r\n, paid_gmv\r\n, dateadd(hour,7,getdate()) created_dt\r\nfrom DWUSER.FF_FREE_DELIVERY \r\nwhere basic_dd = to_char(dateadd(day,-1,getdate()),'YYYYMMDD')", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.trx_list_meta_sum": {"original_file_path": "models\\transactions\\trx_list_meta_sum.sql", "alias": "trx_list_meta_sum", "unique_id": "model.elevenia_c2c.trx_list_meta_sum", "resource_type": "model", "columns": {}, "refs": [["trx_list"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.trx_list"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "transactions", "trx_list_meta_sum"], "tags": [], "name": "trx_list_meta_sum", "schema": "dbt_dev", "path": "transactions\\trx_list_meta_sum.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select trunc(ord_stl_end_dt) ord_stl_end_dt\r\n, meta_ctgr_nm\r\n, sum(paid_gmv) paid_gmv\r\n, sum(paid_gmv_seller_dlv) paid_gmv_seller_dlv\r\nfrom {{ ref('trx_list') }} t1 \r\nwhere meta_ctgr_nm is not null\r\ngroup by 1,2", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.dd_product": {"original_file_path": "models\\transactions\\dd_product.sql", "alias": "dd_product", "unique_id": "model.elevenia_c2c.dd_product", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "transactions", "dd_product"], "tags": [], "name": "dd_product", "schema": "dbt_dev", "path": "transactions\\dd_product.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select * from dwuser.dd_product", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.sessions_ord_test": {"original_file_path": "models\\incremental_table\\sessions_ord_test.sql", "alias": "sessions_ord_test", "unique_id": "model.elevenia_c2c.sessions_ord_test", "resource_type": "model", "columns": {}, "refs": [["ord_trx_hourly"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.ord_trx_hourly"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "sessions_ord_test"], "tags": [], "name": "sessions_ord_test", "schema": "dbt_dev", "path": "incremental_table\\sessions_ord_test.sql", "empty": false, "config": {"materialized": "incremental", "post-hook": [], "vars": {}, "dist": "ord_dt_hour_of_day", "sort": "ord_dt_hour_of_day", "tags": [], "quoting": {}, "column_types": {}, "sql_where": "ord_dt_hour_of_day > (select max(ord_dt_hour_of_day) from )", "pre-hook": [], "unique_key": "ord_dt_hour_of_day", "enabled": true}, "raw_sql": "select * from {{ ref('ord_trx_hourly') }}\r\n{% if adapter.already_exists(this.schema, this.table) and not flags.FULL_REFRESH %}\r\nwhere ord_dt_hour_of_day >= (select max(ord_dt_hour_of_day) from {{ this }})\r\n{% endif %}", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.sessions_paid": {"original_file_path": "models\\incremental_table\\sessions_paid.sql", "alias": "sessions_paid", "unique_id": "model.elevenia_c2c.sessions_paid", "resource_type": "model", "columns": {}, "refs": [["paid_trx_hourly"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.paid_trx_hourly"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "sessions_paid"], "tags": [], "name": "sessions_paid", "schema": "dbt_dev", "path": "incremental_table\\sessions_paid.sql", "empty": false, "config": {"materialized": "incremental", "post-hook": [], "vars": {}, "dist": "ord_stl_end_dt_hour_of_day", "sort": "ord_stl_end_dt_hour_of_day", "tags": [], "quoting": {}, "column_types": {}, "sql_where": "ord_stl_end_dt_hour_of_day > (select max(ord_stl_end_dt_hour_of_day) from )", "pre-hook": [], "unique_key": "ord_stl_end_dt_hour_of_day", "enabled": true}, "raw_sql": "with all_paid_hr as (\r\n\r\n    select * from {{ ref('paid_trx_hourly') }}\r\n\r\n),\r\n\r\n-- Filter out only the events that have arrived since events have last been processed\r\nnew_paid_hr as (\r\n\r\n    select *\r\n    from all_paid_hr\r\n\r\n    -- This conditional is executed just before the model is\r\n    -- executed and returns either True or False\r\n    -- The enclosed `where` filter will be conditionally applied\r\n    -- only if this model exists in the current schema\r\n    {% if adapter.already_exists(this.schema, this.table) and not flags.FULL_REFRESH %}\r\n        where ord_stl_end_dt_hour_of_day >= (select max(ord_stl_end_dt_hour_of_day) from {{ this }})\r\n    {% endif %}\r\n\r\n)\r\nselect * from new_paid_hr", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.trx_list_md_sum": {"original_file_path": "models\\transactions\\trx_list_md_sum.sql", "alias": "trx_list_md_sum", "unique_id": "model.elevenia_c2c.trx_list_md_sum", "resource_type": "model", "columns": {}, "refs": [["trx_list"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.trx_list"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "transactions", "trx_list_md_sum"], "tags": [], "name": "trx_list_md_sum", "schema": "dbt_dev", "path": "transactions\\trx_list_md_sum.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select trunc(ord_stl_end_dt) ord_stl_end_dt\r\n, dept_cd\r\n, sum(paid_gmv) paid_gmv\r\n, sum(paid_gmv_seller_dlv) paid_gmv_seller_dlv\r\nfrom {{ ref('trx_list') }} t1 \r\nwhere dept_cd is not null\r\ngroup by 1,2", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.sd_sales_dtls_mon": {"original_file_path": "models\\incremental_table\\sd_sales_dtls_mon.sql", "alias": "sd_sales_dtls_mon", "unique_id": "model.elevenia_c2c.sd_sales_dtls_mon", "resource_type": "model", "columns": {}, "refs": [["sd_sales_dtls_mon_v"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.sd_sales_dtls_mon_v"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "sd_sales_dtls_mon"], "tags": [], "name": "sd_sales_dtls_mon", "schema": "dbt_dev", "path": "incremental_table\\sd_sales_dtls_mon.sql", "empty": false, "config": {"materialized": "incremental", "post-hook": [], "vars": {}, "dist": "periods", "sort": "periods", "tags": [], "quoting": {}, "column_types": {}, "sql_where": "periods >= ( select max(periods) from dbt_dev.sd_sales_dtls_mon_v )", "pre-hook": [], "unique_key": "periods", "enabled": true}, "raw_sql": "with all_sd_sales_dtls as (\r\n\r\n    select * from {{ ref('sd_sales_dtls_mon_v') }}\r\n\r\n),\r\n\r\n-- Filter out only the events that have arrived since events have last been processed\r\nnew_rows as (\r\n\r\n   select *\r\n   from all_sd_sales_dtls\r\n\r\n    -- This conditional is executed just before the model is\r\n    -- executed and returns either True or False\r\n    -- The enclosed `where` filter will be conditionally applied\r\n    -- only if this model exists in the current schema\r\n   {% if adapter.already_exists(this.schema, this.table) and not flags.FULL_REFRESH %}\r\n        where periods >= ( select max(periods) from {{ this }} )\r\n   {% endif %}\r\n\r\n)\r\nselect * from new_rows", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.sessions": {"original_file_path": "models\\incremental_table\\sessions.sql", "alias": "sessions", "unique_id": "model.elevenia_c2c.sessions", "resource_type": "model", "columns": {}, "refs": [["ord_trx_hourly"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.ord_trx_hourly"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "sessions"], "tags": [], "name": "sessions", "schema": "dbt_dev", "path": "incremental_table\\sessions.sql", "empty": false, "config": {"materialized": "incremental", "post-hook": [], "vars": {}, "dist": "ord_dt_hour_of_day", "sort": "ord_dt_hour_of_day", "tags": [], "quoting": {}, "column_types": {}, "sql_where": "ord_dt_hour_of_day > (select max(ord_dt_hour_of_day) from )", "pre-hook": [], "unique_key": "ord_dt_hour_of_day", "enabled": true}, "raw_sql": "with all_ord_hr as (\r\n\r\n    select * from {{ ref('ord_trx_hourly') }}\r\n\r\n),\r\n\r\n-- Filter out only the events that have arrived since events have last been processed\r\nnew_ord_hr as (\r\n\r\n    select *\r\n    from all_ord_hr\r\n\r\n    -- This conditional is executed just before the model is\r\n    -- executed and returns either True or False\r\n    -- The enclosed `where` filter will be conditionally applied\r\n    -- only if this model exists in the current schema\r\n    {% if adapter.already_exists(this.schema, this.table) and not flags.FULL_REFRESH %}\r\n        where ord_dt_hour_of_day >= (select max(ord_dt_hour_of_day) from {{ this }})\r\n    {% endif %}\r\n\r\n)\r\nselect * from new_ord_hr", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.trx_list_pulsa_sum": {"original_file_path": "models\\transactions\\trx_list_pulsa_sum.sql", "alias": "trx_list_pulsa_sum", "unique_id": "model.elevenia_c2c.trx_list_pulsa_sum", "resource_type": "model", "columns": {}, "refs": [["trx_list"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.trx_list"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "transactions", "trx_list_pulsa_sum"], "tags": [], "name": "trx_list_pulsa_sum", "schema": "dbt_dev", "path": "transactions\\trx_list_pulsa_sum.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "select trunc(ord_stl_end_dt) ord_stl_end_dt\r\n, meta_ctgr_nm\r\n, sum(paid_gmv) paid_gmv\r\n, sum(paid_gmv_seller_dlv) paid_gmv_seller_dlv\r\nfrom {{ ref('trx_list') }} t1 \r\nwhere lower(disp_ctgr_small_cls_nm) = 'phone credit'\r\ngroup by 1,2", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.sd_sales_dtls_mon_v": {"original_file_path": "models\\incremental_table\\sd_sales_dtls_mon_v.sql", "alias": "sd_sales_dtls_mon_v", "unique_id": "model.elevenia_c2c.sd_sales_dtls_mon_v", "resource_type": "model", "columns": {}, "refs": [], "depends_on": {"macros": [], "nodes": []}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "incremental_table", "sd_sales_dtls_mon_v"], "tags": [], "name": "sd_sales_dtls_mon_v", "schema": "dbt_dev", "path": "incremental_table\\sd_sales_dtls_mon_v.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "SELECT to_char(ord_dt,'YYYYMMDDHH24') periods\r\n, dateadd(hour,7,getdate()) exec_dt\r\n, count(1)\r\nfrom dwuser.sd_sales_dtls\r\nwhere to_char(ord_dt,'YYYYMMDD') >= to_char(dateadd(hour,7,getdate()),'YYYYMMDD')\r\ngroup by 1\r\norder by 1 desc", "root_path": "D:\\project\\elevenia_c2c"}, "model.elevenia_c2c.dd_address_v3": {"original_file_path": "models\\p1\\dd_address_v3.sql", "alias": "dd_address_v3", "unique_id": "model.elevenia_c2c.dd_address_v3", "resource_type": "model", "columns": {}, "refs": [["dd_address"]], "depends_on": {"macros": [], "nodes": ["model.elevenia_c2c.dd_address"]}, "description": "", "package_name": "elevenia_c2c", "fqn": ["elevenia_c2c", "p1", "dd_address_v3"], "tags": [], "name": "dd_address_v3", "schema": "dbt_dev", "path": "p1\\dd_address_v3.sql", "empty": false, "config": {"materialized": "view", "post-hook": [], "vars": {}, "quoting": {}, "tags": [], "column_types": {}, "pre-hook": [], "enabled": true}, "raw_sql": "with dd_address as (\r\nselect * from {{ref('dd_address')}}\r\n)\r\nselect province\r\n, count(distinct city) no_of_city\r\nfrom dd_address\r\ngroup by 1", "root_path": "D:\\project\\elevenia_c2c"}}, "child_map": {"model.elevenia_c2c.paid_trx_hourly": ["model.elevenia_c2c.sessions_paid", "model.elevenia_c2c.sessions_paid_test"], "model.elevenia_c2c.sd_sales_dtls": ["model.elevenia_c2c.trx_list"], "model.elevenia_c2c.xbi_1087_top_100": [], "model.elevenia_c2c.sessions_paid_test": [], "model.elevenia_c2c.ord_no_hr_paid": [], "model.elevenia_c2c.sessions_ord_no_hr": [], "model.elevenia_c2c.sessions_dlv": [], "model.elevenia_c2c.trx_list": ["model.elevenia_c2c.trx_list_md_sum", "model.elevenia_c2c.trx_list_meta_sum", "model.elevenia_c2c.trx_list_pulsa_sum"], "model.elevenia_c2c.ord_no_hr": ["model.elevenia_c2c.sessions_ord_no_hr"], "model.elevenia_c2c.dd_disp_ctgr": ["model.elevenia_c2c.trx_list"], "model.elevenia_c2c.dd_tester": ["model.elevenia_c2c.trx_list"], "model.elevenia_c2c.sessions_paid": [], "model.elevenia_c2c.dd_address_v2": [], "model.elevenia_c2c.dd_member": ["model.elevenia_c2c.trx_list", "model.elevenia_c2c.trx_list"], "model.elevenia_c2c.ord_trx_hourly": ["model.elevenia_c2c.sessions", "model.elevenia_c2c.sessions_ord_test"], "model.elevenia_c2c.ff_free_delivery_d": ["model.elevenia_c2c.sessions_dlv"], "model.elevenia_c2c.sessions": [], "model.elevenia_c2c.trx_list_meta_sum": [], "model.elevenia_c2c.dd_product": ["model.elevenia_c2c.trx_list"], "model.elevenia_c2c.sessions_ord_test": [], "model.elevenia_c2c.dd_address": ["model.elevenia_c2c.dd_address_v2", "model.elevenia_c2c.dd_address_v3"], "model.elevenia_c2c.trx_list_md_sum": [], "model.elevenia_c2c.sd_sales_dtls_mon": [], "model.elevenia_c2c.rangga_test": [], "model.elevenia_c2c.trx_list_pulsa_sum": [], "model.elevenia_c2c.sd_sales_dtls_mon_v": ["model.elevenia_c2c.sd_sales_dtls_mon"], "model.elevenia_c2c.dd_address_v3": []}, "generated_at": "2019-01-16T06:30:30.024719Z"}